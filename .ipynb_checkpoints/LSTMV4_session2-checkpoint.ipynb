{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xwtan/anaconda3/envs/james/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/taronzakaryan/predicting-stock-price-using-lstm-model-pytorch\n",
    "#https://bobrupakroy.medium.com/lstms-for-regression-cc9b6677697f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pylab import mpl, plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "torch.manual_seed(8127)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "#We scrap from wikipedia the list of sp500 tickers to use for yahoo finance\n",
    "\n",
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[0].text\n",
    "    tickers.append(ticker)\n",
    "\n",
    "tickers = [s.replace('\\n', '').replace('.','-') for s in tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this if needed. Download data from yahoo finance\n",
    "\n",
    "#data_px = yf.download(tickers, data_source='yahoo', start = '2010-01-01', end = '2022-09-30',timeout=5)\n",
    "#data_px.to_pickle('data_prices.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_px=pd.read_pickle('data_prices.pkl')\n",
    "data_px = data_px['Adj Close']\n",
    "data_px = data_px.fillna(method='ffill')\n",
    "data_px = data_px[data_px.index>='2010-01-03'] #Using data too long ago might not be wise. due to structural/regime change in financial markets.\n",
    "data_px = data_px[data_px.index<'2022-10-01']\n",
    "data_px = data_px[data_px.columns[(~np.isnan(data_px.iloc[0]))]] #So we start around 2011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956, 15)\n",
      "(5912, 15)\n",
      "(8868, 15)\n",
      "(11824, 15)\n",
      "(14780, 15)\n",
      "(17736, 15)\n",
      "(20692, 15)\n",
      "(23648, 15)\n",
      "(26604, 15)\n",
      "(29560, 15)\n",
      "(32516, 15)\n",
      "(35472, 15)\n",
      "(38428, 15)\n",
      "(41384, 15)\n",
      "(44340, 15)\n",
      "(47296, 15)\n",
      "(50252, 15)\n",
      "(53208, 15)\n",
      "(56164, 15)\n",
      "(59120, 15)\n",
      "(62076, 15)\n",
      "(65032, 15)\n",
      "(67988, 15)\n",
      "(70944, 15)\n",
      "(73900, 15)\n",
      "(76856, 15)\n",
      "(79812, 15)\n",
      "(82768, 15)\n",
      "(85724, 15)\n",
      "(88680, 15)\n",
      "(91636, 15)\n",
      "(94592, 15)\n",
      "(97548, 15)\n",
      "(100504, 15)\n",
      "(103460, 15)\n",
      "(106416, 15)\n",
      "(109372, 15)\n",
      "(112328, 15)\n",
      "(115284, 15)\n",
      "(118240, 15)\n",
      "(121196, 15)\n",
      "(124152, 15)\n",
      "(127108, 15)\n",
      "(130064, 15)\n",
      "(133020, 15)\n",
      "(135976, 15)\n",
      "(138932, 15)\n",
      "(141888, 15)\n",
      "(144844, 15)\n",
      "(147800, 15)\n",
      "(150756, 15)\n",
      "(153712, 15)\n",
      "(156668, 15)\n",
      "(159624, 15)\n",
      "(162580, 15)\n",
      "(165536, 15)\n",
      "(168492, 15)\n",
      "(171448, 15)\n",
      "(174404, 15)\n",
      "(177360, 15)\n",
      "(180316, 15)\n",
      "(183272, 15)\n",
      "(186228, 15)\n",
      "(189184, 15)\n",
      "(192140, 15)\n",
      "(195096, 15)\n",
      "(198052, 15)\n",
      "(201008, 15)\n",
      "(203964, 15)\n",
      "(206920, 15)\n",
      "(209876, 15)\n",
      "(212832, 15)\n",
      "(215788, 15)\n",
      "(218744, 15)\n",
      "(221700, 15)\n",
      "(224656, 15)\n",
      "(227612, 15)\n",
      "(230568, 15)\n",
      "(233524, 15)\n",
      "(236480, 15)\n",
      "(239436, 15)\n",
      "(242392, 15)\n",
      "(245348, 15)\n",
      "(248304, 15)\n",
      "(251260, 15)\n",
      "(254216, 15)\n",
      "(257172, 15)\n",
      "(260128, 15)\n",
      "(263084, 15)\n",
      "(266040, 15)\n",
      "(268996, 15)\n",
      "(271952, 15)\n",
      "(274908, 15)\n",
      "(277864, 15)\n",
      "(280820, 15)\n",
      "(283776, 15)\n",
      "(286732, 15)\n",
      "(289688, 15)\n",
      "(292644, 15)\n",
      "(295600, 15)\n",
      "(298556, 15)\n",
      "(301512, 15)\n",
      "(304468, 15)\n",
      "(307424, 15)\n",
      "(310380, 15)\n",
      "(313336, 15)\n",
      "(316292, 15)\n",
      "(319248, 15)\n",
      "(322204, 15)\n",
      "(325160, 15)\n",
      "(328116, 15)\n",
      "(331072, 15)\n",
      "(334028, 15)\n",
      "(336984, 15)\n",
      "(339940, 15)\n",
      "(342896, 15)\n",
      "(345852, 15)\n",
      "(348808, 15)\n",
      "(351764, 15)\n",
      "(354720, 15)\n",
      "(357676, 15)\n",
      "(360632, 15)\n",
      "(363588, 15)\n",
      "(366544, 15)\n",
      "(369500, 15)\n",
      "(372456, 15)\n",
      "(375412, 15)\n",
      "(378368, 15)\n",
      "(381324, 15)\n",
      "(384280, 15)\n",
      "(387236, 15)\n",
      "(390192, 15)\n",
      "(393148, 15)\n",
      "(396104, 15)\n",
      "(399060, 15)\n",
      "(402016, 15)\n",
      "(404972, 15)\n",
      "(407928, 15)\n",
      "(410884, 15)\n",
      "(413840, 15)\n",
      "(416796, 15)\n",
      "(419752, 15)\n",
      "(422708, 15)\n",
      "(425664, 15)\n",
      "(428620, 15)\n",
      "(431576, 15)\n",
      "(434532, 15)\n",
      "(437488, 15)\n",
      "(440444, 15)\n",
      "(443400, 15)\n",
      "(446356, 15)\n",
      "(449312, 15)\n",
      "(452268, 15)\n",
      "(455224, 15)\n",
      "(458180, 15)\n",
      "(461136, 15)\n",
      "(464092, 15)\n",
      "(467048, 15)\n",
      "(470004, 15)\n",
      "(472960, 15)\n",
      "(475916, 15)\n",
      "(478872, 15)\n",
      "(481828, 15)\n",
      "(484784, 15)\n",
      "(487740, 15)\n",
      "(490696, 15)\n",
      "(493652, 15)\n",
      "(496608, 15)\n",
      "(499564, 15)\n",
      "(502520, 15)\n",
      "(505476, 15)\n",
      "(508432, 15)\n",
      "(511388, 15)\n",
      "(514344, 15)\n",
      "(517300, 15)\n",
      "(520256, 15)\n",
      "(523212, 15)\n",
      "(526168, 15)\n",
      "(529124, 15)\n",
      "(532080, 15)\n",
      "(535036, 15)\n",
      "(537992, 15)\n",
      "(540948, 15)\n",
      "(543904, 15)\n",
      "(546860, 15)\n",
      "(549816, 15)\n",
      "(552772, 15)\n",
      "(555728, 15)\n",
      "(558684, 15)\n",
      "(561640, 15)\n",
      "(564596, 15)\n",
      "(567552, 15)\n",
      "(570508, 15)\n",
      "(573464, 15)\n",
      "(576420, 15)\n",
      "(579376, 15)\n",
      "(582332, 15)\n",
      "(585288, 15)\n",
      "(588244, 15)\n",
      "(591200, 15)\n",
      "(594156, 15)\n",
      "(597112, 15)\n",
      "(600068, 15)\n",
      "(603024, 15)\n",
      "(605980, 15)\n",
      "(608936, 15)\n",
      "(611892, 15)\n",
      "(614848, 15)\n",
      "(617804, 15)\n",
      "(620760, 15)\n",
      "(623716, 15)\n",
      "(626672, 15)\n",
      "(629628, 15)\n",
      "(632584, 15)\n",
      "(635540, 15)\n",
      "(638496, 15)\n",
      "(641452, 15)\n",
      "(644408, 15)\n",
      "(647364, 15)\n",
      "(650320, 15)\n",
      "(653276, 15)\n",
      "(656232, 15)\n",
      "(659188, 15)\n",
      "(662144, 15)\n",
      "(665100, 15)\n",
      "(668056, 15)\n",
      "(671012, 15)\n",
      "(673968, 15)\n",
      "(676924, 15)\n",
      "(679880, 15)\n",
      "(682836, 15)\n",
      "(685792, 15)\n",
      "(688748, 15)\n",
      "(691704, 15)\n",
      "(694660, 15)\n",
      "(697616, 15)\n",
      "(700572, 15)\n",
      "(703528, 15)\n",
      "(706484, 15)\n",
      "(709440, 15)\n",
      "(712396, 15)\n",
      "(715352, 15)\n",
      "(718308, 15)\n",
      "(721264, 15)\n",
      "(724220, 15)\n",
      "(727176, 15)\n",
      "(730132, 15)\n",
      "(733088, 15)\n",
      "(736044, 15)\n",
      "(739000, 15)\n",
      "(741956, 15)\n",
      "(744912, 15)\n",
      "(747868, 15)\n",
      "(750824, 15)\n",
      "(753780, 15)\n",
      "(756736, 15)\n",
      "(759692, 15)\n",
      "(762648, 15)\n",
      "(765604, 15)\n",
      "(768560, 15)\n",
      "(771516, 15)\n",
      "(774472, 15)\n",
      "(777428, 15)\n",
      "(780384, 15)\n",
      "(783340, 15)\n",
      "(786296, 15)\n",
      "(789252, 15)\n",
      "(792208, 15)\n",
      "(795164, 15)\n",
      "(798120, 15)\n",
      "(801076, 15)\n",
      "(804032, 15)\n",
      "(806988, 15)\n",
      "(809944, 15)\n",
      "(812900, 15)\n",
      "(815856, 15)\n",
      "(818812, 15)\n",
      "(821768, 15)\n",
      "(824724, 15)\n",
      "(827680, 15)\n",
      "(830636, 15)\n",
      "(833592, 15)\n",
      "(836548, 15)\n",
      "(839504, 15)\n",
      "(842460, 15)\n",
      "(845416, 15)\n",
      "(848372, 15)\n",
      "(851328, 15)\n",
      "(854284, 15)\n",
      "(857240, 15)\n",
      "(860196, 15)\n",
      "(863152, 15)\n",
      "(866108, 15)\n",
      "(869064, 15)\n",
      "(872020, 15)\n",
      "(874976, 15)\n",
      "(877932, 15)\n",
      "(880888, 15)\n",
      "(883844, 15)\n",
      "(886800, 15)\n",
      "(889756, 15)\n",
      "(892712, 15)\n",
      "(895668, 15)\n",
      "(898624, 15)\n",
      "(901580, 15)\n",
      "(904536, 15)\n",
      "(907492, 15)\n",
      "(910448, 15)\n",
      "(913404, 15)\n",
      "(916360, 15)\n",
      "(919316, 15)\n",
      "(922272, 15)\n",
      "(925228, 15)\n",
      "(928184, 15)\n",
      "(931140, 15)\n",
      "(934096, 15)\n",
      "(937052, 15)\n",
      "(940008, 15)\n",
      "(942964, 15)\n",
      "(945920, 15)\n",
      "(948876, 15)\n",
      "(951832, 15)\n",
      "(954788, 15)\n",
      "(957744, 15)\n",
      "(960700, 15)\n",
      "(963656, 15)\n",
      "(966612, 15)\n",
      "(969568, 15)\n",
      "(972524, 15)\n",
      "(975480, 15)\n",
      "(978436, 15)\n",
      "(981392, 15)\n",
      "(984348, 15)\n",
      "(987304, 15)\n",
      "(990260, 15)\n",
      "(993216, 15)\n",
      "(996172, 15)\n",
      "(999128, 15)\n",
      "(1002084, 15)\n",
      "(1005040, 15)\n",
      "(1007996, 15)\n",
      "(1010952, 15)\n",
      "(1013908, 15)\n",
      "(1016864, 15)\n",
      "(1019820, 15)\n",
      "(1022776, 15)\n",
      "(1025732, 15)\n",
      "(1028688, 15)\n",
      "(1031644, 15)\n",
      "(1034600, 15)\n",
      "(1037556, 15)\n",
      "(1040512, 15)\n",
      "(1043468, 15)\n",
      "(1046424, 15)\n",
      "(1049380, 15)\n",
      "(1052336, 15)\n",
      "(1055292, 15)\n",
      "(1058248, 15)\n",
      "(1061204, 15)\n",
      "(1064160, 15)\n",
      "(1067116, 15)\n",
      "(1070072, 15)\n",
      "(1073028, 15)\n",
      "(1075984, 15)\n",
      "(1078940, 15)\n",
      "(1081896, 15)\n",
      "(1084852, 15)\n",
      "(1087808, 15)\n",
      "(1090764, 15)\n",
      "(1093720, 15)\n",
      "(1096676, 15)\n",
      "(1099632, 15)\n",
      "(1102588, 15)\n",
      "(1105544, 15)\n",
      "(1108500, 15)\n",
      "(1111456, 15)\n",
      "(1114412, 15)\n",
      "(1117368, 15)\n",
      "(1120324, 15)\n",
      "(1123280, 15)\n",
      "(1126236, 15)\n",
      "(1129192, 15)\n",
      "(1132148, 15)\n",
      "(1135104, 15)\n",
      "(1138060, 15)\n",
      "(1141016, 15)\n",
      "(1143972, 15)\n",
      "(1146928, 15)\n",
      "(1149884, 15)\n",
      "(1152840, 15)\n",
      "(1155796, 15)\n",
      "(1158752, 15)\n",
      "(1161708, 15)\n",
      "(1164664, 15)\n",
      "(1167620, 15)\n",
      "(1170576, 15)\n",
      "(1173532, 15)\n",
      "(1176488, 15)\n",
      "(1179444, 15)\n",
      "(1182400, 15)\n",
      "(1185356, 15)\n",
      "(1188312, 15)\n",
      "(1191268, 15)\n",
      "(1194224, 15)\n",
      "(1197180, 15)\n",
      "(1200136, 15)\n",
      "(1203092, 15)\n",
      "(1206048, 15)\n",
      "(1209004, 15)\n",
      "(1211960, 15)\n",
      "(1214916, 15)\n",
      "(1217872, 15)\n",
      "(1220828, 15)\n",
      "(1223784, 15)\n",
      "(1226740, 15)\n",
      "(1229696, 15)\n",
      "(1232652, 15)\n",
      "(1235608, 15)\n",
      "(1238564, 15)\n",
      "(1241520, 15)\n",
      "(1244476, 15)\n",
      "(1247432, 15)\n",
      "(1250388, 15)\n",
      "(1253344, 15)\n",
      "(1256300, 15)\n",
      "(1259256, 15)\n",
      "(1262212, 15)\n",
      "(1265168, 15)\n",
      "(1268124, 15)\n",
      "(1271080, 15)\n",
      "(1274036, 15)\n",
      "(1276992, 15)\n",
      "(1279948, 15)\n",
      "(1282904, 15)\n",
      "(1285860, 15)\n",
      "(1288816, 15)\n",
      "(1291772, 15)\n",
      "(1294728, 15)\n"
     ]
    }
   ],
   "source": [
    "#Generate price features\n",
    "# Lag\n",
    "# And Average Price (Typical CTA uses crossover strategy of different frequency)\n",
    "\n",
    "data_px_features = None\n",
    "for asset_i in range(data_px.shape[1]-1):\n",
    "    \n",
    "    temp = data_px.iloc[:,asset_i:asset_i+1].copy()\n",
    "    temp.columns=['PX']\n",
    "    temp['rtn'] = ((temp.iloc[:,0]/temp.iloc[:,0].transform(lambda x: x.shift(1)))-1)\n",
    "    for i in (1,5,21,21*3,21*6,252):\n",
    "        temp['lag_'+str(i)] = temp.iloc[:,0].transform(lambda x: x.shift(i))\n",
    "        if (i==1):\n",
    "            continue\n",
    "        #temp['rolling_mean_'+str(i)] = temp.iloc[:,1].transform(lambda x: x.shift(28).rolling(i).mean())\n",
    "        #temp['rolling_std_'+str(i)]  = temp.iloc[:,1].transform(lambda x: x.shift(28).rolling(i).std())\n",
    "        temp['rolling_meanpx_'+str(i)] = temp.iloc[:,0].transform(lambda x: x.rolling(i).mean())\n",
    "    \n",
    "    temp['ticker'] = data_px.columns[asset_i]\n",
    "    temp['date'] = data_px.iloc[:,asset_i:asset_i+1].index\n",
    "    temp = temp.iloc[np.where(~temp.isnull().any(axis=1))[0][0]:]\n",
    "    \n",
    "    if data_px_features is None:\n",
    "        data_px_features = temp\n",
    "    else:\n",
    "        data_px_features = pd.concat([data_px_features,temp],axis=0,ignore_index=True)\n",
    "    print(data_px_features.shape)\n",
    "\n",
    "\n",
    "data_px_features['date'] = data_px_features['date'].dt.date\n",
    "date_index = list(np.sort(data_px_features['date'].unique()))\n",
    "data_px_features['date_index']  = [date_index.index(i) for i in data_px_features['date'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PX</th>\n",
       "      <th>rtn</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>rolling_meanpx_5</th>\n",
       "      <th>lag_21</th>\n",
       "      <th>rolling_meanpx_21</th>\n",
       "      <th>lag_63</th>\n",
       "      <th>rolling_meanpx_63</th>\n",
       "      <th>lag_126</th>\n",
       "      <th>rolling_meanpx_126</th>\n",
       "      <th>lag_252</th>\n",
       "      <th>rolling_meanpx_252</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>date_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.204119</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>26.911804</td>\n",
       "      <td>27.230104</td>\n",
       "      <td>27.035228</td>\n",
       "      <td>23.852324</td>\n",
       "      <td>25.871875</td>\n",
       "      <td>21.169588</td>\n",
       "      <td>23.815307</td>\n",
       "      <td>18.025654</td>\n",
       "      <td>21.369149</td>\n",
       "      <td>20.331638</td>\n",
       "      <td>21.154197</td>\n",
       "      <td>A</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.950779</td>\n",
       "      <td>-0.009313</td>\n",
       "      <td>27.204119</td>\n",
       "      <td>26.963768</td>\n",
       "      <td>27.032631</td>\n",
       "      <td>24.346004</td>\n",
       "      <td>25.995912</td>\n",
       "      <td>21.461897</td>\n",
       "      <td>23.902432</td>\n",
       "      <td>18.558302</td>\n",
       "      <td>21.435756</td>\n",
       "      <td>20.110786</td>\n",
       "      <td>21.181340</td>\n",
       "      <td>A</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.892326</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>26.950779</td>\n",
       "      <td>27.080704</td>\n",
       "      <td>26.994955</td>\n",
       "      <td>24.118645</td>\n",
       "      <td>26.127992</td>\n",
       "      <td>21.221556</td>\n",
       "      <td>23.992444</td>\n",
       "      <td>18.714205</td>\n",
       "      <td>21.500662</td>\n",
       "      <td>20.039331</td>\n",
       "      <td>21.208534</td>\n",
       "      <td>A</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.944294</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>26.892326</td>\n",
       "      <td>27.015747</td>\n",
       "      <td>26.980664</td>\n",
       "      <td>24.410954</td>\n",
       "      <td>26.248627</td>\n",
       "      <td>21.565832</td>\n",
       "      <td>24.077817</td>\n",
       "      <td>18.610266</td>\n",
       "      <td>21.566805</td>\n",
       "      <td>20.013348</td>\n",
       "      <td>21.236038</td>\n",
       "      <td>A</td>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.035236</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>26.944294</td>\n",
       "      <td>26.911804</td>\n",
       "      <td>27.005351</td>\n",
       "      <td>24.462919</td>\n",
       "      <td>26.371118</td>\n",
       "      <td>21.955568</td>\n",
       "      <td>24.158447</td>\n",
       "      <td>18.317953</td>\n",
       "      <td>21.635989</td>\n",
       "      <td>20.006851</td>\n",
       "      <td>21.263928</td>\n",
       "      <td>A</td>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294723</th>\n",
       "      <td>268.040009</td>\n",
       "      <td>-0.017953</td>\n",
       "      <td>272.940002</td>\n",
       "      <td>288.519989</td>\n",
       "      <td>280.583997</td>\n",
       "      <td>319.339996</td>\n",
       "      <td>297.078570</td>\n",
       "      <td>308.910004</td>\n",
       "      <td>312.448570</td>\n",
       "      <td>427.369995</td>\n",
       "      <td>334.903809</td>\n",
       "      <td>553.460022</td>\n",
       "      <td>426.861468</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294724</th>\n",
       "      <td>265.859985</td>\n",
       "      <td>-0.008133</td>\n",
       "      <td>268.040009</td>\n",
       "      <td>291.209991</td>\n",
       "      <td>275.513995</td>\n",
       "      <td>330.140015</td>\n",
       "      <td>294.017616</td>\n",
       "      <td>309.829987</td>\n",
       "      <td>311.750634</td>\n",
       "      <td>425.660004</td>\n",
       "      <td>333.635555</td>\n",
       "      <td>559.479980</td>\n",
       "      <td>425.696309</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>2022-09-26</td>\n",
       "      <td>2952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294725</th>\n",
       "      <td>264.950012</td>\n",
       "      <td>-0.003423</td>\n",
       "      <td>265.859985</td>\n",
       "      <td>285.649994</td>\n",
       "      <td>271.373999</td>\n",
       "      <td>308.160004</td>\n",
       "      <td>291.959997</td>\n",
       "      <td>302.209991</td>\n",
       "      <td>311.159205</td>\n",
       "      <td>422.279999</td>\n",
       "      <td>332.386904</td>\n",
       "      <td>548.580017</td>\n",
       "      <td>424.570793</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>2953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294726</th>\n",
       "      <td>271.350006</td>\n",
       "      <td>0.024155</td>\n",
       "      <td>264.950012</td>\n",
       "      <td>285.079987</td>\n",
       "      <td>268.628003</td>\n",
       "      <td>307.899994</td>\n",
       "      <td>290.219522</td>\n",
       "      <td>298.329987</td>\n",
       "      <td>310.730952</td>\n",
       "      <td>438.100006</td>\n",
       "      <td>331.063491</td>\n",
       "      <td>531.179993</td>\n",
       "      <td>423.539722</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>2954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294727</th>\n",
       "      <td>265.519989</td>\n",
       "      <td>-0.021485</td>\n",
       "      <td>271.350006</td>\n",
       "      <td>272.940002</td>\n",
       "      <td>267.144000</td>\n",
       "      <td>302.859985</td>\n",
       "      <td>288.441427</td>\n",
       "      <td>293.950012</td>\n",
       "      <td>310.279681</td>\n",
       "      <td>429.609985</td>\n",
       "      <td>329.761190</td>\n",
       "      <td>525.669983</td>\n",
       "      <td>422.507380</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1294728 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PX       rtn       lag_1       lag_5  rolling_meanpx_5  \\\n",
       "0         27.204119  0.010862   26.911804   27.230104         27.035228   \n",
       "1         26.950779 -0.009313   27.204119   26.963768         27.032631   \n",
       "2         26.892326 -0.002169   26.950779   27.080704         26.994955   \n",
       "3         26.944294  0.001932   26.892326   27.015747         26.980664   \n",
       "4         27.035236  0.003375   26.944294   26.911804         27.005351   \n",
       "...             ...       ...         ...         ...               ...   \n",
       "1294723  268.040009 -0.017953  272.940002  288.519989        280.583997   \n",
       "1294724  265.859985 -0.008133  268.040009  291.209991        275.513995   \n",
       "1294725  264.950012 -0.003423  265.859985  285.649994        271.373999   \n",
       "1294726  271.350006  0.024155  264.950012  285.079987        268.628003   \n",
       "1294727  265.519989 -0.021485  271.350006  272.940002        267.144000   \n",
       "\n",
       "             lag_21  rolling_meanpx_21      lag_63  rolling_meanpx_63  \\\n",
       "0         23.852324          25.871875   21.169588          23.815307   \n",
       "1         24.346004          25.995912   21.461897          23.902432   \n",
       "2         24.118645          26.127992   21.221556          23.992444   \n",
       "3         24.410954          26.248627   21.565832          24.077817   \n",
       "4         24.462919          26.371118   21.955568          24.158447   \n",
       "...             ...                ...         ...                ...   \n",
       "1294723  319.339996         297.078570  308.910004         312.448570   \n",
       "1294724  330.140015         294.017616  309.829987         311.750634   \n",
       "1294725  308.160004         291.959997  302.209991         311.159205   \n",
       "1294726  307.899994         290.219522  298.329987         310.730952   \n",
       "1294727  302.859985         288.441427  293.950012         310.279681   \n",
       "\n",
       "            lag_126  rolling_meanpx_126     lag_252  rolling_meanpx_252  \\\n",
       "0         18.025654           21.369149   20.331638           21.154197   \n",
       "1         18.558302           21.435756   20.110786           21.181340   \n",
       "2         18.714205           21.500662   20.039331           21.208534   \n",
       "3         18.610266           21.566805   20.013348           21.236038   \n",
       "4         18.317953           21.635989   20.006851           21.263928   \n",
       "...             ...                 ...         ...                 ...   \n",
       "1294723  427.369995          334.903809  553.460022          426.861468   \n",
       "1294724  425.660004          333.635555  559.479980          425.696309   \n",
       "1294725  422.279999          332.386904  548.580017          424.570793   \n",
       "1294726  438.100006          331.063491  531.179993          423.539722   \n",
       "1294727  429.609985          329.761190  525.669983          422.507380   \n",
       "\n",
       "        ticker        date  date_index  \n",
       "0            A  2011-01-03           0  \n",
       "1            A  2011-01-04           1  \n",
       "2            A  2011-01-05           2  \n",
       "3            A  2011-01-06           3  \n",
       "4            A  2011-01-07           4  \n",
       "...        ...         ...         ...  \n",
       "1294723   ZBRA  2022-09-23        2951  \n",
       "1294724   ZBRA  2022-09-26        2952  \n",
       "1294725   ZBRA  2022-09-27        2953  \n",
       "1294726   ZBRA  2022-09-28        2954  \n",
       "1294727   ZBRA  2022-09-29        2955  \n",
       "\n",
       "[1294728 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data starts 2011\n",
    "#2011 to 2019 (Train), 2020,2021,2022 (Backtest)\n",
    "data_px_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to create train, test data given stock data and sequence length\n",
    "def load_data(stock, look_back,scaler=None,y_scaler=None):\n",
    "        \n",
    "    ignore_columns = ['ticker','date','date_index']\n",
    "    \n",
    "\n",
    "    if scaler is None:\n",
    "        print('Fitting X Scaler')\n",
    "        scaler  = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler  = scaler.fit(stock.drop(ignore_columns,axis=1).values)\n",
    "    if y_scaler is None:\n",
    "        print('Fitting Y Scaler')\n",
    "        y_scaler  = MinMaxScaler(feature_range=(0, 1))\n",
    "        y_scaler = y_scaler.fit(stock.iloc[:,:1].values)\n",
    "\n",
    "    scaled_data = scaler.transform(stock.drop(ignore_columns,axis=1).values)\n",
    "    scaled_y_data = y_scaler.transform(stock['PX'].values.reshape([-1,1]))\n",
    "\n",
    "\n",
    "    total_look_back = look_back + oos_prediction_window\n",
    "\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    index_train = None\n",
    "    ticker_train = None\n",
    "\n",
    "\n",
    "    #Because it is LSTM, we have to create rolling-window data\n",
    "    for ticker in stock['ticker'].unique():\n",
    "\n",
    "        stock_data = scaled_data[np.where(stock['ticker']==ticker)]\n",
    "        stock_rtn_data = scaled_y_data[np.where(stock['ticker']==ticker)]\n",
    "        print(ticker)\n",
    "\n",
    "\n",
    "        data = []\n",
    "        datay = []\n",
    "        for index in range(len(stock_data) - look_back): \n",
    "            data.append(stock_data[index: (index + look_back)])        \n",
    "                \n",
    "        for index in range(len(stock_data) - look_back - oos_prediction_window): \n",
    "            datay.append(stock_rtn_data[(index+look_back): (index + look_back + oos_prediction_window),:1])\n",
    "\n",
    "        data = np.array(data)\n",
    "        datay = np.array(datay)\n",
    "        \n",
    "        padding = np.empty([21,21,1])\n",
    "        padding[:] = np.nan\n",
    "        datay = np.vstack([datay,padding])\n",
    "\n",
    "        index_train_i = stock[stock['ticker']==ticker]['date_index']\n",
    "        index_train_i = index_train_i[look_back:]\n",
    "\n",
    "        ticker_train_i = np.repeat(np.array(ticker),index_train_i.shape[0]).reshape([-1,1])\n",
    "        \n",
    "\n",
    "        if x_train is None:\n",
    "            x_train = data\n",
    "            y_train = datay\n",
    "            index_train = index_train_i\n",
    "            ticker_train = ticker_train_i\n",
    "            \n",
    "        else:\n",
    "            x_train = np.vstack([x_train,data])\n",
    "            y_train = np.vstack([y_train,datay])\n",
    "            index_train = np.vstack([index_train,index_train_i])\n",
    "            ticker_train = np.vstack([ticker_train,ticker_train_i])\n",
    "        \n",
    "    \n",
    "    #y_train = np.apply_along_axis(lambda x:((x[-1]/x[0])-1),1,y_train)\n",
    "    #y_test = np.apply_along_axis(lambda x:((x[-1]/x[0])-1),1,y_test)\n",
    "\n",
    "    index_train = index_train.reshape([-1,1])\n",
    "    ticker_train = ticker_train.reshape([-1,1])\n",
    "    \n",
    "    return [index_train,ticker_train,x_train, y_train,scaler,y_scaler]\n",
    "\n",
    "look_back = 21*3 # choose sequence length\n",
    "oos_prediction_window = 21 #Our returns forecast are 1 month ahead\n",
    "\n",
    "#index_train,ticker_train,x_train, y_train,scaler,y_scaler = load_data(data_px_features, look_back)\n",
    "\n",
    "#Load from cache\n",
    "\n",
    "x_train = np.load('x_train.pkl.npy')\n",
    "y_train = np.load('y_train.pkl.npy')\n",
    "index_train = np.load('index_train.pkl.npy')\n",
    "ticker_train = np.load('ticker_train.pkl.npy')\n",
    "\n",
    "ignore_columns = ['ticker','date','date_index']\n",
    "scaler  = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler  = scaler.fit(data_px_features.drop(ignore_columns,axis=1).values)\n",
    "\n",
    "y_scaler  = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = y_scaler.fit(data_px_features.iloc[:,:1].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (1267134, 63, 13)\n",
      "y_train.shape =  (1267134, 21, 1)\n",
      "index_train.shape =  (1267134, 1)\n",
      "ticker_train.shape =  (1267134, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ',y_train.shape)\n",
    "print('index_train.shape = ',index_train.shape)\n",
    "print('ticker_train.shape = ',ticker_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save('x_train.pkl',x_train)\n",
    "np.save('y_train.pkl',y_train)\n",
    "np.save('index_train.pkl',index_train)\n",
    "np.save('ticker_train.pkl',ticker_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add in macro variables\n",
    "#These are forward looking indicators that may increase predictive power\n",
    "\n",
    "#https://www.clevelandfed.org/indicators-and-data/inflation-nowcasting\n",
    "data_macro = pd.read_csv('data_cpi_nc_all.csv')[['obs_date','CCPI_INFLATION_YoY','CPI_INFLATION_YoY']]\n",
    "data_macro.columns=['Date','CCPI_INFLATION_YoY','CPI_INFLATION_YoY']\n",
    "\n",
    "#https://en.macromicro.me/charts/3997/global-ofr-fsi\n",
    "data_macro2 = pd.read_csv('data_ofr_all.csv')[['obs_date','ofr_usa','ofr_otherdm','ofr_em']]\n",
    "data_macro2.columns=['Date','FSI_USA','FSI_DM','FSI,EM']\n",
    "\n",
    "#https://en.macromicro.me/charts/22907/sp-sector-performance\n",
    "data_macro3 = pd.read_csv('data_sp_all.csv')\n",
    "data_macro3 = data_macro3[['obs_date']+list(data_macro3.columns[:-1][1:])]\n",
    "data_macro3.columns = ['Date']+list(['SEC_'+s for s in data_macro3.columns[1:].str.upper()])\n",
    "\n",
    "data_macro = pd.merge(data_macro,data_macro2,on=\"Date\",how=\"outer\")\n",
    "data_macro = pd.merge(data_macro,data_macro3,on=\"Date\",how=\"outer\")\n",
    "data_macro = data_macro[data_macro['Date']>='2014-01-03']\n",
    "data_macro = data_macro.sort_values('Date')\n",
    "\n",
    "data_macro = pd.merge(data_macro,pd.DataFrame({'Date':list(data_px.index.strftime('%Y-%m-%d'))}),on=\"Date\",how=\"right\")\n",
    "\n",
    "data_macro = data_macro.fillna(method='ffill')\n",
    "data_macro.index = pd.to_datetime(data_macro['Date'])\n",
    "del data_macro['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_scaler  = MinMaxScaler(feature_range=(0, 1))\n",
    "m_scaled_macro = m_scaler.fit_transform(data_macro[data_macro.index>='2014-01-03'].values) #Macro data unfortuantely starts from 2014\n",
    "m_scaled_macro = pd.DataFrame(m_scaled_macro)\n",
    "m_scaled_macro.index = data_macro.index[data_macro.index>='2014-01-03']\n",
    "data_macro.index = data_macro.index.date\n",
    "m_scaled_macro.columns=data_macro.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting the model assuming monthly rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class redirect_output(object):\n",
    "    \"\"\"context manager for reditrecting stdout/err to files\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, stdout='', stderr=''):\n",
    "        self.stdout = stdout\n",
    "        self.stderr = stderr\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.sys_stdout = sys.stdout\n",
    "        self.sys_stderr = sys.stderr\n",
    "\n",
    "        if self.stdout:\n",
    "            sys.stdout = open(self.stdout, 'w')\n",
    "        if self.stderr:\n",
    "            if self.stderr == self.stdout:\n",
    "                sys.stderr = sys.stdout\n",
    "            else:\n",
    "                sys.stderr = open(self.stderr, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        sys.stdout = self.sys_stdout\n",
    "        sys.stderr = self.sys_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Our custom error function\n",
    "def custom_weight_mae(output, target):\n",
    "    \n",
    "    wgt = (torch.arange(1,22)/torch.arange(1,22).sum()).to(device) #Note this is 21 days of weight even though it is 22\n",
    "\n",
    "    loss = torch.matmul((torch.abs((output - target))),wgt).mean()\n",
    "    return loss\n",
    "\n",
    "def custom_weight_rmse(output, target):\n",
    "    \n",
    "    wgt = (torch.arange(1,22)/torch.arange(1,22).sum()).to(device) #Note this is 21 days of weight even though it is 22\n",
    "\n",
    "    loss = torch.matmul(((output - target)**2),wgt).mean()**0.5\n",
    "    return loss\n",
    "\n",
    "#Our early stopper class\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training Function\n",
    "#for updates, LR must be smaller\n",
    "#https://machinelearningmastery.com/update-neural-network-models-with-more-data/\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_lstm_model(model,x_train,y_train,x_valid=None,y_valid=None,start_lr=1e-5,num_epochs=50,batch_size=256):\n",
    "\n",
    "    \n",
    "    #Parameters which can be tuned\n",
    "    optimiser = torch.optim.RAdam(model.parameters(), lr=start_lr)\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min',factor=0.5,patience=3,min_lr =1e-8)\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "    best_model = None\n",
    "    \n",
    "    hist = np.zeros(num_epochs)\n",
    "    hist_val = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    #Prepare data\n",
    "    BATCH_SIZE = batch_size\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_set = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    if (x_valid is not None):\n",
    "        valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "        val_set = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "    for t in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for id_batch, (x_batch, y_batch) in enumerate(train_set):\n",
    "\n",
    "            # Forward pass\n",
    "            y_train_pred = model(x_batch)\n",
    "            #print(f'y_train_pred = {y_train_pred.shape} y_batch = {y_batch.shape}')\n",
    "            loss = custom_weight_mae(y_train_pred, y_batch)\n",
    "            del y_train_pred\n",
    "\n",
    "\n",
    "            hist[t] += loss.item()\n",
    "\n",
    "            # Zero out gradient, else they will accumulate between epochs\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1) #Prevent exploding gradient\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimiser.step()\n",
    "\n",
    "            if id_batch % 100 == 0:\n",
    "                loss, current =  hist[t]/(id_batch+1), (id_batch + 1)* len(x_batch)\n",
    "                print(f\"Epoch {t}. Cumulative Train loss: {loss:>7f}  [{current:>5d}/{len(train_set.dataset):>5d}]\")        \n",
    "\n",
    "\n",
    "\n",
    "        hist[t]/=(id_batch+1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if (x_valid is not None):\n",
    "        \n",
    "            #Compute validation loss\n",
    "            model.eval()\n",
    "            val_loss =0\n",
    "            for id_batch, (x_batch, y_batch) in enumerate(val_set):\n",
    "                y_test_pred = model(x_batch)\n",
    "                hist_val[t] += custom_weight_mae(y_test_pred, y_batch)\n",
    "                del y_test_pred\n",
    "\n",
    "            hist_val[t]/=(id_batch+1)\n",
    "            print(\"Epoch \", t, \"TRAIN-MSE: \", hist[t],\" VAL-MSE:\",hist_val[t],f\" Best Val-MSE {np.min(hist_val[0:(t+1)])}   LR:\",optimiser.param_groups[0]['lr'])\n",
    "            \n",
    "            \n",
    "            if (hist_val[t]==np.min(hist_val[0:(t+1)])):\n",
    "                print('Best Epoch. Setting Saved')\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "            # Update Scheduler\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            #Update Early Scheduler\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                print('[INFO] Early Stop triggered !')\n",
    "                break\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)        \n",
    "    return(model,hist,hist_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(13, 32, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=21, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Global LSTM\n",
    "#####################\n",
    "input_dim = 13 #Number of features x_train.shape[2]\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,dropout=0.2).to(device)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim,32).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 21).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        #print(f'h0 size = {h0.size()}')\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        #print(f'c0 size = {c0.size()}')\n",
    "\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 32, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "global_lstm_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "global_lstm_model.load_state_dict(torch.load('models/lstmv4_global.pth'))\n",
    "global_lstm_model.to(device)\n",
    "\n",
    "\n",
    "print(global_lstm_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Local Asset Mixed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class MixedModel(nn.Module):\n",
    "    def __init__(self,global_lstm_model):\n",
    "        \n",
    "        super(MixedModel, self).__init__()\n",
    "        #self.internal_lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "        #self.internal_lstm.load_state_dict(torch.load('models/lstmv3.pth'))\n",
    "        self.global_lstm_model = global_lstm_model\n",
    "        self.global_lstm_model.train()\n",
    "\n",
    "        # Readout layer\n",
    "        self.macrol1 = nn.Linear(16,21).to(device)\n",
    "        self.mrelu = nn.ReLU()\n",
    "        self.do = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.macrol1.weight.data.fill_(0)\n",
    "        self.macrol1.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(21*2,21).to(device)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.fc2 = nn.Linear(32, 21).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, data_prices,data_macro):\n",
    "\n",
    "        lstm_out = self.global_lstm_model(data_prices)\n",
    "        macro_out = self.macrol1(data_macro)\n",
    "        macro_out  = self.mrelu(macro_out)\n",
    "        macro_out = self.do(macro_out)\n",
    "        \n",
    "        #print(lstm_out.shape)\n",
    "        #print(macro_out.shape)\n",
    "        \n",
    "        combined_output = torch.hstack([lstm_out,macro_out])\n",
    "        \n",
    "        #print(combined_output.shape)\n",
    "        \n",
    "        out = self.fc(combined_output)\n",
    "        #out = self.relu(out)\n",
    "        #out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First train base models until 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Cumulative Train loss: 0.099428  [  256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.097517  [25856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.092174  [51456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.084446  [77056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.075057  [102656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.065933  [128256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.057777  [153856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.051285  [179456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.046208  [205056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.042170  [230656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.038850  [256256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.036086  [281856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.033756  [307456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.031761  [333056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.030025  [358656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.028417  [384256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.026894  [409856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.025507  [435456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.024261  [461056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.023121  [486656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.022078  [512256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.021132  [537856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.020267  [563456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.019468  [589056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.018730  [614656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.018050  [640256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.017417  [665856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.016827  [691456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.016279  [717056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.015767  [742656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.015287  [768256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.014836  [793856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.014412  [819456/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.014012  [845056/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.013637  [870656/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.013281  [896256/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.012945  [921856/964038]\n",
      "Epoch 0. Cumulative Train loss: 0.012626  [947456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001175  [  256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001107  [25856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001105  [51456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001090  [77056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001079  [102656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001071  [128256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001064  [153856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001057  [179456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001050  [205056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001046  [230656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001036  [256256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001029  [281856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001024  [307456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001018  [333056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001012  [358656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001007  [384256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.001002  [409856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000998  [435456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000993  [461056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000989  [486656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000985  [512256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000979  [537856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000976  [563456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000971  [589056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000967  [614656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000963  [640256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000959  [665856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000955  [691456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000952  [717056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000948  [742656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000944  [768256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000940  [793856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000937  [819456/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000934  [845056/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000931  [870656/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000927  [896256/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000924  [921856/964038]\n",
      "Epoch 1. Cumulative Train loss: 0.000921  [947456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000915  [  256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000794  [25856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000800  [51456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000809  [77056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000804  [102656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000805  [128256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000800  [153856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000797  [179456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000794  [205056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000793  [230656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000790  [256256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000789  [281856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000788  [307456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000785  [333056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000785  [358656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000783  [384256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000783  [409856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000783  [435456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000782  [461056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000782  [486656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000780  [512256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000779  [537856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000778  [563456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000776  [589056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000775  [614656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000773  [640256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000772  [665856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000771  [691456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000769  [717056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000768  [742656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000766  [768256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000765  [793856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000764  [819456/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000763  [845056/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000763  [870656/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000763  [896256/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000762  [921856/964038]\n",
      "Epoch 2. Cumulative Train loss: 0.000761  [947456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000633  [  256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000719  [25856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000715  [51456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000722  [77056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000720  [102656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000721  [128256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000723  [153856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000724  [179456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000723  [205056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000722  [230656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000722  [256256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000720  [281856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000720  [307456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000719  [333056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000716  [358656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000715  [384256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000714  [409856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000715  [435456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000715  [461056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000713  [486656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000713  [512256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000712  [537856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000712  [563456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000710  [589056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000710  [614656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000709  [640256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000709  [665856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000708  [691456/964038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. Cumulative Train loss: 0.000708  [717056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000708  [742656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000707  [768256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000707  [793856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000706  [819456/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000706  [845056/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000706  [870656/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000706  [896256/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000706  [921856/964038]\n",
      "Epoch 3. Cumulative Train loss: 0.000705  [947456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000605  [  256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000689  [25856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000687  [51456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000684  [77056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000677  [102656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000679  [128256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [153856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000682  [179456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000681  [205056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000681  [230656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000683  [256256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000682  [281856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000683  [307456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000683  [333056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000682  [358656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000683  [384256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000681  [409856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [435456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000681  [461056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000681  [486656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [512256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [537856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [563456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [589056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000680  [614656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000679  [640256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000678  [665856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000677  [691456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000677  [717056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000677  [742656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000677  [768256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000677  [793856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000676  [819456/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000676  [845056/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000676  [870656/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000676  [896256/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000676  [921856/964038]\n",
      "Epoch 4. Cumulative Train loss: 0.000675  [947456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000578  [  256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000665  [25856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000663  [51456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000660  [77056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000665  [102656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000665  [128256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [153856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [179456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [205056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [230656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [256256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [281856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000663  [307456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [333056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [358656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [384256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000663  [409856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000663  [435456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000663  [461056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000663  [486656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [512256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [537856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [563456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [589056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [614656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [640256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [665856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [691456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [717056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000662  [742656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000661  [768256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000660  [793856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000660  [819456/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000659  [845056/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000659  [870656/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000659  [896256/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000659  [921856/964038]\n",
      "Epoch 5. Cumulative Train loss: 0.000659  [947456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000582  [  256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000652  [25856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000648  [51456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000645  [77056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000649  [102656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [128256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000652  [153856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000653  [179456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000654  [205056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000652  [230656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [256256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000653  [281856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000652  [307456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000652  [333056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000652  [358656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [384256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [409856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [435456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [461056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [486656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [512256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [537856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [563456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [589056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [614656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [640256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000651  [665856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [691456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [717056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [742656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [768256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [793856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [819456/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [845056/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [870656/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000650  [896256/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000649  [921856/964038]\n",
      "Epoch 6. Cumulative Train loss: 0.000649  [947456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000516  [  256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000655  [25856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [51456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [77056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000645  [102656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [128256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000644  [153856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000646  [179456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000646  [205056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [230656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000644  [256256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000644  [281856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000644  [307456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000645  [333056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000645  [358656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000644  [384256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000642  [409856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000642  [435456/964038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7. Cumulative Train loss: 0.000642  [461056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [486656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [512256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [537856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [563456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [589056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [614656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [640256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000643  [665856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000642  [691456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000642  [717056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [742656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [768256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [793856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [819456/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [845056/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [870656/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [896256/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [921856/964038]\n",
      "Epoch 7. Cumulative Train loss: 0.000641  [947456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000858  [  256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000639  [25856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000641  [51456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [77056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000639  [102656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000638  [128256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000640  [153856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000641  [179456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000639  [205056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000638  [230656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000638  [256256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [281856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [307456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000638  [333056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [358656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [384256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [409856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [435456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [461056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [486656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [512256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [537856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [563456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000638  [589056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [614656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000637  [640256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [665856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [691456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [717056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000635  [742656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [768256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [793856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [819456/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [845056/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [870656/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [896256/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [921856/964038]\n",
      "Epoch 8. Cumulative Train loss: 0.000636  [947456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000668  [  256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000653  [25856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000642  [51456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000635  [77056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000636  [102656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000636  [128256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000635  [153856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000636  [179456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000636  [205056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000636  [230656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000635  [256256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000634  [281856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000633  [307456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000633  [333056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000633  [358656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000634  [384256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000634  [409856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000634  [435456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000633  [461056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000633  [486656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000632  [512256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000631  [537856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [563456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000631  [589056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [614656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [640256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [665856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000631  [691456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [717056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [742656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [768256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [793856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [819456/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [845056/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000631  [870656/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [896256/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [921856/964038]\n",
      "Epoch 9. Cumulative Train loss: 0.000630  [947456/964038]\n"
     ]
    }
   ],
   "source": [
    "index_until_train = date_index.index(datetime.date(2019, 12, 31)) #Train until 2017\n",
    "#index_until_valid = date_index.index(datetime.date(2019, 1, 2)) #Validate via 2018,2019\n",
    "\n",
    "\n",
    "current_train_x = x_train[np.where(index_train<=index_until_train)[0],:,:]\n",
    "current_train_x = torch.from_numpy(current_train_x).type(torch.Tensor).to(device)\n",
    "current_train_y = y_train[np.where(index_train<=index_until_train)[0],:,:]\n",
    "current_train_y = torch.from_numpy(current_train_y).type(torch.Tensor).to(device)\n",
    "current_train_y = current_train_y.reshape([-1,21])\n",
    "\n",
    "#current_valid_x = x_train[np.where((index_train<=index_until_valid) & (index_train>index_until_train))[0],:,:]\n",
    "#current_valid_x = torch.from_numpy(current_valid_x).type(torch.Tensor).to(device)\n",
    "#current_valid_y = y_train[np.where((index_train<=index_until_valid) & (index_train>index_until_train))[0],:,:]\n",
    "#current_valid_y = torch.from_numpy(current_valid_y).type(torch.Tensor).to(device)\n",
    "#current_valid_y = current_valid_y.reshape([-1,21])\n",
    "\n",
    "\n",
    "global_lstm_model,hist,hist_val = train_lstm_model(model=global_lstm_model,\n",
    "                                     x_train=current_train_x,\n",
    "                                     y_train=current_train_y,\n",
    "                                     start_lr=1e-4,\n",
    "                                     num_epochs=10,\n",
    "                                     batch_size=256)\n",
    "\n",
    "\n",
    "#del current_train_x\n",
    "#del current_train_y\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.save(global_lstm_model.state_dict(),'models/lstmv4_global.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist[1:10], label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second phase, Mixed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def predict_individual_assets(global_lstm_model,prev_date_index):\n",
    "\n",
    "    asset_predicted_returns = {}\n",
    "    macro_data = m_scaled_macro[m_scaled_macro.index.date<=date_index[prev_date_index]]\n",
    "    macro_data = torch.from_numpy(macro_data.values).type(torch.Tensor).to(device)\n",
    "    #loss_fn = nn.MSELoss()\n",
    "\n",
    "    for tic in data_px_features['ticker'].unique():\n",
    "        \n",
    "        print(tic)\n",
    "\n",
    "        model_i = MixedModel(global_lstm_model=copy.deepcopy(global_lstm_model))\n",
    "        model_i.to(device)\n",
    "        optimiser = torch.optim.RAdam(model_i.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "        price_i_data = x_train[np.where((index_train<=prev_date_index) & (index_train>=date_index.index(datetime.date(2014, 1, 3)))\n",
    "                                        & (ticker_train==tic))[0],:,:]\n",
    "        price_i_data = torch.from_numpy(price_i_data).type(torch.Tensor).to(device)\n",
    "\n",
    "        oos_i_data = price_i_data[-1,:,:]\n",
    "        \n",
    "        price_i_data = price_i_data[:-21]\n",
    "\n",
    "        y_i_data = y_train[np.where((index_train<=prev_date_index) & (index_train>=date_index.index(datetime.date(2014, 1, 3)))\n",
    "                                        & (ticker_train==tic))[0],:,:]\n",
    "\n",
    "        y_i_data = torch.from_numpy(y_i_data).type(torch.Tensor).to(device)\n",
    "        y_i_data = y_i_data.reshape(y_i_data.shape[:-1])\n",
    "        oos_r_data=  y_i_data[-1,]\n",
    "        y_i_data = y_i_data[:-21]\n",
    "\n",
    "\n",
    "\n",
    "        n_epochs_asset_i = 500   \n",
    "        train_error = np.zeros(n_epochs_asset_i)\n",
    "\n",
    "        for t in range(n_epochs_asset_i):\n",
    "\n",
    "            # Forward pass\n",
    "            model_i.train()\n",
    "            y_train_pred = model_i(price_i_data, macro_data[:-21])\n",
    "            loss = custom_weight_rmse(y_train_pred, y_i_data)\n",
    "            #loss = loss_fn(y_train_pred, y_i_data)\n",
    "\n",
    "\n",
    "            train_error[t] = loss.item()\n",
    "\n",
    "            # Zero out gradient, else they will accumulate between epochs\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model_i.parameters(), 1) #Prevent exploding gradient\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimiser.step()\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Asset {tic} Epoch {t}. Cumulative Train loss: {loss:>7f}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Predict out of sample\n",
    "        model_i.eval()\n",
    "        global_lstm_model.eval()\n",
    "        oos_price_predictions = model_i(oos_i_data.reshape([1,-1,13]), macro_data[-1:])\n",
    "        oos_price_predictions_global = global_lstm_model(oos_i_data.reshape([1,-1,13]))\n",
    "        \n",
    "        asset_predicted_returns[tic] = {\n",
    "            'predicted_global': y_scaler.inverse_transform(oos_price_predictions_global.detach().cpu()).reshape(-1),\n",
    "            'predicted_finetuned': y_scaler.inverse_transform(oos_price_predictions.detach().cpu()).reshape(-1),\n",
    "            'actual': y_scaler.inverse_transform(oos_r_data.detach().cpu().reshape(-1, 1)).reshape(-1)}\n",
    "\n",
    "\n",
    "        \n",
    "        actual_rtn = (asset_predicted_returns[tic]['actual'][-1]/asset_predicted_returns[tic]['actual'][0])-1\n",
    "        predicted_rtn = (asset_predicted_returns[tic]['predicted_finetuned'][-1]/asset_predicted_returns[tic]['predicted_finetuned'][0])-1\n",
    "        predicted_global_rtn = (asset_predicted_returns[tic]['predicted_global'][-1]/asset_predicted_returns[tic]['predicted_global'][0])-1\n",
    "        \n",
    "        asset_predicted_returns[tic]['actual_rtn'] = actual_rtn\n",
    "        asset_predicted_returns[tic]['predicted_rtn'] = predicted_rtn\n",
    "        asset_predicted_returns[tic]['predicted_global_rtn'] = predicted_global_rtn\n",
    "        \n",
    "         \n",
    "        print(f'{tic} actual rtn = {actual_rtn}. predicted rtn = {predicted_rtn}, predicted_global_rtn = {predicted_global_rtn}')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        del oos_i_data\n",
    "        del model_i\n",
    "        del price_i_data\n",
    "        del y_i_data\n",
    "\n",
    "        \n",
    "    return(asset_predicted_returns)\n",
    "    \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_lstm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m new_train_y \u001b[38;5;241m=\u001b[39m new_train_y\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m21\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[INFO] Updating Global model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m global_lstm_model,hist,hist_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm_model\u001b[49m(model\u001b[38;5;241m=\u001b[39mglobal_lstm_model,\n\u001b[1;32m     28\u001b[0m                                      x_train\u001b[38;5;241m=\u001b[39mnew_train_x,\n\u001b[1;32m     29\u001b[0m                                      y_train\u001b[38;5;241m=\u001b[39mnew_train_y,\n\u001b[1;32m     30\u001b[0m                                      start_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1e-1\u001b[39m), \u001b[38;5;66;03m#So that old gradient dont get flushed out\u001b[39;00m\n\u001b[1;32m     31\u001b[0m                                      num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     32\u001b[0m                                      batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     35\u001b[0m prev_date_index \u001b[38;5;241m=\u001b[39m current_date_index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_lstm_model' is not defined"
     ]
    }
   ],
   "source": [
    "month_end_dates = data_px.reset_index().groupby([(data_px.index.year),(data_px.index.month)])['Date'].max()\n",
    "month_end_dates = month_end_dates.dt.date.values\n",
    "month_end_dates = month_end_dates[month_end_dates>datetime.date(2019, 12, 31)]\n",
    "prev_date_index = date_index.index(datetime.date(2019, 12, 31))\n",
    "\n",
    "\n",
    "backtest_returns = {}\n",
    "\n",
    "import time\n",
    "with redirect_output(\"lstm_train_log2.txt\"):\n",
    "    for date_i in month_end_dates:\n",
    "        print(f'Date=={date_i}')\n",
    "        current_date_index = date_index.index(date_i)\n",
    "\n",
    "        #Predict expected returns    \n",
    "        backtest_returns[date_i] = predict_individual_assets(global_lstm_model,prev_date_index)\n",
    "\n",
    "        #Update model after observing new prices\n",
    "        new_train_x = x_train[np.where((index_train<=current_date_index) & (index_train>prev_date_index))[0],:,:]\n",
    "        new_train_x = torch.from_numpy(new_train_x).type(torch.Tensor).to(device)\n",
    "        new_train_y = y_train[np.where((index_train<=current_date_index) & (index_train>prev_date_index))[0],:,:]\n",
    "        new_train_y = torch.from_numpy(new_train_y).type(torch.Tensor).to(device)\n",
    "        new_train_y = new_train_y.reshape([-1,21])\n",
    "        \n",
    "        \n",
    "        print('[INFO] Updating Global model')\n",
    "        global_lstm_model,hist,hist_val = train_lstm_model(model=global_lstm_model,\n",
    "                                             x_train=new_train_x,\n",
    "                                             y_train=new_train_y,\n",
    "                                             start_lr=1e-4*(1e-1), #So that old gradient dont get flushed out\n",
    "                                             num_epochs=25,\n",
    "                                             batch_size=256)\n",
    "\n",
    "\n",
    "        prev_date_index = current_date_index\n",
    "        print(datetime.datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"expected_returns.json\", \"w\") as outfile:\n",
    "    json.dump(asset_predicted_returns, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_lstm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_lstm_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_lstm_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "james",
   "language": "python",
   "name": "james"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
