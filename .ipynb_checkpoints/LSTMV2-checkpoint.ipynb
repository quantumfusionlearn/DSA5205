{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bear\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/taronzakaryan/predicting-stock-price-using-lstm-model-pytorch\n",
    "#https://bobrupakroy.medium.com/lstms-for-regression-cc9b6677697f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pylab import mpl, plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "torch.manual_seed(8127)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[0].text\n",
    "    tickers.append(ticker)\n",
    "\n",
    "tickers = [s.replace('\\n', '').replace('.','-') for s in tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_px = yf.download(tickers, data_source='yahoo', start = '2011-01-01', end = '2022-09-30',timeout=5)\n",
    "#data_px.to_pickle('data_prices.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_px=pd.read_pickle('data_prices.pkl')\n",
    "data_px = data_px['Adj Close']\n",
    "data_px = data_px.fillna(method='ffill')\n",
    "#data_px = data_px[data_px.index>='2014-01-03']\n",
    "data_px = data_px[data_px.index<'2022-10-01']\n",
    "data_px = data_px[data_px.columns[(~np.isnan(data_px.iloc[0]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2676, 19)\n",
      "(5352, 19)\n",
      "(8028, 19)\n",
      "(10704, 19)\n",
      "(13380, 19)\n",
      "(16056, 19)\n",
      "(18732, 19)\n",
      "(21408, 19)\n",
      "(24084, 19)\n",
      "(26760, 19)\n",
      "(29436, 19)\n",
      "(32112, 19)\n",
      "(34788, 19)\n",
      "(37464, 19)\n",
      "(40140, 19)\n",
      "(42816, 19)\n",
      "(45492, 19)\n",
      "(48168, 19)\n",
      "(50844, 19)\n",
      "(53520, 19)\n",
      "(56196, 19)\n",
      "(58872, 19)\n",
      "(61548, 19)\n",
      "(64224, 19)\n",
      "(66900, 19)\n",
      "(69576, 19)\n",
      "(72252, 19)\n",
      "(74928, 19)\n",
      "(77604, 19)\n",
      "(80280, 19)\n",
      "(82956, 19)\n",
      "(85632, 19)\n",
      "(88308, 19)\n",
      "(90984, 19)\n",
      "(93660, 19)\n",
      "(96336, 19)\n",
      "(99012, 19)\n",
      "(101688, 19)\n",
      "(104364, 19)\n",
      "(107040, 19)\n",
      "(109716, 19)\n",
      "(112392, 19)\n",
      "(115068, 19)\n",
      "(117744, 19)\n",
      "(120420, 19)\n",
      "(123096, 19)\n",
      "(125772, 19)\n",
      "(128448, 19)\n",
      "(131124, 19)\n",
      "(133800, 19)\n",
      "(136476, 19)\n",
      "(139152, 19)\n",
      "(141828, 19)\n",
      "(144504, 19)\n",
      "(147180, 19)\n",
      "(149856, 19)\n",
      "(152532, 19)\n",
      "(155208, 19)\n",
      "(157884, 19)\n",
      "(160560, 19)\n",
      "(163236, 19)\n",
      "(165912, 19)\n",
      "(168588, 19)\n",
      "(171264, 19)\n",
      "(173940, 19)\n",
      "(176616, 19)\n",
      "(179292, 19)\n",
      "(181968, 19)\n",
      "(184644, 19)\n",
      "(187320, 19)\n",
      "(189996, 19)\n",
      "(192672, 19)\n",
      "(195348, 19)\n",
      "(198024, 19)\n",
      "(200700, 19)\n",
      "(203376, 19)\n",
      "(206052, 19)\n",
      "(208728, 19)\n",
      "(211404, 19)\n",
      "(214080, 19)\n",
      "(216756, 19)\n",
      "(219432, 19)\n",
      "(222108, 19)\n",
      "(224784, 19)\n",
      "(227460, 19)\n",
      "(230136, 19)\n",
      "(232812, 19)\n",
      "(235488, 19)\n",
      "(238164, 19)\n",
      "(240840, 19)\n",
      "(243516, 19)\n",
      "(246192, 19)\n",
      "(248868, 19)\n",
      "(251544, 19)\n",
      "(254220, 19)\n",
      "(256896, 19)\n",
      "(259572, 19)\n",
      "(262248, 19)\n",
      "(264924, 19)\n",
      "(267600, 19)\n",
      "(270276, 19)\n",
      "(272952, 19)\n",
      "(275628, 19)\n",
      "(278304, 19)\n",
      "(280980, 19)\n",
      "(283656, 19)\n",
      "(286332, 19)\n",
      "(289008, 19)\n",
      "(291684, 19)\n",
      "(294360, 19)\n",
      "(297036, 19)\n",
      "(299712, 19)\n",
      "(302388, 19)\n",
      "(305064, 19)\n",
      "(307740, 19)\n",
      "(310416, 19)\n",
      "(313092, 19)\n",
      "(315768, 19)\n",
      "(318444, 19)\n",
      "(321120, 19)\n",
      "(323796, 19)\n",
      "(326472, 19)\n",
      "(329148, 19)\n",
      "(331824, 19)\n",
      "(334500, 19)\n",
      "(337176, 19)\n",
      "(339852, 19)\n",
      "(342528, 19)\n",
      "(345204, 19)\n",
      "(347880, 19)\n",
      "(350556, 19)\n",
      "(353232, 19)\n",
      "(355908, 19)\n",
      "(358584, 19)\n",
      "(361260, 19)\n",
      "(363936, 19)\n",
      "(366612, 19)\n",
      "(369288, 19)\n",
      "(371964, 19)\n",
      "(374640, 19)\n",
      "(377316, 19)\n",
      "(379992, 19)\n",
      "(382668, 19)\n",
      "(385344, 19)\n",
      "(388020, 19)\n",
      "(390696, 19)\n",
      "(393372, 19)\n",
      "(396048, 19)\n",
      "(398724, 19)\n",
      "(401400, 19)\n",
      "(404076, 19)\n",
      "(406752, 19)\n",
      "(409428, 19)\n",
      "(412104, 19)\n",
      "(414780, 19)\n",
      "(417456, 19)\n",
      "(420132, 19)\n",
      "(422808, 19)\n",
      "(425484, 19)\n",
      "(428160, 19)\n",
      "(430836, 19)\n",
      "(433512, 19)\n",
      "(436188, 19)\n",
      "(438864, 19)\n",
      "(441540, 19)\n",
      "(444216, 19)\n",
      "(446892, 19)\n",
      "(449568, 19)\n",
      "(452244, 19)\n",
      "(454920, 19)\n",
      "(457596, 19)\n",
      "(460272, 19)\n",
      "(462948, 19)\n",
      "(465624, 19)\n",
      "(468300, 19)\n",
      "(470976, 19)\n",
      "(473652, 19)\n",
      "(476328, 19)\n",
      "(479004, 19)\n",
      "(481680, 19)\n",
      "(484356, 19)\n",
      "(487032, 19)\n",
      "(489708, 19)\n",
      "(492384, 19)\n",
      "(495060, 19)\n",
      "(497736, 19)\n",
      "(500412, 19)\n",
      "(503088, 19)\n",
      "(505764, 19)\n",
      "(508440, 19)\n",
      "(511116, 19)\n",
      "(513792, 19)\n",
      "(516468, 19)\n",
      "(519144, 19)\n",
      "(521820, 19)\n",
      "(524496, 19)\n",
      "(527172, 19)\n",
      "(529848, 19)\n",
      "(532524, 19)\n",
      "(535200, 19)\n",
      "(537876, 19)\n",
      "(540552, 19)\n",
      "(543228, 19)\n",
      "(545904, 19)\n",
      "(548580, 19)\n",
      "(551256, 19)\n",
      "(553932, 19)\n",
      "(556608, 19)\n",
      "(559284, 19)\n",
      "(561960, 19)\n",
      "(564636, 19)\n",
      "(567312, 19)\n",
      "(569988, 19)\n",
      "(572664, 19)\n",
      "(575340, 19)\n",
      "(578016, 19)\n",
      "(580692, 19)\n",
      "(583368, 19)\n",
      "(586044, 19)\n",
      "(588720, 19)\n",
      "(591396, 19)\n",
      "(594072, 19)\n",
      "(596748, 19)\n",
      "(599424, 19)\n",
      "(602100, 19)\n",
      "(604776, 19)\n",
      "(607452, 19)\n",
      "(610128, 19)\n",
      "(612804, 19)\n",
      "(615480, 19)\n",
      "(618156, 19)\n",
      "(620832, 19)\n",
      "(623508, 19)\n",
      "(626184, 19)\n",
      "(628860, 19)\n",
      "(631536, 19)\n",
      "(634212, 19)\n",
      "(636888, 19)\n",
      "(639564, 19)\n",
      "(642240, 19)\n",
      "(644916, 19)\n",
      "(647592, 19)\n",
      "(650268, 19)\n",
      "(652944, 19)\n",
      "(655620, 19)\n",
      "(658296, 19)\n",
      "(660972, 19)\n",
      "(663648, 19)\n",
      "(666324, 19)\n",
      "(669000, 19)\n",
      "(671676, 19)\n",
      "(674352, 19)\n",
      "(677028, 19)\n",
      "(679704, 19)\n",
      "(682380, 19)\n",
      "(685056, 19)\n",
      "(687732, 19)\n",
      "(690408, 19)\n",
      "(693084, 19)\n",
      "(695760, 19)\n",
      "(698436, 19)\n",
      "(701112, 19)\n",
      "(703788, 19)\n",
      "(706464, 19)\n",
      "(709140, 19)\n",
      "(711816, 19)\n",
      "(714492, 19)\n",
      "(717168, 19)\n",
      "(719844, 19)\n",
      "(722520, 19)\n",
      "(725196, 19)\n",
      "(727872, 19)\n",
      "(730548, 19)\n",
      "(733224, 19)\n",
      "(735900, 19)\n",
      "(738576, 19)\n",
      "(741252, 19)\n",
      "(743928, 19)\n",
      "(746604, 19)\n",
      "(749280, 19)\n",
      "(751956, 19)\n",
      "(754632, 19)\n",
      "(757308, 19)\n",
      "(759984, 19)\n",
      "(762660, 19)\n",
      "(765336, 19)\n",
      "(768012, 19)\n",
      "(770688, 19)\n",
      "(773364, 19)\n",
      "(776040, 19)\n",
      "(778716, 19)\n",
      "(781392, 19)\n",
      "(784068, 19)\n",
      "(786744, 19)\n",
      "(789420, 19)\n",
      "(792096, 19)\n",
      "(794772, 19)\n",
      "(797448, 19)\n",
      "(800124, 19)\n",
      "(802800, 19)\n",
      "(805476, 19)\n",
      "(808152, 19)\n",
      "(810828, 19)\n",
      "(813504, 19)\n",
      "(816180, 19)\n",
      "(818856, 19)\n",
      "(821532, 19)\n",
      "(824208, 19)\n",
      "(826884, 19)\n",
      "(829560, 19)\n",
      "(832236, 19)\n",
      "(834912, 19)\n",
      "(837588, 19)\n",
      "(840264, 19)\n",
      "(842940, 19)\n",
      "(845616, 19)\n",
      "(848292, 19)\n",
      "(850968, 19)\n",
      "(853644, 19)\n",
      "(856320, 19)\n",
      "(858996, 19)\n",
      "(861672, 19)\n",
      "(864348, 19)\n",
      "(867024, 19)\n",
      "(869700, 19)\n",
      "(872376, 19)\n",
      "(875052, 19)\n",
      "(877728, 19)\n",
      "(880404, 19)\n",
      "(883080, 19)\n",
      "(885756, 19)\n",
      "(888432, 19)\n",
      "(891108, 19)\n",
      "(893784, 19)\n",
      "(896460, 19)\n",
      "(899136, 19)\n",
      "(901812, 19)\n",
      "(904488, 19)\n",
      "(907164, 19)\n",
      "(909840, 19)\n",
      "(912516, 19)\n",
      "(915192, 19)\n",
      "(917868, 19)\n",
      "(920544, 19)\n",
      "(923220, 19)\n",
      "(925896, 19)\n",
      "(928572, 19)\n",
      "(931248, 19)\n",
      "(933924, 19)\n",
      "(936600, 19)\n",
      "(939276, 19)\n",
      "(941952, 19)\n",
      "(944628, 19)\n",
      "(947304, 19)\n",
      "(949980, 19)\n",
      "(952656, 19)\n",
      "(955332, 19)\n",
      "(958008, 19)\n",
      "(960684, 19)\n",
      "(963360, 19)\n",
      "(966036, 19)\n",
      "(968712, 19)\n",
      "(971388, 19)\n",
      "(974064, 19)\n",
      "(976740, 19)\n",
      "(979416, 19)\n",
      "(982092, 19)\n",
      "(984768, 19)\n",
      "(987444, 19)\n",
      "(990120, 19)\n",
      "(992796, 19)\n",
      "(995472, 19)\n",
      "(998148, 19)\n",
      "(1000824, 19)\n",
      "(1003500, 19)\n",
      "(1006176, 19)\n",
      "(1008852, 19)\n",
      "(1011528, 19)\n",
      "(1014204, 19)\n",
      "(1016880, 19)\n",
      "(1019556, 19)\n",
      "(1022232, 19)\n",
      "(1024908, 19)\n",
      "(1027584, 19)\n",
      "(1030260, 19)\n",
      "(1032936, 19)\n",
      "(1035612, 19)\n",
      "(1038288, 19)\n",
      "(1040964, 19)\n",
      "(1043640, 19)\n",
      "(1046316, 19)\n",
      "(1048992, 19)\n",
      "(1051668, 19)\n",
      "(1054344, 19)\n",
      "(1057020, 19)\n",
      "(1059696, 19)\n",
      "(1062372, 19)\n",
      "(1065048, 19)\n",
      "(1067724, 19)\n",
      "(1070400, 19)\n",
      "(1073076, 19)\n",
      "(1075752, 19)\n",
      "(1078428, 19)\n",
      "(1081104, 19)\n",
      "(1083780, 19)\n",
      "(1086456, 19)\n",
      "(1089132, 19)\n",
      "(1091808, 19)\n",
      "(1094484, 19)\n",
      "(1097160, 19)\n",
      "(1099836, 19)\n",
      "(1102512, 19)\n",
      "(1105188, 19)\n",
      "(1107864, 19)\n",
      "(1110540, 19)\n",
      "(1113216, 19)\n",
      "(1115892, 19)\n",
      "(1118568, 19)\n",
      "(1121244, 19)\n",
      "(1123920, 19)\n",
      "(1126596, 19)\n",
      "(1129272, 19)\n",
      "(1131948, 19)\n",
      "(1134624, 19)\n",
      "(1137300, 19)\n",
      "(1139976, 19)\n",
      "(1142652, 19)\n",
      "(1145328, 19)\n",
      "(1148004, 19)\n",
      "(1150680, 19)\n",
      "(1153356, 19)\n",
      "(1156032, 19)\n",
      "(1158708, 19)\n",
      "(1161384, 19)\n",
      "(1164060, 19)\n",
      "(1166736, 19)\n",
      "(1169412, 19)\n",
      "(1172088, 19)\n",
      "(1174764, 19)\n",
      "(1177440, 19)\n",
      "(1180116, 19)\n",
      "(1182792, 19)\n",
      "(1185468, 19)\n",
      "(1188144, 19)\n",
      "(1190820, 19)\n",
      "(1193496, 19)\n",
      "(1196172, 19)\n",
      "(1198848, 19)\n",
      "(1201524, 19)\n"
     ]
    }
   ],
   "source": [
    "#Generate price features\n",
    "\n",
    "data_px_features = None\n",
    "for asset_i in range(data_px.shape[1]):\n",
    "    \n",
    "    temp = data_px.iloc[:,asset_i:asset_i+1].copy()\n",
    "    temp.columns=['PX']\n",
    "    temp['rtn'] = ((temp.iloc[:,0]/temp.iloc[:,0].transform(lambda x: x.shift(1)))-1)\n",
    "    for i in (1,5,21,21*3,252):\n",
    "        temp['lag_'+str(i)] = temp.iloc[:,1].transform(lambda x: x.shift(i))\n",
    "        if (i==1):\n",
    "            continue\n",
    "        temp['rolling_mean_'+str(i)] = temp.iloc[:,1].transform(lambda x: x.shift(28).rolling(i).mean())\n",
    "        temp['rolling_std_'+str(i)]  = temp.iloc[:,1].transform(lambda x: x.shift(28).rolling(i).std())\n",
    "        temp['rolling_meanpx_'+str(i)] = temp.iloc[:,0].transform(lambda x: x.shift(28).rolling(i).mean())\n",
    "        \n",
    "    temp = temp.iloc[np.where(~temp.isnull().any(axis=1))[0][0]:]\n",
    "    \n",
    "    if data_px_features is None:\n",
    "        data_px_features = temp\n",
    "    else:\n",
    "        data_px_features = pd.concat([temp,data_px_features],axis=0,ignore_index=True)\n",
    "    print(data_px_features.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in macro variables\n",
    "\n",
    "#https://www.clevelandfed.org/indicators-and-data/inflation-nowcasting\n",
    "data_macro = pd.read_csv('data_cpi_nc_all.csv')[['obs_date','CCPI_INFLATION_YoY','CPI_INFLATION_YoY']]\n",
    "data_macro.columns=['Date','CCPI_INFLATION_YoY','CPI_INFLATION_YoY']\n",
    "\n",
    "#https://en.macromicro.me/charts/3997/global-ofr-fsi\n",
    "data_macro2 = pd.read_csv('data_ofr_all.csv')[['obs_date','ofr_usa','ofr_otherdm','ofr_em']]\n",
    "data_macro2.columns=['Date','FSI_USA','FSI_DM','FSI,EM']\n",
    "\n",
    "#https://en.macromicro.me/charts/22907/sp-sector-performance\n",
    "data_macro3 = pd.read_csv('data_sp_all.csv')\n",
    "data_macro3 = data_macro3[['obs_date']+list(data_macro3.columns[:-1][1:])]\n",
    "data_macro3.columns = ['Date']+list(['SEC_'+s for s in data_macro3.columns[1:].str.upper()])\n",
    "\n",
    "data_macro = pd.merge(data_macro,data_macro2,on=\"Date\",how=\"outer\")\n",
    "data_macro = pd.merge(data_macro,data_macro3,on=\"Date\",how=\"outer\")\n",
    "data_macro = data_macro[data_macro['Date']>='2014-01-03']\n",
    "data_macro = data_macro.sort_values('Date')\n",
    "\n",
    "data_macro = pd.merge(data_macro,pd.DataFrame({'Date':list(data_px.index.strftime('%Y-%m-%d'))}),on=\"Date\",how=\"right\")\n",
    "\n",
    "data_macro = data_macro.fillna(method='ffill')\n",
    "data_macro.index = pd.to_datetime(data_macro['Date'])\n",
    "del data_macro['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a universal Price Trend LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, test data given stock data and sequence length\n",
    "def load_data(stock, look_back):\n",
    "    data_raw = stock.values # convert to numpy array\n",
    "    test_set_size = int(np.round(0.2*data_raw.shape[0]));\n",
    "    train_set_size = data_raw.shape[0] - (test_set_size);    \n",
    "    \n",
    "    \n",
    "    scaler  = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler  = scaler.fit(data_raw[:train_set_size])\n",
    "    data_raw_scaled  = scaler.transform(data_raw)\n",
    "    \n",
    "    data = []\n",
    "    datay = []\n",
    "    total_look_back = look_back + oos_prediction_window\n",
    "\n",
    "    # create all possible sequences of length look_back\n",
    "    for index in range(len(data_raw_scaled) - total_look_back): \n",
    "        data.append(data_raw_scaled[index: index + total_look_back])\n",
    "\n",
    "    data = np.array(data)\n",
    "\n",
    "    x_train = data[:train_set_size,range(0,look_back),:]    \n",
    "    x_test = data[train_set_size:,range(0,look_back),:]\n",
    "    \n",
    "    del data\n",
    "    for index in range(len(data_raw_scaled) - total_look_back): \n",
    "        datay.append(data_raw[index: index + total_look_back,:1])\n",
    "    \n",
    "    datay = np.array(datay)\n",
    "    \n",
    "    y_train = datay[:train_set_size,range(look_back,total_look_back),0]\n",
    "    y_test = datay[train_set_size:,range(look_back,total_look_back),0]\n",
    "    \n",
    "    y_train = np.apply_along_axis(lambda x:((x[-1]/x[0])-1),1,y_train)\n",
    "    y_test = np.apply_along_axis(lambda x:((x[-1]/x[0])-1),1,y_test)\n",
    "\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test,scaler]\n",
    "\n",
    "look_back = 21*3 # choose sequence length\n",
    "oos_prediction_window = 21\n",
    "\n",
    "x_train, y_train, x_test, y_test,scaler = load_data(data_px_features, look_back)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  torch.Size([961219, 63, 19])\n",
      "y_train.shape =  torch.Size([961219, 1])\n",
      "x_test.shape =  torch.Size([240221, 63, 19])\n",
      "y_test.shape =  torch.Size([240221, 1])\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape([-1,1])\n",
    "y_test = y_test.reshape([-1,1])\n",
    "\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor).to(device)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor).to(device)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor).to(device)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor).to(device)\n",
    "\n",
    "\n",
    "\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ',y_train.shape)\n",
    "print('x_test.shape = ',x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "BATCH_SIZE = 512\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(19, 312, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (fc): Linear(in_features=312, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "#####################\n",
    "input_dim = x_train.shape[2]\n",
    "hidden_dim = 312\n",
    "num_layers = 2 \n",
    "\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,dropout=0.4).to(device)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim,32).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        #print(f'h0 size = {h0.size()}')\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        #print(f'c0 size = {c0.size()}')\n",
    "\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 32, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "    \n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Cumulative Train loss: 0.017376  [  512/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.087704  [51712/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.091214  [102912/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.064546  [154112/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.059369  [205312/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.065697  [256512/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.063293  [307712/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.055965  [358912/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.054512  [410112/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.053279  [461312/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.055461  [512512/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.051598  [563712/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.048258  [614912/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.050447  [666112/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.047672  [717312/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.047441  [768512/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.045265  [819712/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.050222  [870912/961219]\n",
      "Epoch 0. Cumulative Train loss: 0.051859  [922112/961219]\n",
      "Epoch  0 TRAIN-MSE:  0.05025168334176571  VAL-MSE: 0.06500374814297291 LR: 0.0001\n",
      "Epoch 1. Cumulative Train loss: 0.011250  [  512/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.011884  [51712/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.030635  [102912/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.024459  [154112/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.053883  [205312/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.051905  [256512/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.045121  [307712/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.044647  [358912/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.043803  [410112/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.052203  [461312/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.048265  [512512/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.048041  [563712/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.050311  [614912/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.049896  [666112/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.053792  [717312/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.050994  [768512/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.050320  [819712/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.053929  [870912/961219]\n",
      "Epoch 1. Cumulative Train loss: 0.051612  [922112/961219]\n",
      "Epoch  1 TRAIN-MSE:  0.04993386739811387  VAL-MSE: 0.06486754315964718 LR: 0.0001\n",
      "Epoch 2. Cumulative Train loss: 0.006599  [  512/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.010477  [51712/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.010966  [102912/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.044855  [154112/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.036588  [205312/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.053922  [256512/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.052194  [307712/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.051114  [358912/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.046184  [410112/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.049970  [461312/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.046192  [512512/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.048963  [563712/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.054978  [614912/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.056912  [666112/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.053750  [717312/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.051057  [768512/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.051466  [819712/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.053884  [870912/961219]\n",
      "Epoch 2. Cumulative Train loss: 0.051492  [922112/961219]\n",
      "Epoch  2 TRAIN-MSE:  0.04983838152138395  VAL-MSE: 0.06482372283935547 LR: 0.0001\n",
      "Epoch 3. Cumulative Train loss: 0.008021  [  512/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.012146  [51712/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.058132  [102912/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.054496  [154112/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.066497  [205312/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.061288  [256512/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.058440  [307712/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.056307  [358912/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.050677  [410112/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.049960  [461312/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.046103  [512512/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.043168  [563712/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.043781  [614912/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.041302  [666112/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.039209  [717312/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.045219  [768512/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.046265  [819712/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.046523  [870912/961219]\n",
      "Epoch 3. Cumulative Train loss: 0.048138  [922112/961219]\n",
      "Epoch  3 TRAIN-MSE:  0.04975662946317214  VAL-MSE: 0.06495733464017828 LR: 0.0001\n",
      "Epoch 4. Cumulative Train loss: 0.010836  [  512/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.048885  [51712/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.062659  [102912/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.077580  [154112/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.083593  [205312/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.078317  [256512/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.067311  [307712/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.059353  [358912/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.058347  [410112/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.058473  [461312/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.053736  [512512/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.052747  [563712/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.054497  [614912/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.051136  [666112/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.048257  [717312/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.047940  [768512/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.047589  [819712/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.045504  [870912/961219]\n",
      "Epoch 4. Cumulative Train loss: 0.048592  [922112/961219]\n",
      "Epoch  4 TRAIN-MSE:  0.04947047650464934  VAL-MSE: 0.06504910245854804 LR: 0.0001\n",
      "Epoch 5. Cumulative Train loss: 0.009685  [  512/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.011785  [51712/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.030253  [102912/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.034149  [154112/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.038181  [205312/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.039472  [256512/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.044257  [307712/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.039460  [358912/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.043541  [410112/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.040113  [461312/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.040301  [512512/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.037628  [563712/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.046097  [614912/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.043359  [666112/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.044340  [717312/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.044215  [768512/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.042172  [819712/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.042287  [870912/961219]\n",
      "Epoch 5. Cumulative Train loss: 0.046956  [922112/961219]\n",
      "Epoch  5 TRAIN-MSE:  0.04956811011909717  VAL-MSE: 0.06425257743673121 LR: 5e-05\n",
      "Epoch 6. Cumulative Train loss: 0.007602  [  512/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.082616  [51712/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.076456  [102912/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.070403  [154112/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.071116  [205312/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.058960  [256512/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.063204  [307712/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.055844  [358912/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.050263  [410112/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.049850  [461312/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.045880  [512512/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.045679  [563712/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.049016  [614912/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.049457  [666112/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.048702  [717312/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.046191  [768512/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.048430  [819712/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.046240  [870912/961219]\n",
      "Epoch 6. Cumulative Train loss: 0.050612  [922112/961219]\n",
      "Epoch  6 TRAIN-MSE:  0.048956777728623904  VAL-MSE: 0.06416731489465592 LR: 5e-05\n",
      "Epoch 7. Cumulative Train loss: 0.010937  [  512/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.010286  [51712/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.027566  [102912/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.045906  [154112/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.060730  [205312/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.057633  [256512/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.060457  [307712/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.064621  [358912/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.061483  [410112/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.059916  [461312/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.058721  [512512/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.057030  [563712/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.055717  [614912/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.054633  [666112/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.051483  [717312/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.050800  [768512/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.051210  [819712/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.050426  [870912/961219]\n",
      "Epoch 7. Cumulative Train loss: 0.048255  [922112/961219]\n",
      "Epoch  7 TRAIN-MSE:  0.04882889208944239  VAL-MSE: 0.06414736078140583 LR: 5e-05\n",
      "Epoch 8. Cumulative Train loss: 5.042528  [  512/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.062183  [51712/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.058811  [102912/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.042921  [154112/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.034654  [205312/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.044166  [256512/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.058403  [307712/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.051881  [358912/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.063010  [410112/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.064011  [461312/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.062241  [512512/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.057537  [563712/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.053603  [614912/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.052438  [666112/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.053855  [717312/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.050928  [768512/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.048375  [819712/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.048403  [870912/961219]\n",
      "Epoch 8. Cumulative Train loss: 0.048913  [922112/961219]\n",
      "Epoch  8 TRAIN-MSE:  0.04879480058821246  VAL-MSE: 0.06399554394661112 LR: 5e-05\n",
      "Epoch 9. Cumulative Train loss: 0.012609  [  512/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.010156  [51712/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.026473  [102912/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.021157  [154112/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.030181  [205312/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.033895  [256512/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.048674  [307712/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.043226  [358912/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.048521  [410112/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.044238  [461312/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.040959  [512512/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.051127  [563712/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.050926  [614912/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.050040  [666112/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.053778  [717312/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.050905  [768512/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.048405  [819712/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.048557  [870912/961219]\n",
      "Epoch 9. Cumulative Train loss: 0.048204  [922112/961219]\n",
      "Epoch  9 TRAIN-MSE:  0.04840090151363438  VAL-MSE: 0.06399489869462682 LR: 2.5e-05\n",
      "Epoch 10. Cumulative Train loss: 0.013383  [  512/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.009946  [51712/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.009860  [102912/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.010142  [154112/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.029526  [205312/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.047301  [256512/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.041208  [307712/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.050576  [358912/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.045399  [410112/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.041579  [461312/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.041130  [512512/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.044811  [563712/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.049975  [614912/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.055284  [666112/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.052039  [717312/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.049256  [768512/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.050715  [819712/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.050196  [870912/961219]\n",
      "Epoch 10. Cumulative Train loss: 0.049836  [922112/961219]\n",
      "Epoch  10 TRAIN-MSE:  0.04821332199950291  VAL-MSE: 0.06394849330820936 LR: 2.5e-05\n",
      "Epoch 11. Cumulative Train loss: 0.010481  [  512/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.010009  [51712/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.009626  [102912/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.029363  [154112/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.032287  [205312/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.035350  [256512/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.043123  [307712/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.038485  [358912/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.034865  [410112/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.032179  [461312/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.034530  [512512/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.037933  [563712/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.046584  [614912/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.043977  [666112/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.045950  [717312/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.043535  [768512/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.041482  [819712/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.045815  [870912/961219]\n",
      "Epoch 11. Cumulative Train loss: 0.045624  [922112/961219]\n",
      "Epoch  11 TRAIN-MSE:  0.04833095394426022  VAL-MSE: 0.06390477444263215 LR: 2.5e-05\n",
      "Epoch 12. Cumulative Train loss: 0.006567  [  512/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.040139  [51712/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.047053  [102912/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.035009  [154112/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.038852  [205312/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.040815  [256512/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.044048  [307712/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.048142  [358912/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.065489  [410112/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.062531  [461312/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.057443  [512512/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.056089  [563712/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.055078  [614912/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.054076  [666112/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.053597  [717312/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.050802  [768512/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.048338  [819712/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.049879  [870912/961219]\n",
      "Epoch 12. Cumulative Train loss: 0.047675  [922112/961219]\n",
      "Epoch  12 TRAIN-MSE:  0.048598385199505725  VAL-MSE: 0.06421075780340965 LR: 2.5e-05\n",
      "Epoch 13. Cumulative Train loss: 0.008776  [  512/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.042679  [51712/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.026165  [102912/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.029618  [154112/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.042754  [205312/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.036229  [256512/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.045100  [307712/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.044609  [358912/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.040215  [410112/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.049090  [461312/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.051536  [512512/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.053462  [563712/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.049896  [614912/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.051258  [666112/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.054835  [717312/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.053869  [768512/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.051221  [819712/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.048805  [870912/961219]\n",
      "Epoch 13. Cumulative Train loss: 0.046682  [922112/961219]\n",
      "Epoch  13 TRAIN-MSE:  0.047607802661211174  VAL-MSE: 0.07957127347905585 LR: 1.25e-05\n",
      "Epoch 14. Cumulative Train loss: 0.015302  [  512/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.041267  [51712/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.044729  [102912/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.032994  [154112/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.035772  [205312/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.030769  [256512/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.031998  [307712/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.038182  [358912/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.038635  [410112/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.045004  [461312/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.041572  [512512/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.045640  [563712/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.042725  [614912/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.052165  [666112/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.049339  [717312/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.046848  [768512/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.046666  [819712/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.044601  [870912/961219]\n",
      "Epoch 14. Cumulative Train loss: 0.047500  [922112/961219]\n",
      "Epoch  14 TRAIN-MSE:  0.04776447447025762  VAL-MSE: 0.06466715386573305 LR: 1.25e-05\n",
      "Epoch 15. Cumulative Train loss: 0.006379  [  512/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.011041  [51712/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.010942  [102912/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.023743  [154112/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.020727  [205312/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.018744  [256512/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.027299  [307712/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.031236  [358912/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.028560  [410112/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.030055  [461312/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.044969  [512512/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.047441  [563712/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.046969  [614912/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.049141  [666112/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.046384  [717312/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.046182  [768512/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.046655  [819712/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.046283  [870912/961219]\n",
      "Epoch 15. Cumulative Train loss: 0.046292  [922112/961219]\n",
      "Epoch  15 TRAIN-MSE:  0.04707590820683371  VAL-MSE: 0.06443518374828582 LR: 1.25e-05\n",
      "Epoch 16. Cumulative Train loss: 0.006826  [  512/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.072770  [51712/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.042219  [102912/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.042053  [154112/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.034135  [205312/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.042577  [256512/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.048440  [307712/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.043123  [358912/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.048584  [410112/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.048303  [461312/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.049023  [512512/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.053011  [563712/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.051792  [614912/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.054199  [666112/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.051104  [717312/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.050580  [768512/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.049907  [819712/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.047652  [870912/961219]\n",
      "Epoch 16. Cumulative Train loss: 0.047362  [922112/961219]\n",
      "Epoch  16 TRAIN-MSE:  0.04602819930040997  VAL-MSE: 0.06489843003293301 LR: 1.25e-05\n",
      "Epoch 17. Cumulative Train loss: 0.008598  [  512/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.010738  [51712/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.010295  [102912/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.040069  [154112/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.032662  [205312/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.037062  [256512/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.032769  [307712/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.038959  [358912/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.035435  [410112/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.036078  [461312/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.033626  [512512/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.037085  [563712/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.038233  [614912/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.041157  [666112/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.039095  [717312/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.037186  [768512/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.042132  [819712/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.047398  [870912/961219]\n",
      "Epoch 17. Cumulative Train loss: 0.047090  [922112/961219]\n",
      "Epoch  17 TRAIN-MSE:  0.048793480747168105  VAL-MSE: 0.06510177368813373 LR: 6.25e-06\n",
      "Epoch 18. Cumulative Train loss: 0.014937  [  512/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.076136  [51712/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.056430  [102912/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.041124  [154112/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.041699  [205312/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.048385  [256512/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.047153  [307712/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.041919  [358912/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.038179  [410112/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.047325  [461312/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.050854  [512512/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.052472  [563712/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.051078  [614912/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.050601  [666112/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.047819  [717312/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.045333  [768512/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.044957  [819712/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.047580  [870912/961219]\n",
      "Epoch 18. Cumulative Train loss: 0.047347  [922112/961219]\n",
      "Epoch  18 TRAIN-MSE:  0.04588264332046778  VAL-MSE: 0.06455234771079206 LR: 6.25e-06\n",
      "Epoch 19. Cumulative Train loss: 0.008472  [  512/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.011489  [51712/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.026804  [102912/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.031062  [154112/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.041549  [205312/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.041849  [256512/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.045489  [307712/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.048820  [358912/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.050560  [410112/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.050537  [461312/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.049477  [512512/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.048412  [563712/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.045210  [614912/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.044862  [666112/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.042483  [717312/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.042194  [768512/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.046129  [819712/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.047964  [870912/961219]\n",
      "Epoch 19. Cumulative Train loss: 0.045955  [922112/961219]\n",
      "Epoch  19 TRAIN-MSE:  0.044572202669044  VAL-MSE: 0.06459695126147981 LR: 6.25e-06\n",
      "Epoch 20. Cumulative Train loss: 0.007940  [  512/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.010711  [51712/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.011052  [102912/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.010872  [154112/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.036710  [205312/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.041395  [256512/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.041818  [307712/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.037527  [358912/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.038067  [410112/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.041518  [461312/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.041276  [512512/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.040634  [563712/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.038150  [614912/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.036142  [666112/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.040142  [717312/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.042672  [768512/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.046003  [819712/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.046929  [870912/961219]\n",
      "Epoch 20. Cumulative Train loss: 0.044927  [922112/961219]\n",
      "Epoch  20 TRAIN-MSE:  0.043472927765914805  VAL-MSE: 0.06623424367701754 LR: 6.25e-06\n",
      "Epoch 21. Cumulative Train loss: 0.009147  [  512/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.060836  [51712/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.063728  [102912/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.068256  [154112/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.054393  [205312/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.068113  [256512/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.058838  [307712/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.052175  [358912/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.046987  [410112/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.049949  [461312/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.046066  [512512/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.042819  [563712/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.040144  [614912/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.037918  [666112/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.037629  [717312/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.039471  [768512/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.039584  [819712/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.039515  [870912/961219]\n",
      "Epoch 21. Cumulative Train loss: 0.039416  [922112/961219]\n",
      "Epoch  21 TRAIN-MSE:  0.04217483878212051  VAL-MSE: 0.06551146811627327 LR: 3.125e-06\n",
      "Epoch 22. Cumulative Train loss: 0.008178  [  512/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.046963  [51712/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.029409  [102912/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.023125  [154112/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.020263  [205312/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.018087  [256512/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.026017  [307712/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.023804  [358912/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.022103  [410112/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.023654  [461312/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.026223  [512512/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.034562  [563712/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.032655  [614912/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.035267  [666112/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.035348  [717312/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.037439  [768512/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.038410  [819712/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.039558  [870912/961219]\n",
      "Epoch 22. Cumulative Train loss: 0.041648  [922112/961219]\n",
      "Epoch  22 TRAIN-MSE:  0.04167948153685219  VAL-MSE: 0.06550950801118892 LR: 3.125e-06\n",
      "Epoch 23. Cumulative Train loss: 0.009154  [  512/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.039984  [51712/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.055937  [102912/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.040603  [154112/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.045865  [205312/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.053738  [256512/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.054574  [307712/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.048320  [358912/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.043740  [410112/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.050634  [461312/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.046772  [512512/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.043412  [563712/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.040730  [614912/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.038451  [666112/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.038378  [717312/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.038974  [768512/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.039104  [819712/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.043615  [870912/961219]\n",
      "Epoch 23. Cumulative Train loss: 0.041831  [922112/961219]\n",
      "Epoch  23 TRAIN-MSE:  0.04168323644003469  VAL-MSE: 0.06520922640536694 LR: 3.125e-06\n",
      "Epoch 24. Cumulative Train loss: 0.008979  [  512/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.103124  [51712/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.105559  [102912/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.074133  [154112/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.064117  [205312/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.068037  [256512/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.058449  [307712/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.051440  [358912/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.046296  [410112/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.049076  [461312/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.045190  [512512/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.042086  [563712/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.039469  [614912/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.044888  [666112/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.044729  [717312/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.042593  [768512/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.042148  [819712/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.041744  [870912/961219]\n",
      "Epoch 24. Cumulative Train loss: 0.040037  [922112/961219]\n",
      "Epoch  24 TRAIN-MSE:  0.040752618632371576  VAL-MSE: 0.06574776020455868 LR: 3.125e-06\n",
      "Epoch 25. Cumulative Train loss: 0.007504  [  512/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.010229  [51712/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.030988  [102912/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.050261  [154112/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.046069  [205312/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.043675  [256512/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.038192  [307712/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.037815  [358912/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.034455  [410112/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.033994  [461312/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.031594  [512512/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.032086  [563712/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.030356  [614912/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.036573  [666112/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.034734  [717312/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.037851  [768512/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.040398  [819712/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.043551  [870912/961219]\n",
      "Epoch 25. Cumulative Train loss: 0.041704  [922112/961219]\n",
      "Epoch  25 TRAIN-MSE:  0.040434152730448016  VAL-MSE: 0.06527148713456823 LR: 1.5625e-06\n",
      "Epoch 26. Cumulative Train loss: 0.008501  [  512/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.010256  [51712/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.035624  [102912/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.027234  [154112/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.023041  [205312/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.026931  [256512/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.024153  [307712/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.029715  [358912/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.038409  [410112/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.038045  [461312/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.041875  [512512/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.043097  [563712/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.044784  [614912/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.042146  [666112/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.041692  [717312/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.039564  [768512/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.043086  [819712/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.043230  [870912/961219]\n",
      "Epoch 26. Cumulative Train loss: 0.041359  [922112/961219]\n",
      "Epoch  26 TRAIN-MSE:  0.040122390072098495  VAL-MSE: 0.06538535178975856 LR: 1.5625e-06\n",
      "Epoch 27. Cumulative Train loss: 0.011531  [  512/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.009350  [51712/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.030706  [102912/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.044131  [154112/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.042681  [205312/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.041337  [256512/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.036281  [307712/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.032678  [358912/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.032913  [410112/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.030477  [461312/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.034427  [512512/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.040095  [563712/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.042420  [614912/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.045894  [666112/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.043363  [717312/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.043863  [768512/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.043314  [819712/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.041526  [870912/961219]\n",
      "Epoch 27. Cumulative Train loss: 0.039848  [922112/961219]\n",
      "Epoch  27 TRAIN-MSE:  0.039867996708048595  VAL-MSE: 0.06681262888806931 LR: 1.5625e-06\n",
      "Epoch 28. Cumulative Train loss: 0.008577  [  512/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.011074  [51712/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.021219  [102912/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.017459  [154112/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.028826  [205312/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.032111  [256512/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.036477  [307712/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.032746  [358912/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.034309  [410112/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.034343  [461312/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.031902  [512512/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.034612  [563712/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.032593  [614912/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.036902  [666112/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.035080  [717312/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.039137  [768512/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.040317  [819712/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.038622  [870912/961219]\n",
      "Epoch 28. Cumulative Train loss: 0.040920  [922112/961219]\n",
      "Epoch  28 TRAIN-MSE:  0.03962580442376022  VAL-MSE: 0.06485372502753076 LR: 1.5625e-06\n",
      "Epoch 29. Cumulative Train loss: 0.007570  [  512/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.071019  [51712/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.040869  [102912/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.038156  [154112/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.031036  [205312/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.026935  [256512/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.028171  [307712/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.025664  [358912/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.023824  [410112/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.029472  [461312/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.030160  [512512/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.030497  [563712/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.033254  [614912/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.031579  [666112/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.037262  [717312/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.042242  [768512/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.040228  [819712/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.040371  [870912/961219]\n",
      "Epoch 29. Cumulative Train loss: 0.038663  [922112/961219]\n",
      "Epoch  29 TRAIN-MSE:  0.03938789222428141  VAL-MSE: 0.06521547601578083 LR: 7.8125e-07\n",
      "Epoch 30. Cumulative Train loss: 0.012892  [  512/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.010518  [51712/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.010302  [102912/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.019809  [154112/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.022808  [205312/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.033525  [256512/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.029692  [307712/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.030271  [358912/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.031705  [410112/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.032036  [461312/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.035666  [512512/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.033482  [563712/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.035853  [614912/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.033887  [666112/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.034021  [717312/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.034057  [768512/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.034465  [819712/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.032987  [870912/961219]\n",
      "Epoch 30. Cumulative Train loss: 0.035066  [922112/961219]\n",
      "Epoch  30 TRAIN-MSE:  0.03924844938149841  VAL-MSE: 0.06472742608252992 LR: 7.8125e-07\n",
      "Epoch 31. Cumulative Train loss: 0.008308  [  512/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.044151  [51712/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.045089  [102912/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.041779  [154112/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.033999  [205312/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.042280  [256512/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.041499  [307712/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.040397  [358912/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.041873  [410112/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.045166  [461312/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.041740  [512512/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.045751  [563712/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.045388  [614912/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.044991  [666112/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.045360  [717312/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.044691  [768512/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.042588  [819712/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.040648  [870912/961219]\n",
      "Epoch 31. Cumulative Train loss: 0.040345  [922112/961219]\n",
      "Epoch  31 TRAIN-MSE:  0.039159294976818124  VAL-MSE: 0.064679080881971 LR: 7.8125e-07\n",
      "Epoch 32. Cumulative Train loss: 0.043284  [  512/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.033806  [51712/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.031434  [102912/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.024710  [154112/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.021247  [205312/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.040437  [256512/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.035451  [307712/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.041368  [358912/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.040369  [410112/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.037047  [461312/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.034297  [512512/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.036503  [563712/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.037232  [614912/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.035250  [666112/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.036830  [717312/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.039129  [768512/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.040946  [819712/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.040348  [870912/961219]\n",
      "Epoch 32. Cumulative Train loss: 0.040306  [922112/961219]\n",
      "Epoch  32 TRAIN-MSE:  0.03909037962180976  VAL-MSE: 0.06462268423526846 LR: 7.8125e-07\n",
      "Epoch 33. Cumulative Train loss: 0.006661  [  512/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.033716  [51712/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.022233  [102912/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.047001  [154112/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.053904  [205312/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.050539  [256512/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.047418  [307712/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.051206  [358912/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.052110  [410112/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.047605  [461312/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.048097  [512512/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.044667  [563712/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.044886  [614912/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.043831  [666112/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.043111  [717312/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.044087  [768512/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.041984  [819712/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.041917  [870912/961219]\n",
      "Epoch 33. Cumulative Train loss: 0.040156  [922112/961219]\n",
      "Epoch  33 TRAIN-MSE:  0.03896434040724263  VAL-MSE: 0.0646631971318671 LR: 3.90625e-07\n",
      "Epoch 34. Cumulative Train loss: 0.010133  [  512/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.011510  [51712/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.034794  [102912/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.033819  [154112/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.027885  [205312/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.030265  [256512/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.032946  [307712/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.033063  [358912/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.037217  [410112/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.039324  [461312/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.042224  [512512/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.039290  [563712/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.039212  [614912/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.037026  [666112/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.039087  [717312/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.037140  [768512/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.038649  [819712/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.038454  [870912/961219]\n",
      "Epoch 34. Cumulative Train loss: 0.036863  [922112/961219]\n",
      "Epoch  34 TRAIN-MSE:  0.03893114107113462  VAL-MSE: 0.06472156200003117 LR: 3.90625e-07\n",
      "Epoch 35. Cumulative Train loss: 0.007310  [  512/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.045487  [51712/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.068832  [102912/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.049251  [154112/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.039762  [205312/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.038640  [256512/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.033974  [307712/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.042085  [358912/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.044582  [410112/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.040951  [461312/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.039695  [512512/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.036987  [563712/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.034697  [614912/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.034765  [666112/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.033037  [717312/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.038501  [768512/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.036728  [819712/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.035297  [870912/961219]\n",
      "Epoch 35. Cumulative Train loss: 0.038606  [922112/961219]\n",
      "Epoch  35 TRAIN-MSE:  0.0388236279332568  VAL-MSE: 0.06488375156483751 LR: 3.90625e-07\n",
      "Epoch 36. Cumulative Train loss: 0.010381  [  512/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.058572  [51712/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.055851  [102912/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.047364  [154112/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.048964  [205312/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.041316  [256512/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.041825  [307712/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.041453  [358912/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.041512  [410112/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.042466  [461312/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.044550  [512512/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.047966  [563712/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.044919  [614912/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.045887  [666112/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.045034  [717312/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.044180  [768512/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.043774  [819712/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.041766  [870912/961219]\n",
      "Epoch 36. Cumulative Train loss: 0.040043  [922112/961219]\n",
      "Epoch  36 TRAIN-MSE:  0.0387740562584636  VAL-MSE: 0.06486983197800657 LR: 3.90625e-07\n",
      "Epoch 37. Cumulative Train loss: 3.912279  [  512/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.049755  [51712/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.031086  [102912/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.063258  [154112/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.049946  [205312/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.046183  [256512/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.048058  [307712/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.052042  [358912/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.046850  [410112/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.042948  [461312/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.048361  [512512/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.044891  [563712/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.041915  [614912/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.042724  [666112/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.040383  [717312/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.038411  [768512/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.040049  [819712/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.039688  [870912/961219]\n",
      "Epoch 37. Cumulative Train loss: 0.039967  [922112/961219]\n",
      "Epoch  37 TRAIN-MSE:  0.03869493943450653  VAL-MSE: 0.06489999243553649 LR: 1.953125e-07\n",
      "Epoch 38. Cumulative Train loss: 0.009278  [  512/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.010279  [51712/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.043914  [102912/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.033084  [154112/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.046453  [205312/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.039281  [256512/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.034396  [307712/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.038796  [358912/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.043864  [410112/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.042690  [461312/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.039395  [512512/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.036847  [563712/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.034575  [614912/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.037764  [666112/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.037521  [717312/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.038819  [768512/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.037084  [819712/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.037032  [870912/961219]\n",
      "Epoch 38. Cumulative Train loss: 0.035524  [922112/961219]\n",
      "Epoch  38 TRAIN-MSE:  0.03869885394892222  VAL-MSE: 0.06496249259786403 LR: 1.953125e-07\n",
      "Epoch 39. Cumulative Train loss: 0.005876  [  512/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.070895  [51712/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.051549  [102912/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.044533  [154112/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.036119  [205312/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.030986  [256512/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.031372  [307712/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.028308  [358912/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.035106  [410112/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.042749  [461312/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.039437  [512512/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.036813  [563712/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.037196  [614912/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.037327  [666112/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.037407  [717312/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.038595  [768512/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.037017  [819712/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.037477  [870912/961219]\n",
      "Epoch 39. Cumulative Train loss: 0.037278  [922112/961219]\n",
      "Epoch  39 TRAIN-MSE:  0.03868032825780443  VAL-MSE: 0.06503940338784076 LR: 1.953125e-07\n",
      "Epoch 40. Cumulative Train loss: 0.009488  [  512/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.122044  [51712/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.066136  [102912/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.067334  [154112/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.053302  [205312/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.058869  [256512/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.054326  [307712/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.048316  [358912/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.051597  [410112/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.052740  [461312/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.048398  [512512/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.044885  [563712/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.049325  [614912/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.051087  [666112/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.048262  [717312/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.045727  [768512/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.043541  [819712/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.041636  [870912/961219]\n",
      "Epoch 40. Cumulative Train loss: 0.039985  [922112/961219]\n",
      "Epoch  40 TRAIN-MSE:  0.03879397466388968  VAL-MSE: 0.06491512947894157 LR: 1.953125e-07\n",
      "Epoch 41. Cumulative Train loss: 0.007638  [  512/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.046557  [51712/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.039509  [102912/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.045695  [154112/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.041701  [205312/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.046884  [256512/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.040966  [307712/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.036612  [358912/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.037175  [410112/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.042215  [461312/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.042968  [512512/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.044737  [563712/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.041893  [614912/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.039452  [666112/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.039122  [717312/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.037175  [768512/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.036800  [819712/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.035285  [870912/961219]\n",
      "Epoch 41. Cumulative Train loss: 0.037212  [922112/961219]\n",
      "Epoch  41 TRAIN-MSE:  0.03858052269769862  VAL-MSE: 0.06485930097864029 LR: 9.765625e-08\n",
      "Epoch 42. Cumulative Train loss: 0.008054  [  512/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.042032  [51712/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.051134  [102912/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.037995  [154112/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.042472  [205312/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.042017  [256512/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.046501  [307712/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.044651  [358912/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.040339  [410112/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.036956  [461312/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.040649  [512512/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.041457  [563712/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.038913  [614912/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.036779  [666112/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.036424  [717312/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.038469  [768512/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.042030  [819712/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.040176  [870912/961219]\n",
      "Epoch 42. Cumulative Train loss: 0.039783  [922112/961219]\n",
      "Epoch  42 TRAIN-MSE:  0.03857063647177114  VAL-MSE: 0.06487203963259433 LR: 9.765625e-08\n",
      "Epoch 43. Cumulative Train loss: 0.017305  [  512/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.056805  [51712/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.051254  [102912/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.044988  [154112/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.036638  [205312/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.038966  [256512/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.034401  [307712/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.036874  [358912/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.036566  [410112/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.036082  [461312/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.036280  [512512/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.036087  [563712/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.038137  [614912/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.038185  [666112/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.038521  [717312/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.039565  [768512/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.039964  [819712/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.038295  [870912/961219]\n",
      "Epoch 43. Cumulative Train loss: 0.039762  [922112/961219]\n",
      "Epoch  43 TRAIN-MSE:  0.03857040987176477  VAL-MSE: 0.06488164130677568 LR: 9.765625e-08\n",
      "Epoch 44. Cumulative Train loss: 0.008395  [  512/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.089307  [51712/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.049846  [102912/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.044189  [154112/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.042554  [205312/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.040942  [256512/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.043249  [307712/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.041970  [358912/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.041522  [410112/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.045451  [461312/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.044393  [512512/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.048158  [563712/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.045105  [614912/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.042540  [666112/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.041819  [717312/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.039698  [768512/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.037801  [819712/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.039364  [870912/961219]\n",
      "Epoch 44. Cumulative Train loss: 0.037782  [922112/961219]\n",
      "Epoch  44 TRAIN-MSE:  0.03858786134056239  VAL-MSE: 0.0649169718965571 LR: 9.765625e-08\n",
      "Epoch 45. Cumulative Train loss: 0.008928  [  512/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.032705  [51712/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.046355  [102912/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.044012  [154112/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.047445  [205312/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.040373  [256512/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.038886  [307712/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.040446  [358912/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.036905  [410112/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.034068  [461312/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.035069  [512512/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.036742  [563712/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.034522  [614912/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.037227  [666112/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.036913  [717312/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.039004  [768512/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.041872  [819712/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.040027  [870912/961219]\n",
      "Epoch 45. Cumulative Train loss: 0.039812  [922112/961219]\n",
      "Epoch  45 TRAIN-MSE:  0.038601440981554144  VAL-MSE: 0.0649108683809321 LR: 4.8828125e-08\n",
      "Epoch 46. Cumulative Train loss: 0.007965  [  512/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.034225  [51712/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.031924  [102912/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.044941  [154112/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.036251  [205312/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.031094  [256512/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.033242  [307712/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.029928  [358912/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.033779  [410112/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.042682  [461312/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.039488  [512512/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.043300  [563712/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.045591  [614912/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.044425  [666112/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.041983  [717312/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.042981  [768512/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.040958  [819712/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.042041  [870912/961219]\n",
      "Epoch 46. Cumulative Train loss: 0.040262  [922112/961219]\n",
      "Epoch  46 TRAIN-MSE:  0.03902274914828741  VAL-MSE: 0.06491603851318359 LR: 4.8828125e-08\n",
      "Epoch 47. Cumulative Train loss: 0.006356  [  512/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.060142  [51712/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.051779  [102912/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.047498  [154112/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.050071  [205312/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.046886  [256512/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.049484  [307712/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.053925  [358912/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.048338  [410112/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.044106  [461312/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.040670  [512512/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.041686  [563712/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.039114  [614912/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.038725  [666112/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.042598  [717312/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.041931  [768512/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.041413  [819712/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.039738  [870912/961219]\n",
      "Epoch 47. Cumulative Train loss: 0.038112  [922112/961219]\n",
      "Epoch  47 TRAIN-MSE:  0.03855336726250383  VAL-MSE: 0.06488906373368933 LR: 4.8828125e-08\n",
      "Epoch 48. Cumulative Train loss: 0.008847  [  512/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.030125  [51712/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.048911  [102912/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.036061  [154112/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.053824  [205312/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.045170  [256512/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.039287  [307712/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.035176  [358912/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.032113  [410112/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.034866  [461312/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.036035  [512512/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.033703  [563712/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.034017  [614912/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.034128  [666112/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.033890  [717312/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.035822  [768512/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.038830  [819712/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.038374  [870912/961219]\n",
      "Epoch 48. Cumulative Train loss: 0.036808  [922112/961219]\n",
      "Epoch  48 TRAIN-MSE:  0.03854573705542091  VAL-MSE: 0.06489121457363697 LR: 4.8828125e-08\n",
      "Epoch 49. Cumulative Train loss: 0.018495  [  512/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.082732  [51712/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.047101  [102912/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.035464  [154112/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.033941  [205312/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.029282  [256512/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.029947  [307712/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.035831  [358912/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.032679  [410112/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.032427  [461312/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.033587  [512512/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.040207  [563712/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.037723  [614912/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.037801  [666112/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.037414  [717312/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.039873  [768512/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.038036  [819712/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.038112  [870912/961219]\n",
      "Epoch 49. Cumulative Train loss: 0.039745  [922112/961219]\n",
      "Epoch  49 TRAIN-MSE:  0.03856266168649363  VAL-MSE: 0.06494938059056059 LR: 2.44140625e-08\n",
      "Epoch 50. Cumulative Train loss: 0.005498  [  512/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.040372  [51712/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.048930  [102912/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.045290  [154112/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.042187  [205312/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.042532  [256512/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.043035  [307712/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.038326  [358912/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.044689  [410112/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.041034  [461312/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.042493  [512512/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.039581  [563712/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.041398  [614912/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.039015  [666112/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.040919  [717312/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.042166  [768512/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.040157  [819712/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.040178  [870912/961219]\n",
      "Epoch 50. Cumulative Train loss: 0.039773  [922112/961219]\n",
      "Epoch  50 TRAIN-MSE:  0.038598247668115745  VAL-MSE: 0.06487161758098196 LR: 2.44140625e-08\n",
      "Epoch 51. Cumulative Train loss: 0.009315  [  512/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.010666  [51712/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.011491  [102912/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.010912  [154112/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.016012  [205312/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.024111  [256512/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.027721  [307712/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.034989  [358912/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.034275  [410112/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.040599  [461312/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.040547  [512512/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.037731  [563712/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.038098  [614912/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.035991  [666112/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.035550  [717312/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.035494  [768512/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.037577  [819712/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.038743  [870912/961219]\n",
      "Epoch 51. Cumulative Train loss: 0.037157  [922112/961219]\n",
      "Epoch  51 TRAIN-MSE:  0.038493722667312595  VAL-MSE: 0.06488101634573429 LR: 2.44140625e-08\n",
      "Epoch 52. Cumulative Train loss: 0.007667  [  512/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.009765  [51712/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.009539  [102912/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.024976  [154112/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.035822  [205312/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.030665  [256512/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.030538  [307712/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.039144  [358912/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.038143  [410112/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.040771  [461312/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.037742  [512512/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.038563  [563712/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.040562  [614912/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.038369  [666112/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.040712  [717312/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.038654  [768512/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.039988  [819712/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.038365  [870912/961219]\n",
      "Epoch 52. Cumulative Train loss: 0.038188  [922112/961219]\n",
      "Epoch  52 TRAIN-MSE:  0.03855194375107384  VAL-MSE: 0.06487905218246136 LR: 2.44140625e-08\n",
      "Epoch 53. Cumulative Train loss: 0.008389  [  512/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.044589  [51712/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.027822  [102912/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.022271  [154112/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.034313  [205312/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.029510  [256512/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.030214  [307712/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.033947  [358912/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.033926  [410112/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.038149  [461312/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.037625  [512512/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.039707  [563712/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.037234  [614912/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.042737  [666112/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.040376  [717312/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.038387  [768512/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.040155  [819712/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.041457  [870912/961219]\n",
      "Epoch 53. Cumulative Train loss: 0.039730  [922112/961219]\n",
      "Epoch  53 TRAIN-MSE:  0.03849658469398753  VAL-MSE: 0.06488112185863738 LR: 1.220703125e-08\n",
      "Epoch 54. Cumulative Train loss: 0.007451  [  512/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.030809  [51712/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.067606  [102912/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.063863  [154112/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.050755  [205312/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.048742  [256512/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.042407  [307712/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.041754  [358912/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.037917  [410112/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.037709  [461312/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.035173  [512512/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.037920  [563712/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.037562  [614912/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.035453  [666112/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.033725  [717312/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.036325  [768512/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.036148  [819712/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.037190  [870912/961219]\n",
      "Epoch 54. Cumulative Train loss: 0.038088  [922112/961219]\n",
      "Epoch  54 TRAIN-MSE:  0.03851834644739049  VAL-MSE: 0.06490252880339926 LR: 1.220703125e-08\n",
      "Epoch 55. Cumulative Train loss: 0.006656  [  512/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.039407  [51712/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.025295  [102912/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.030267  [154112/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.025521  [205312/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.031252  [256512/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.031620  [307712/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.028746  [358912/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.026308  [410112/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.024538  [461312/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.032910  [512512/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.032835  [563712/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.032559  [614912/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.043134  [666112/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.044846  [717312/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.042475  [768512/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.040503  [819712/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.040148  [870912/961219]\n",
      "Epoch 55. Cumulative Train loss: 0.038518  [922112/961219]\n",
      "Epoch  55 TRAIN-MSE:  0.038524833668511324  VAL-MSE: 0.06489208708418176 LR: 1.220703125e-08\n",
      "Epoch 56. Cumulative Train loss: 0.008805  [  512/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.010686  [51712/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.036768  [102912/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.035832  [154112/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.041947  [205312/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.040299  [256512/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.041130  [307712/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.040655  [358912/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.046762  [410112/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.045417  [461312/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.041935  [512512/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.041674  [563712/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.039162  [614912/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.038552  [666112/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.038237  [717312/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.040010  [768512/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.039664  [819712/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.037953  [870912/961219]\n",
      "Epoch 56. Cumulative Train loss: 0.038435  [922112/961219]\n",
      "Epoch  56 TRAIN-MSE:  0.03849156008397563  VAL-MSE: 0.0648917056144552 LR: 1.220703125e-08\n",
      "Epoch 57. Cumulative Train loss: 0.009020  [  512/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.010023  [51712/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.038937  [102912/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.029786  [154112/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.035598  [205312/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.030413  [256512/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.031048  [307712/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.031380  [358912/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.031856  [410112/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.034003  [461312/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.037862  [512512/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.039491  [563712/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.041696  [614912/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.041154  [666112/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.044309  [717312/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.042167  [768512/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.043584  [819712/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.041656  [870912/961219]\n",
      "Epoch 57. Cumulative Train loss: 0.039879  [922112/961219]\n",
      "Epoch  57 TRAIN-MSE:  0.038652351480899766  VAL-MSE: 0.06488405187079246 LR: 1.220703125e-08\n",
      "Epoch 58. Cumulative Train loss: 0.006136  [  512/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.054644  [51712/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.033087  [102912/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.037227  [154112/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.042035  [205312/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.040310  [256512/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.035551  [307712/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.049053  [358912/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.050653  [410112/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.046276  [461312/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.046972  [512512/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.045354  [563712/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.042429  [614912/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.039926  [666112/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.039847  [717312/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.039872  [768512/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.038071  [819712/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.036391  [870912/961219]\n",
      "Epoch 58. Cumulative Train loss: 0.037762  [922112/961219]\n",
      "Epoch  58 TRAIN-MSE:  0.03850712508748705  VAL-MSE: 0.0648856670298475 LR: 1.220703125e-08\n",
      "Epoch 59. Cumulative Train loss: 0.010625  [  512/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.010068  [51712/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.023577  [102912/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.033751  [154112/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.028175  [205312/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.035250  [256512/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.034899  [307712/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.038649  [358912/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.037828  [410112/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.039014  [461312/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.036157  [512512/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.039020  [563712/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.036706  [614912/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.037306  [666112/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.039023  [717312/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.038745  [768512/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.037064  [819712/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.040234  [870912/961219]\n",
      "Epoch 59. Cumulative Train loss: 0.039711  [922112/961219]\n",
      "Epoch  59 TRAIN-MSE:  0.038489801632647064  VAL-MSE: 0.06488912866470661 LR: 1.220703125e-08\n",
      "Epoch 60. Cumulative Train loss: 0.006263  [  512/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.070847  [51712/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.040995  [102912/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.054251  [154112/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.049183  [205312/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.041272  [256512/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.036113  [307712/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.032593  [358912/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.035773  [410112/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.035426  [461312/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.039421  [512512/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.038705  [563712/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.041589  [614912/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.039312  [666112/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.042646  [717312/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.041962  [768512/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.040014  [819712/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.039551  [870912/961219]\n",
      "Epoch 60. Cumulative Train loss: 0.039884  [922112/961219]\n",
      "Epoch  60 TRAIN-MSE:  0.03868670491040491  VAL-MSE: 0.06488860110019115 LR: 1.220703125e-08\n",
      "Epoch 61. Cumulative Train loss: 0.008430  [  512/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.010010  [51712/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.041383  [102912/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.031431  [154112/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.035258  [205312/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.044112  [256512/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.050170  [307712/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.050558  [358912/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.057511  [410112/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.052242  [461312/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.050919  [512512/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.047216  [563712/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.044145  [614912/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.041479  [666112/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.040789  [717312/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.040859  [768512/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.040545  [819712/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.039978  [870912/961219]\n",
      "Epoch 61. Cumulative Train loss: 0.039690  [922112/961219]\n",
      "Epoch  61 TRAIN-MSE:  0.03849312666028984  VAL-MSE: 0.0649083360712579 LR: 1.220703125e-08\n",
      "Epoch 62. Cumulative Train loss: 0.010231  [  512/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.066540  [51712/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.049923  [102912/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.036838  [154112/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.041359  [205312/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.049934  [256512/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.043373  [307712/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.046821  [358912/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.044764  [410112/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.043409  [461312/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.044127  [512512/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.043745  [563712/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.042989  [614912/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.043201  [666112/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.040728  [717312/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.038754  [768512/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.038908  [819712/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.037213  [870912/961219]\n",
      "Epoch 62. Cumulative Train loss: 0.038101  [922112/961219]\n",
      "Epoch  62 TRAIN-MSE:  0.038492332690438065  VAL-MSE: 0.06488832920155627 LR: 1.220703125e-08\n",
      "Epoch 63. Cumulative Train loss: 0.008182  [  512/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.072427  [51712/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.071055  [102912/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.062194  [154112/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.049425  [205312/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.041744  [256512/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.044285  [307712/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.042134  [358912/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.045389  [410112/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.044758  [461312/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.041381  [512512/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.041188  [563712/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.041073  [614912/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.040461  [666112/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.039888  [717312/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.042657  [768512/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.040607  [819712/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.039894  [870912/961219]\n",
      "Epoch 63. Cumulative Train loss: 0.038269  [922112/961219]\n",
      "Epoch  63 TRAIN-MSE:  0.0385106461550604  VAL-MSE: 0.06489571510477268 LR: 1.220703125e-08\n",
      "Epoch 64. Cumulative Train loss: 0.010491  [  512/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.055889  [51712/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.033638  [102912/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.025688  [154112/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.043787  [205312/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.047422  [256512/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.041178  [307712/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.041444  [358912/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.046802  [410112/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.045343  [461312/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.043870  [512512/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.040924  [563712/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.038485  [614912/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.041184  [666112/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.041532  [717312/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.039487  [768512/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.037718  [819712/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.038986  [870912/961219]\n",
      "Epoch 64. Cumulative Train loss: 0.038403  [922112/961219]\n",
      "Epoch  64 TRAIN-MSE:  0.03848144170432426  VAL-MSE: 0.0649152065845246 LR: 1.220703125e-08\n",
      "Epoch 65. Cumulative Train loss: 0.009664  [  512/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.010899  [51712/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.035536  [102912/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.034635  [154112/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.041194  [205312/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.039223  [256512/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.042134  [307712/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.042784  [358912/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.044676  [410112/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.040880  [461312/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.041303  [512512/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.040310  [563712/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.040577  [614912/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.041472  [666112/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.043122  [717312/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.043532  [768512/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.041536  [819712/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.039654  [870912/961219]\n",
      "Epoch 65. Cumulative Train loss: 0.039684  [922112/961219]\n",
      "Epoch  65 TRAIN-MSE:  0.03850363669579767  VAL-MSE: 0.06491361171641248 LR: 1.220703125e-08\n",
      "Epoch 66. Cumulative Train loss: 0.009106  [  512/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.045566  [51712/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.028703  [102912/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.047014  [154112/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.045300  [205312/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.038490  [256512/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.043073  [307712/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.038523  [358912/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.034988  [410112/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.039087  [461312/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.041090  [512512/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.040047  [563712/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.039450  [614912/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.037261  [666112/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.039896  [717312/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.040916  [768512/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.040501  [819712/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.038733  [870912/961219]\n",
      "Epoch 66. Cumulative Train loss: 0.038405  [922112/961219]\n",
      "Epoch  66 TRAIN-MSE:  0.03847731898426692  VAL-MSE: 0.06490278852746842 LR: 1.220703125e-08\n",
      "Epoch 67. Cumulative Train loss: 0.008542  [  512/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.066921  [51712/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.038259  [102912/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.036065  [154112/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.029461  [205312/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.032658  [256512/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.028849  [307712/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.032744  [358912/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.032810  [410112/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.033616  [461312/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.035181  [512512/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.037259  [563712/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.037388  [614912/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.039588  [666112/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.037563  [717312/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.037249  [768512/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.038545  [819712/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.038629  [870912/961219]\n",
      "Epoch 67. Cumulative Train loss: 0.037060  [922112/961219]\n",
      "Epoch  67 TRAIN-MSE:  0.03849923485122359  VAL-MSE: 0.06491337634147482 LR: 1.220703125e-08\n",
      "Epoch 68. Cumulative Train loss: 0.007595  [  512/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.009382  [51712/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.029841  [102912/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.050755  [154112/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.040659  [205312/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.040168  [256512/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.042339  [307712/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.044396  [358912/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.043199  [410112/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.039720  [461312/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.041496  [512512/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.038706  [563712/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.038901  [614912/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.038402  [666112/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.038574  [717312/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.040483  [768512/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.038622  [819712/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.039064  [870912/961219]\n",
      "Epoch 68. Cumulative Train loss: 0.039588  [922112/961219]\n",
      "Epoch  68 TRAIN-MSE:  0.03848463802348477  VAL-MSE: 0.06491323836306308 LR: 1.220703125e-08\n",
      "Epoch 69. Cumulative Train loss: 0.006897  [  512/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.009721  [51712/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.022304  [102912/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.042770  [154112/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.047626  [205312/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.049653  [256512/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.052912  [307712/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.050307  [358912/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.049446  [410112/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.045134  [461312/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.043729  [512512/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.043988  [563712/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.042966  [614912/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.040404  [666112/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.040323  [717312/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.042253  [768512/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.043334  [819712/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.041389  [870912/961219]\n",
      "Epoch 69. Cumulative Train loss: 0.039670  [922112/961219]\n",
      "Epoch  69 TRAIN-MSE:  0.03853844618449569  VAL-MSE: 0.06490927757100856 LR: 1.220703125e-08\n",
      "Epoch 70. Cumulative Train loss: 0.009154  [  512/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.069072  [51712/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.039328  [102912/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.030501  [154112/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.037287  [205312/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.037767  [256512/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.033311  [307712/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036564  [358912/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.039993  [410112/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036782  [461312/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036988  [512512/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036668  [563712/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036204  [614912/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.034149  [666112/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.034072  [717312/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036631  [768512/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.036487  [819712/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.037671  [870912/961219]\n",
      "Epoch 70. Cumulative Train loss: 0.039725  [922112/961219]\n",
      "Epoch  70 TRAIN-MSE:  0.03857494274568888  VAL-MSE: 0.06489979358429604 LR: 1.220703125e-08\n",
      "Epoch 71. Cumulative Train loss: 0.008998  [  512/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.009632  [51712/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.010254  [102912/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.032766  [154112/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.038749  [205312/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.040978  [256512/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.047368  [307712/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.047018  [358912/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.048580  [410112/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.044506  [461312/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.046465  [512512/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.043225  [563712/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.042734  [614912/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.041695  [666112/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.040900  [717312/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.040472  [768512/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.041449  [819712/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.039608  [870912/961219]\n",
      "Epoch 71. Cumulative Train loss: 0.039598  [922112/961219]\n",
      "Epoch  71 TRAIN-MSE:  0.038453954848965695  VAL-MSE: 0.06490795460153133 LR: 1.220703125e-08\n",
      "Epoch 72. Cumulative Train loss: 0.009178  [  512/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.011648  [51712/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.022880  [102912/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.026367  [154112/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.032479  [205312/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.027976  [256512/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.032794  [307712/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.037214  [358912/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.036677  [410112/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.041698  [461312/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.038598  [512512/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.042899  [563712/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.045054  [614912/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.044621  [666112/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.042147  [717312/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.040022  [768512/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.041384  [819712/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.039698  [870912/961219]\n",
      "Epoch 72. Cumulative Train loss: 0.038113  [922112/961219]\n",
      "Epoch  72 TRAIN-MSE:  0.03846882434122471  VAL-MSE: 0.06490248416332488 LR: 1.220703125e-08\n",
      "Epoch 73. Cumulative Train loss: 0.006484  [  512/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.063434  [51712/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.059797  [102912/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.055315  [154112/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.066101  [205312/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.055207  [256512/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.047515  [307712/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.049250  [358912/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.044463  [410112/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.047360  [461312/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.050334  [512512/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.046639  [563712/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.045550  [614912/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.044643  [666112/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.044318  [717312/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.043354  [768512/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.041322  [819712/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.039510  [870912/961219]\n",
      "Epoch 73. Cumulative Train loss: 0.037842  [922112/961219]\n",
      "Epoch  73 TRAIN-MSE:  0.03853544451472874  VAL-MSE: 0.06490878653019033 LR: 1.220703125e-08\n",
      "Epoch 74. Cumulative Train loss: 0.005943  [  512/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.010034  [51712/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.021579  [102912/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.026948  [154112/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.027739  [205312/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.024290  [256512/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.021987  [307712/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.034281  [358912/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.035726  [410112/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.036624  [461312/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.036982  [512512/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.043475  [563712/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.040783  [614912/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.044589  [666112/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.042154  [717312/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.040057  [768512/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.039449  [819712/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.039127  [870912/961219]\n",
      "Epoch 74. Cumulative Train loss: 0.037518  [922112/961219]\n",
      "Epoch  74 TRAIN-MSE:  0.03847498363464143  VAL-MSE: 0.06491665129965925 LR: 1.220703125e-08\n",
      "Epoch 75. Cumulative Train loss: 0.007973  [  512/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.079084  [51712/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.056417  [102912/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.041071  [154112/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.039375  [205312/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.033671  [256512/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.029978  [307712/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.031248  [358912/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.028733  [410112/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.032337  [461312/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.033656  [512512/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.035493  [563712/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.039534  [614912/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.038801  [666112/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.040501  [717312/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.038520  [768512/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.038567  [819712/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.038331  [870912/961219]\n",
      "Epoch 75. Cumulative Train loss: 0.039679  [922112/961219]\n",
      "Epoch  75 TRAIN-MSE:  0.03848930990325408  VAL-MSE: 0.06492184578104222 LR: 1.220703125e-08\n",
      "Epoch 76. Cumulative Train loss: 2.312677  [  512/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.032609  [51712/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.047331  [102912/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.055457  [154112/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.054242  [205312/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.057314  [256512/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.053262  [307712/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.050002  [358912/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.052239  [410112/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.047602  [461312/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.043807  [512512/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.046792  [563712/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.043931  [614912/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.041288  [666112/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.039084  [717312/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.039200  [768512/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.040461  [819712/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.038732  [870912/961219]\n",
      "Epoch 76. Cumulative Train loss: 0.039770  [922112/961219]\n",
      "Epoch  76 TRAIN-MSE:  0.03853514117185776  VAL-MSE: 0.064911014475721 LR: 1.220703125e-08\n",
      "Epoch 77. Cumulative Train loss: 0.007844  [  512/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.010922  [51712/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.010806  [102912/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.035920  [154112/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.029210  [205312/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.033184  [256512/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.038614  [307712/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.034458  [358912/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.034470  [410112/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.037068  [461312/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.034465  [512512/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.034140  [563712/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.034168  [614912/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.038473  [666112/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.036574  [717312/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.034800  [768512/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.037834  [819712/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.036304  [870912/961219]\n",
      "Epoch 77. Cumulative Train loss: 0.038310  [922112/961219]\n",
      "Epoch  77 TRAIN-MSE:  0.038503959506682665  VAL-MSE: 0.0649182177604513 LR: 1.220703125e-08\n",
      "Epoch 78. Cumulative Train loss: 0.004812  [  512/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.032943  [51712/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.021109  [102912/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.017499  [154112/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.016150  [205312/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.019043  [256512/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039799  [307712/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039802  [358912/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.040828  [410112/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.037486  [461312/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.034732  [512512/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039361  [563712/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039894  [614912/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039703  [666112/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039286  [717312/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.038598  [768512/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.039632  [819712/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.037902  [870912/961219]\n",
      "Epoch 78. Cumulative Train loss: 0.037540  [922112/961219]\n",
      "Epoch  78 TRAIN-MSE:  0.03852537242473521  VAL-MSE: 0.0649191917257106 LR: 1.220703125e-08\n",
      "Epoch 79. Cumulative Train loss: 0.016305  [  512/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.114114  [51712/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.091286  [102912/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.064346  [154112/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.056659  [205312/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.069259  [256512/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.065160  [307712/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.057203  [358912/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.054454  [410112/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.049702  [461312/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.048644  [512512/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.045140  [563712/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.042267  [614912/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.039800  [666112/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.037721  [717312/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.037232  [768512/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.038292  [819712/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.037758  [870912/961219]\n",
      "Epoch 79. Cumulative Train loss: 0.036255  [922112/961219]\n",
      "Epoch  79 TRAIN-MSE:  0.038495382745586594  VAL-MSE: 0.06492734868475732 LR: 1.220703125e-08\n",
      "Epoch 80. Cumulative Train loss: 0.006483  [  512/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.011437  [51712/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.030170  [102912/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.023715  [154112/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.032087  [205312/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.038420  [256512/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.033599  [307712/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.039954  [358912/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.038845  [410112/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.041519  [461312/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.040822  [512512/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.040533  [563712/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.040935  [614912/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.040334  [666112/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.040203  [717312/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.038218  [768512/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.036547  [819712/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.039637  [870912/961219]\n",
      "Epoch 80. Cumulative Train loss: 0.039680  [922112/961219]\n",
      "Epoch  80 TRAIN-MSE:  0.0384476667300315  VAL-MSE: 0.06492813597334192 LR: 1.220703125e-08\n",
      "Epoch 81. Cumulative Train loss: 0.007669  [  512/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.032267  [51712/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.021605  [102912/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.029623  [154112/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.039271  [205312/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.038454  [256512/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.044822  [307712/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.040009  [358912/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.036180  [410112/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.041947  [461312/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.043219  [512512/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.040272  [563712/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.039704  [614912/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.039257  [666112/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.040644  [717312/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.038634  [768512/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.038324  [819712/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.038340  [870912/961219]\n",
      "Epoch 81. Cumulative Train loss: 0.038603  [922112/961219]\n",
      "Epoch  81 TRAIN-MSE:  0.03848131291897565  VAL-MSE: 0.06493090771614236 LR: 1.220703125e-08\n",
      "Epoch 82. Cumulative Train loss: 0.009422  [  512/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.065140  [51712/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.038581  [102912/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.040898  [154112/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.033196  [205312/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.037375  [256512/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.036936  [307712/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.037705  [358912/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.039569  [410112/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.041460  [461312/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.043820  [512512/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.044295  [563712/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.043522  [614912/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.043269  [666112/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.040862  [717312/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.040339  [768512/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.038477  [819712/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.037949  [870912/961219]\n",
      "Epoch 82. Cumulative Train loss: 0.039663  [922112/961219]\n",
      "Epoch  82 TRAIN-MSE:  0.038475057280536136  VAL-MSE: 0.06493611843027967 LR: 1.220703125e-08\n",
      "Epoch 83. Cumulative Train loss: 0.018282  [  512/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.010604  [51712/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.025273  [102912/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.020859  [154112/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.024286  [205312/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.021519  [256512/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.022805  [307712/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.027298  [358912/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.025148  [410112/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.031260  [461312/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.029214  [512512/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.029532  [563712/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.035263  [614912/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.033423  [666112/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.033267  [717312/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.034201  [768512/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.036276  [819712/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.036461  [870912/961219]\n",
      "Epoch 83. Cumulative Train loss: 0.038438  [922112/961219]\n",
      "Epoch  83 TRAIN-MSE:  0.038508907750475045  VAL-MSE: 0.0649668348596451 LR: 1.220703125e-08\n",
      "Epoch 84. Cumulative Train loss: 0.010386  [  512/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.010108  [51712/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.021979  [102912/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.040529  [154112/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.033433  [205312/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.034361  [256512/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.030556  [307712/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.031084  [358912/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.043646  [410112/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.039889  [461312/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.036822  [512512/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.039343  [563712/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.043719  [614912/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.041135  [666112/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.038899  [717312/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.038595  [768512/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.038073  [819712/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.036423  [870912/961219]\n",
      "Epoch 84. Cumulative Train loss: 0.036515  [922112/961219]\n",
      "Epoch  84 TRAIN-MSE:  0.03846718511232735  VAL-MSE: 0.06495319934601479 LR: 1.220703125e-08\n",
      "Epoch 85. Cumulative Train loss: 0.005776  [  512/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.083327  [51712/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.075547  [102912/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.053941  [154112/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.055795  [205312/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.052630  [256512/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.050239  [307712/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.044682  [358912/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.043259  [410112/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.042001  [461312/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.038815  [512512/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.039474  [563712/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.037009  [614912/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.040274  [666112/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.039935  [717312/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.040694  [768512/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.038798  [819712/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.037121  [870912/961219]\n",
      "Epoch 85. Cumulative Train loss: 0.037750  [922112/961219]\n",
      "Epoch  85 TRAIN-MSE:  0.038447421351328684  VAL-MSE: 0.06494529399466008 LR: 1.220703125e-08\n",
      "Epoch 86. Cumulative Train loss: 0.011386  [  512/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.010470  [51712/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.010886  [102912/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.010947  [154112/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.039845  [205312/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.043149  [256512/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.042640  [307712/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.044609  [358912/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.046241  [410112/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.042220  [461312/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.041415  [512512/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.040475  [563712/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.040801  [614912/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.038433  [666112/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.036423  [717312/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.036554  [768512/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.037153  [819712/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.036977  [870912/961219]\n",
      "Epoch 86. Cumulative Train loss: 0.035581  [922112/961219]\n",
      "Epoch  86 TRAIN-MSE:  0.038444857178275754  VAL-MSE: 0.06494088274367313 LR: 1.220703125e-08\n",
      "Epoch 87. Cumulative Train loss: 0.011712  [  512/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.029802  [51712/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.032340  [102912/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.036672  [154112/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.038902  [205312/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.033227  [256512/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.038123  [307712/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.038851  [358912/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.035247  [410112/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.032491  [461312/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.030316  [512512/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.031166  [563712/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.031404  [614912/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.033270  [666112/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.033066  [717312/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.033176  [768512/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.034579  [819712/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.034546  [870912/961219]\n",
      "Epoch 87. Cumulative Train loss: 0.036328  [922112/961219]\n",
      "Epoch  87 TRAIN-MSE:  0.038463717636962734  VAL-MSE: 0.0649306317593189 LR: 1.220703125e-08\n",
      "Epoch 88. Cumulative Train loss: 0.009015  [  512/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.010028  [51712/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.021379  [102912/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.035718  [154112/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.034072  [205312/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.035406  [256512/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.035194  [307712/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.038230  [358912/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.043391  [410112/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.041996  [461312/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.045156  [512512/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.045575  [563712/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.042844  [614912/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.040343  [666112/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.039793  [717312/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.039339  [768512/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.039043  [819712/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.039817  [870912/961219]\n",
      "Epoch 88. Cumulative Train loss: 0.039587  [922112/961219]\n",
      "Epoch  88 TRAIN-MSE:  0.03843001369758845  VAL-MSE: 0.06493287593760388 LR: 1.220703125e-08\n",
      "Epoch 89. Cumulative Train loss: 0.008673  [  512/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.010079  [51712/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.010725  [102912/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.022257  [154112/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.023988  [205312/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.025773  [256512/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.023244  [307712/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.021445  [358912/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.022768  [410112/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.028847  [461312/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.027033  [512512/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.032185  [563712/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.032894  [614912/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.032926  [666112/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.033009  [717312/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.034949  [768512/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.033532  [819712/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.035904  [870912/961219]\n",
      "Epoch 89. Cumulative Train loss: 0.035743  [922112/961219]\n",
      "Epoch  89 TRAIN-MSE:  0.03847960067870288  VAL-MSE: 0.06495563831735164 LR: 1.220703125e-08\n",
      "Epoch 90. Cumulative Train loss: 0.005682  [  512/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.011038  [51712/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.021432  [102912/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.024264  [154112/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.034512  [205312/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.029830  [256512/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.030389  [307712/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.027685  [358912/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.027848  [410112/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.028432  [461312/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.028779  [512512/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.033902  [563712/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.034804  [614912/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.035464  [666112/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.033624  [717312/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.033769  [768512/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.037962  [819712/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.036342  [870912/961219]\n",
      "Epoch 90. Cumulative Train loss: 0.034915  [922112/961219]\n",
      "Epoch  90 TRAIN-MSE:  0.0384393652898078  VAL-MSE: 0.06493660541290933 LR: 1.220703125e-08\n",
      "Epoch 91. Cumulative Train loss: 0.009131  [  512/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.028647  [51712/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.048610  [102912/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.045316  [154112/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.046292  [205312/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.043671  [256512/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.042009  [307712/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.048323  [358912/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.043516  [410112/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.042510  [461312/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.042323  [512512/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.039457  [563712/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.037052  [614912/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.037652  [666112/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.038068  [717312/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.036234  [768512/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.037442  [819712/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.037109  [870912/961219]\n",
      "Epoch 91. Cumulative Train loss: 0.036953  [922112/961219]\n",
      "Epoch  91 TRAIN-MSE:  0.0384397937140613  VAL-MSE: 0.06496590147627161 LR: 1.220703125e-08\n",
      "Epoch 92. Cumulative Train loss: 0.008759  [  512/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.029557  [51712/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.019380  [102912/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.025809  [154112/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.028030  [205312/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.029156  [256512/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.039273  [307712/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.038058  [358912/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.034545  [410112/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.031789  [461312/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.039411  [512512/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.036838  [563712/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.036535  [614912/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.041219  [666112/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.040735  [717312/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.040275  [768512/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.039811  [819712/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.039919  [870912/961219]\n",
      "Epoch 92. Cumulative Train loss: 0.038240  [922112/961219]\n",
      "Epoch  92 TRAIN-MSE:  0.0384413670288266  VAL-MSE: 0.06493860204169091 LR: 1.220703125e-08\n",
      "Epoch 93. Cumulative Train loss: 0.008514  [  512/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.038610  [51712/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.048259  [102912/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.050629  [154112/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.040353  [205312/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.039059  [256512/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.050680  [307712/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.048406  [358912/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.043549  [410112/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.047028  [461312/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.045542  [512512/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.044349  [563712/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.043257  [614912/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.044050  [666112/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.044399  [717312/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.043904  [768512/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.041817  [819712/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.041390  [870912/961219]\n",
      "Epoch 93. Cumulative Train loss: 0.039714  [922112/961219]\n",
      "Epoch  93 TRAIN-MSE:  0.03848604246834144  VAL-MSE: 0.06494378434850814 LR: 1.220703125e-08\n",
      "Epoch 94. Cumulative Train loss: 0.007591  [  512/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.031517  [51712/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.033105  [102912/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.025577  [154112/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.051853  [205312/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.057327  [256512/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.055070  [307712/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.049173  [358912/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.044387  [410112/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.043994  [461312/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.040572  [512512/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.045715  [563712/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.045128  [614912/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.045172  [666112/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.042698  [717312/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.043906  [768512/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.041776  [819712/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.039888  [870912/961219]\n",
      "Epoch 94. Cumulative Train loss: 0.039781  [922112/961219]\n",
      "Epoch  94 TRAIN-MSE:  0.038588585356831615  VAL-MSE: 0.06493372815720579 LR: 1.220703125e-08\n",
      "Epoch 95. Cumulative Train loss: 0.006315  [  512/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.010858  [51712/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.026960  [102912/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.028568  [154112/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.024231  [205312/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.026309  [256512/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.027588  [307712/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.033221  [358912/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.038026  [410112/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.042421  [461312/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.039175  [512512/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.039112  [563712/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.040021  [614912/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.040035  [666112/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.037903  [717312/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.039081  [768512/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.037286  [819712/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.035632  [870912/961219]\n",
      "Epoch 95. Cumulative Train loss: 0.038501  [922112/961219]\n",
      "Epoch  95 TRAIN-MSE:  0.03846553368201327  VAL-MSE: 0.06492872846887467 LR: 1.220703125e-08\n",
      "Epoch 96. Cumulative Train loss: 0.008688  [  512/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.010402  [51712/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.038314  [102912/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.037534  [154112/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.045543  [205312/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.042804  [256512/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.047018  [307712/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.046954  [358912/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.042316  [410112/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.038767  [461312/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.038281  [512512/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.039632  [563712/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.037225  [614912/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.036878  [666112/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.038918  [717312/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.039024  [768512/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.040217  [819712/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.038499  [870912/961219]\n",
      "Epoch 96. Cumulative Train loss: 0.038216  [922112/961219]\n",
      "Epoch  96 TRAIN-MSE:  0.03842438007923115  VAL-MSE: 0.06493225503475109 LR: 1.220703125e-08\n",
      "Epoch 97. Cumulative Train loss: 0.008301  [  512/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.031870  [51712/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.033515  [102912/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.041478  [154112/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.047556  [205312/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.040089  [256512/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.035082  [307712/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.031879  [358912/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.032898  [410112/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.036759  [461312/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.036410  [512512/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.038059  [563712/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.035775  [614912/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.038198  [666112/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.041748  [717312/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.041849  [768512/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.041363  [819712/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.039580  [870912/961219]\n",
      "Epoch 97. Cumulative Train loss: 0.037995  [922112/961219]\n",
      "Epoch  97 TRAIN-MSE:  0.038436520511445145  VAL-MSE: 0.06493234431489985 LR: 1.220703125e-08\n",
      "Epoch 98. Cumulative Train loss: 0.009813  [  512/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.076281  [51712/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.055288  [102912/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.040539  [154112/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.038767  [205312/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.033101  [256512/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.042963  [307712/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.042247  [358912/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.038133  [410112/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.035072  [461312/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.032659  [512512/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.033317  [563712/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.034322  [614912/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.034351  [666112/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.038432  [717312/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.036611  [768512/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.036997  [819712/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.037182  [870912/961219]\n",
      "Epoch 98. Cumulative Train loss: 0.036769  [922112/961219]\n",
      "Epoch  98 TRAIN-MSE:  0.03845926357831913  VAL-MSE: 0.06493739270149393 LR: 1.220703125e-08\n",
      "Epoch 99. Cumulative Train loss: 0.006938  [  512/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.047649  [51712/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.040161  [102912/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.030381  [154112/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.034255  [205312/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.029609  [256512/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.026388  [307712/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.024343  [358912/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.022709  [410112/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.028053  [461312/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.028637  [512512/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.029608  [563712/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.034678  [614912/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.034342  [666112/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.036259  [717312/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.036073  [768512/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.037907  [819712/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.037634  [870912/961219]\n",
      "Epoch 99. Cumulative Train loss: 0.039674  [922112/961219]\n",
      "Epoch  99 TRAIN-MSE:  0.03845067246999548  VAL-MSE: 0.06494069606699843 LR: 1.220703125e-08\n",
      "Epoch 100. Cumulative Train loss: 0.007930  [  512/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.011450  [51712/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.011521  [102912/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.034473  [154112/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.039483  [205312/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.038282  [256512/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.033655  [307712/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.030241  [358912/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.032174  [410112/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.032367  [461312/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.032342  [512512/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.036067  [563712/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.038048  [614912/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.045499  [666112/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.043114  [717312/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.040923  [768512/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.040819  [819712/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.040270  [870912/961219]\n",
      "Epoch 100. Cumulative Train loss: 0.038581  [922112/961219]\n",
      "Epoch  100 TRAIN-MSE:  0.038455440989607216  VAL-MSE: 0.0649324985260659 LR: 1.220703125e-08\n",
      "Epoch 101. Cumulative Train loss: 0.007107  [  512/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.011729  [51712/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.011161  [102912/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.026061  [154112/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.037605  [205312/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.036390  [256512/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.032266  [307712/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.029261  [358912/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.030494  [410112/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.032066  [461312/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.032161  [512512/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.033339  [563712/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.038055  [614912/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.041297  [666112/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.040769  [717312/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.042529  [768512/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.041910  [819712/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.040082  [870912/961219]\n",
      "Epoch 101. Cumulative Train loss: 0.038490  [922112/961219]\n",
      "Epoch  101 TRAIN-MSE:  0.038429508904139965  VAL-MSE: 0.06493480357717961 LR: 1.220703125e-08\n",
      "Epoch 102. Cumulative Train loss: 0.008928  [  512/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.066832  [51712/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.048713  [102912/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.043569  [154112/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.043182  [205312/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.036493  [256512/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.032287  [307712/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.038386  [358912/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.037866  [410112/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.037186  [461312/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.039010  [512512/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.042311  [563712/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.041293  [614912/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.039015  [666112/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.040113  [717312/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.041975  [768512/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.040039  [819712/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.039580  [870912/961219]\n",
      "Epoch 102. Cumulative Train loss: 0.037979  [922112/961219]\n",
      "Epoch  102 TRAIN-MSE:  0.0384414550849297  VAL-MSE: 0.06492922356788149 LR: 1.220703125e-08\n",
      "Epoch 103. Cumulative Train loss: 0.006503  [  512/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.032021  [51712/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.039600  [102912/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.029969  [154112/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.033792  [205312/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.034046  [256512/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.033431  [307712/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.033341  [358912/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.042005  [410112/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.044310  [461312/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.040961  [512512/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.038193  [563712/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.039684  [614912/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.040798  [666112/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.038582  [717312/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.043950  [768512/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.041898  [819712/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.040039  [870912/961219]\n",
      "Epoch 103. Cumulative Train loss: 0.039617  [922112/961219]\n",
      "Epoch  103 TRAIN-MSE:  0.03843489327915291  VAL-MSE: 0.0649300920202377 LR: 1.220703125e-08\n",
      "Epoch 104. Cumulative Train loss: 0.027251  [  512/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.053582  [51712/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.049483  [102912/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.036848  [154112/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.050136  [205312/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.048199  [256512/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.041839  [307712/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.044607  [358912/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.040377  [410112/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.037201  [461312/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.036791  [512512/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.041644  [563712/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.039029  [614912/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.036846  [666112/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.034998  [717312/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.037827  [768512/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.040954  [819712/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.039170  [870912/961219]\n",
      "Epoch 104. Cumulative Train loss: 0.039681  [922112/961219]\n",
      "Epoch  104 TRAIN-MSE:  0.038469756404443764  VAL-MSE: 0.0649430660491294 LR: 1.220703125e-08\n",
      "Epoch 105. Cumulative Train loss: 0.009369  [  512/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.051366  [51712/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.030661  [102912/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.033866  [154112/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.040782  [205312/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.034893  [256512/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.034888  [307712/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.034784  [358912/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.038805  [410112/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.035576  [461312/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.039114  [512512/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.040136  [563712/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.041125  [614912/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.040479  [666112/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.040045  [717312/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.040486  [768512/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.038622  [819712/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.037061  [870912/961219]\n",
      "Epoch 105. Cumulative Train loss: 0.038041  [922112/961219]\n",
      "Epoch  105 TRAIN-MSE:  0.03845369189613646  VAL-MSE: 0.06494731497257314 LR: 1.220703125e-08\n",
      "Epoch 106. Cumulative Train loss: 0.007797  [  512/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.011483  [51712/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.022747  [102912/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.025518  [154112/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.021882  [205312/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.019535  [256512/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.026636  [307712/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.027025  [358912/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.024978  [410112/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.025907  [461312/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.037327  [512512/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.037042  [563712/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.034780  [614912/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.035642  [666112/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.041169  [717312/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.039093  [768512/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.037269  [819712/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.037443  [870912/961219]\n",
      "Epoch 106. Cumulative Train loss: 0.038139  [922112/961219]\n",
      "Epoch  106 TRAIN-MSE:  0.03842287702675357  VAL-MSE: 0.06494266428845993 LR: 1.220703125e-08\n",
      "Epoch 107. Cumulative Train loss: 0.011837  [  512/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.044665  [51712/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.041005  [102912/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.031054  [154112/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.026269  [205312/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.023012  [256512/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.029691  [307712/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.033587  [358912/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.036813  [410112/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.033844  [461312/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.034350  [512512/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.033970  [563712/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.032066  [614912/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.032089  [666112/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.032016  [717312/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.033529  [768512/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.034243  [819712/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.039065  [870912/961219]\n",
      "Epoch 107. Cumulative Train loss: 0.038724  [922112/961219]\n",
      "Epoch  107 TRAIN-MSE:  0.03867975081425236  VAL-MSE: 0.06494741642728764 LR: 1.220703125e-08\n",
      "Epoch 108. Cumulative Train loss: 0.041030  [  512/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.030304  [51712/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.020485  [102912/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.017375  [154112/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.015817  [205312/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.014689  [256512/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.027052  [307712/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.024518  [358912/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.031399  [410112/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.029077  [461312/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.032656  [512512/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.032596  [563712/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.032654  [614912/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.034961  [666112/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.036405  [717312/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.037800  [768512/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.041142  [819712/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.039445  [870912/961219]\n",
      "Epoch 108. Cumulative Train loss: 0.039677  [922112/961219]\n",
      "Epoch  108 TRAIN-MSE:  0.038441988535144064  VAL-MSE: 0.0649361143720911 LR: 1.220703125e-08\n",
      "Epoch 109. Cumulative Train loss: 0.010847  [  512/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.011292  [51712/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.035994  [102912/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.027777  [154112/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.029171  [205312/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.030208  [256512/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.026906  [307712/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.024560  [358912/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.035384  [410112/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.037024  [461312/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.036681  [512512/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.034305  [563712/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.035904  [614912/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.036671  [666112/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.038185  [717312/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.037617  [768512/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.037572  [819712/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.039321  [870912/961219]\n",
      "Epoch 109. Cumulative Train loss: 0.037698  [922112/961219]\n",
      "Epoch  109 TRAIN-MSE:  0.038442655151592876  VAL-MSE: 0.06495780944824218 LR: 1.220703125e-08\n",
      "Epoch 110. Cumulative Train loss: 0.011136  [  512/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.079656  [51712/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.070458  [102912/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.062178  [154112/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.056429  [205312/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.054682  [256512/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.051055  [307712/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.048414  [358912/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.048875  [410112/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.044505  [461312/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.041121  [512512/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.038331  [563712/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.036172  [614912/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.036125  [666112/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.041600  [717312/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.040952  [768512/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.041311  [819712/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.041284  [870912/961219]\n",
      "Epoch 110. Cumulative Train loss: 0.039565  [922112/961219]\n",
      "Epoch  110 TRAIN-MSE:  0.03841563654940524  VAL-MSE: 0.0649433379477643 LR: 1.220703125e-08\n",
      "Epoch 111. Cumulative Train loss: 0.006011  [  512/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.009559  [51712/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.019034  [102912/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.016017  [154112/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.020431  [205312/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.022667  [256512/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.025230  [307712/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.031075  [358912/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.034087  [410112/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.035423  [461312/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.039724  [512512/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.037090  [563712/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.037182  [614912/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.038070  [666112/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.036163  [717312/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.035998  [768512/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.036260  [819712/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.039903  [870912/961219]\n",
      "Epoch 111. Cumulative Train loss: 0.039661  [922112/961219]\n",
      "Epoch  111 TRAIN-MSE:  0.03841686918632315  VAL-MSE: 0.06494060678684965 LR: 1.220703125e-08\n",
      "Epoch 112. Cumulative Train loss: 0.013213  [  512/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.068728  [51712/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.054556  [102912/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.040154  [154112/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.032827  [205312/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.040833  [256512/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.040625  [307712/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.036477  [358912/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.038489  [410112/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.035416  [461312/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.037354  [512512/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.034846  [563712/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.036574  [614912/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.037296  [666112/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.039056  [717312/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.037137  [768512/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.038177  [819712/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.038088  [870912/961219]\n",
      "Epoch 112. Cumulative Train loss: 0.037794  [922112/961219]\n",
      "Epoch  112 TRAIN-MSE:  0.038462771994931595  VAL-MSE: 0.06496293088223072 LR: 1.220703125e-08\n",
      "Epoch 113. Cumulative Train loss: 0.006883  [  512/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.031146  [51712/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.049799  [102912/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.036487  [154112/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.044242  [205312/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.043013  [256512/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.041562  [307712/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.037233  [358912/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.033964  [410112/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.038868  [461312/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.036089  [512512/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.036140  [563712/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.040424  [614912/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.039982  [666112/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.037898  [717312/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.037488  [768512/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.037628  [819712/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.039284  [870912/961219]\n",
      "Epoch 113. Cumulative Train loss: 0.039572  [922112/961219]\n",
      "Epoch  113 TRAIN-MSE:  0.03841839413901472  VAL-MSE: 0.06495454666462351 LR: 1.220703125e-08\n",
      "Epoch 114. Cumulative Train loss: 0.006560  [  512/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.010099  [51712/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.010387  [102912/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.010135  [154112/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.010271  [205312/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.025506  [256512/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.038896  [307712/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.034608  [358912/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.034352  [410112/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.034215  [461312/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.034125  [512512/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.033895  [563712/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.037859  [614912/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.037616  [666112/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.037938  [717312/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.036135  [768512/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.036669  [819712/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.036727  [870912/961219]\n",
      "Epoch 114. Cumulative Train loss: 0.037613  [922112/961219]\n",
      "Epoch  114 TRAIN-MSE:  0.03839851791040101  VAL-MSE: 0.06494536704205452 LR: 1.220703125e-08\n",
      "Epoch 115. Cumulative Train loss: 0.007598  [  512/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.012670  [51712/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.011097  [102912/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.011205  [154112/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.040284  [205312/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.039340  [256512/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.034631  [307712/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.031254  [358912/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.032491  [410112/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.032384  [461312/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.038914  [512512/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.042644  [563712/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.041866  [614912/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.042023  [666112/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.039744  [717312/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.040412  [768512/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.038599  [819712/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.037014  [870912/961219]\n",
      "Epoch 115. Cumulative Train loss: 0.038384  [922112/961219]\n",
      "Epoch  115 TRAIN-MSE:  0.03839741502486812  VAL-MSE: 0.06495521626573927 LR: 1.220703125e-08\n",
      "Epoch 116. Cumulative Train loss: 0.010469  [  512/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.040043  [51712/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.026092  [102912/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.052776  [154112/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.051170  [205312/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.043278  [256512/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.054133  [307712/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.047846  [358912/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.047853  [410112/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.046232  [461312/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.048474  [512512/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.046914  [563712/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.048177  [614912/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.045198  [666112/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.044170  [717312/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.041940  [768512/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.041218  [819712/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.039380  [870912/961219]\n",
      "Epoch 116. Cumulative Train loss: 0.039654  [922112/961219]\n",
      "Epoch  116 TRAIN-MSE:  0.03839687718434559  VAL-MSE: 0.06495407591474817 LR: 1.220703125e-08\n",
      "Epoch 117. Cumulative Train loss: 0.006020  [  512/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.100330  [51712/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.055586  [102912/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.048511  [154112/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.046082  [205312/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.043010  [256512/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.040641  [307712/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.036264  [358912/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.035994  [410112/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.033189  [461312/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.039363  [512512/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.036791  [563712/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.034538  [614912/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.034490  [666112/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.038294  [717312/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.036443  [768512/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.036651  [819712/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.040099  [870912/961219]\n",
      "Epoch 117. Cumulative Train loss: 0.039651  [922112/961219]\n",
      "Epoch  117 TRAIN-MSE:  0.03842681217655992  VAL-MSE: 0.06494619085433635 LR: 1.220703125e-08\n",
      "Epoch 118. Cumulative Train loss: 0.007757  [  512/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.060638  [51712/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.052485  [102912/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.038272  [154112/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.045125  [205312/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.047568  [256512/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.044969  [307712/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.040373  [358912/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.042262  [410112/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.040765  [461312/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.040172  [512512/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.042729  [563712/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.044703  [614912/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.042173  [666112/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.039851  [717312/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.037943  [768512/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.036164  [819712/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.035945  [870912/961219]\n",
      "Epoch 118. Cumulative Train loss: 0.038001  [922112/961219]\n",
      "Epoch  118 TRAIN-MSE:  0.0384085444927898  VAL-MSE: 0.06495075225830078 LR: 1.220703125e-08\n",
      "Epoch 119. Cumulative Train loss: 0.008136  [  512/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.052060  [51712/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.055838  [102912/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.056445  [154112/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.045065  [205312/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.044103  [256512/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.054376  [307712/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.054537  [358912/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.049168  [410112/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.044909  [461312/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.044814  [512512/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.041836  [563712/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.039138  [614912/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.041045  [666112/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.040564  [717312/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.040102  [768512/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.040021  [819712/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.040052  [870912/961219]\n",
      "Epoch 119. Cumulative Train loss: 0.038443  [922112/961219]\n",
      "Epoch  119 TRAIN-MSE:  0.038401582274591915  VAL-MSE: 0.06495285439998545 LR: 1.220703125e-08\n",
      "Epoch 120. Cumulative Train loss: 0.010233  [  512/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.039717  [51712/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.053487  [102912/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.053292  [154112/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.048349  [205312/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.053727  [256512/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.046433  [307712/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.041414  [358912/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.037628  [410112/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.034694  [461312/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.032361  [512512/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.034483  [563712/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.040591  [614912/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.038223  [666112/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.036271  [717312/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.036035  [768512/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.040471  [819712/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.039906  [870912/961219]\n",
      "Epoch 120. Cumulative Train loss: 0.038293  [922112/961219]\n",
      "Epoch  120 TRAIN-MSE:  0.038419676840471956  VAL-MSE: 0.06496105599910655 LR: 1.220703125e-08\n",
      "Epoch 121. Cumulative Train loss: 0.012738  [  512/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.010907  [51712/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.020215  [102912/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.033397  [154112/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.035125  [205312/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.030241  [256512/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.027141  [307712/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.030354  [358912/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.027852  [410112/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.025830  [461312/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.030154  [512512/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.034246  [563712/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.032325  [614912/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.034738  [666112/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.034627  [717312/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.036780  [768512/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.036472  [819712/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.037589  [870912/961219]\n",
      "Epoch 121. Cumulative Train loss: 0.038043  [922112/961219]\n",
      "Epoch  121 TRAIN-MSE:  0.03839137114899846  VAL-MSE: 0.06495293962194565 LR: 1.220703125e-08\n",
      "Epoch 122. Cumulative Train loss: 0.007500  [  512/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.056514  [51712/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.046173  [102912/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.034260  [154112/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.028539  [205312/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.029442  [256512/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.033287  [307712/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.036927  [358912/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.033701  [410112/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.033857  [461312/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.031514  [512512/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.029648  [563712/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.028059  [614912/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.029830  [666112/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.031711  [717312/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.030224  [768512/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.029028  [819712/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.032254  [870912/961219]\n",
      "Epoch 122. Cumulative Train loss: 0.032969  [922112/961219]\n",
      "Epoch  122 TRAIN-MSE:  0.03839513256474417  VAL-MSE: 0.06494482324478475 LR: 1.220703125e-08\n",
      "Epoch 123. Cumulative Train loss: 0.007990  [  512/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.068094  [51712/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.062848  [102912/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.045775  [154112/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.036862  [205312/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.035692  [256512/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.039959  [307712/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.035701  [358912/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.036903  [410112/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.039956  [461312/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.039944  [512512/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.037373  [563712/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.044153  [614912/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.041663  [666112/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.042357  [717312/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.043094  [768512/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.041091  [819712/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.041333  [870912/961219]\n",
      "Epoch 123. Cumulative Train loss: 0.039620  [922112/961219]\n",
      "Epoch  123 TRAIN-MSE:  0.03845024348288195  VAL-MSE: 0.06493880089293134 LR: 1.220703125e-08\n",
      "Epoch 124. Cumulative Train loss: 0.009301  [  512/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.077951  [51712/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.073617  [102912/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.052900  [154112/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.049181  [205312/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.045756  [256512/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.040047  [307712/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.050162  [358912/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.050735  [410112/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.048803  [461312/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.047324  [512512/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.045708  [563712/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.047601  [614912/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.044790  [666112/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.044763  [717312/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.042505  [768512/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.041922  [819712/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.040057  [870912/961219]\n",
      "Epoch 124. Cumulative Train loss: 0.038379  [922112/961219]\n",
      "Epoch  124 TRAIN-MSE:  0.03849140848692693  VAL-MSE: 0.06493421919802402 LR: 1.220703125e-08\n",
      "Epoch 125. Cumulative Train loss: 0.009382  [  512/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.033809  [51712/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.046812  [102912/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.042483  [154112/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.043261  [205312/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.047281  [256512/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.044750  [307712/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.039938  [358912/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.036240  [410112/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.036472  [461312/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.040221  [512512/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.039594  [563712/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.041952  [614912/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.039592  [666112/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.037676  [717312/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.037214  [768512/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.039377  [819712/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.039778  [870912/961219]\n",
      "Epoch 125. Cumulative Train loss: 0.039639  [922112/961219]\n",
      "Epoch  125 TRAIN-MSE:  0.038449574416380326  VAL-MSE: 0.06492980794703707 LR: 1.220703125e-08\n",
      "Epoch 126. Cumulative Train loss: 0.017882  [  512/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.009391  [51712/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.009948  [102912/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.038720  [154112/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.031792  [205312/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.034324  [256512/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.034242  [307712/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.039267  [358912/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.038151  [410112/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.039417  [461312/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.039426  [512512/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.043414  [563712/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.046613  [614912/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.043790  [666112/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.044347  [717312/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.043709  [768512/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.041637  [819712/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.039771  [870912/961219]\n",
      "Epoch 126. Cumulative Train loss: 0.038145  [922112/961219]\n",
      "Epoch  126 TRAIN-MSE:  0.038437674214559604  VAL-MSE: 0.06493953136687583 LR: 1.220703125e-08\n",
      "Epoch 127. Cumulative Train loss: 0.007775  [  512/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.011693  [51712/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.034287  [102912/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.026619  [154112/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.022761  [205312/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.028269  [256512/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.031118  [307712/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.033934  [358912/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.036570  [410112/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.036945  [461312/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.039955  [512512/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.037275  [563712/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.037013  [614912/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.040459  [666112/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.040192  [717312/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.038150  [768512/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.040120  [819712/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.039732  [870912/961219]\n",
      "Epoch 127. Cumulative Train loss: 0.038059  [922112/961219]\n",
      "Epoch  127 TRAIN-MSE:  0.038485961393686256  VAL-MSE: 0.06492791277297 LR: 1.220703125e-08\n",
      "Epoch 128. Cumulative Train loss: 0.009870  [  512/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.066882  [51712/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.071912  [102912/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.059058  [154112/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.058555  [205312/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.053395  [256512/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.046193  [307712/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.052474  [358912/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.050508  [410112/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.046138  [461312/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.042584  [512512/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.043788  [563712/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.040913  [614912/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.041201  [666112/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.040864  [717312/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.038884  [768512/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.041340  [819712/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.039539  [870912/961219]\n",
      "Epoch 128. Cumulative Train loss: 0.039586  [922112/961219]\n",
      "Epoch  128 TRAIN-MSE:  0.03839998386395625  VAL-MSE: 0.0649239682136698 LR: 1.220703125e-08\n",
      "Epoch 129. Cumulative Train loss: 2.371227  [  512/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.057357  [51712/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.043950  [102912/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.040283  [154112/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.037456  [205312/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.032228  [256512/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.037253  [307712/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.033430  [358912/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.030617  [410112/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.037049  [461312/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.037216  [512512/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.034761  [563712/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.036705  [614912/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.034812  [666112/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.033073  [717312/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.035741  [768512/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.036307  [819712/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.036157  [870912/961219]\n",
      "Epoch 129. Cumulative Train loss: 0.039577  [922112/961219]\n",
      "Epoch  129 TRAIN-MSE:  0.03839359713963276  VAL-MSE: 0.06492222725076878 LR: 1.220703125e-08\n",
      "Epoch 130. Cumulative Train loss: 0.006477  [  512/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.055015  [51712/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.032579  [102912/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.025143  [154112/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.036907  [205312/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.041986  [256512/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.036766  [307712/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.032828  [358912/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.042645  [410112/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.039144  [461312/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.036367  [512512/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.036624  [563712/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.034572  [614912/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.032687  [666112/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.031117  [717312/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.033201  [768512/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.035512  [819712/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.036441  [870912/961219]\n",
      "Epoch 130. Cumulative Train loss: 0.034947  [922112/961219]\n",
      "Epoch  130 TRAIN-MSE:  0.038379305399135075  VAL-MSE: 0.06492063644084524 LR: 1.220703125e-08\n",
      "Epoch 131. Cumulative Train loss: 0.008258  [  512/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.009770  [51712/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.028435  [102912/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.030023  [154112/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.031014  [205312/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.026909  [256512/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.032760  [307712/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.033011  [358912/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.035078  [410112/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.040608  [461312/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.037795  [512512/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.035378  [563712/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.035221  [614912/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.034856  [666112/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.034627  [717312/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.034444  [768512/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.036182  [819712/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.038220  [870912/961219]\n",
      "Epoch 131. Cumulative Train loss: 0.037830  [922112/961219]\n",
      "Epoch  131 TRAIN-MSE:  0.03848706271957618  VAL-MSE: 0.0649143340739798 LR: 1.220703125e-08\n",
      "Epoch 132. Cumulative Train loss: 0.008618  [  512/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.037811  [51712/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.024262  [102912/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.031391  [154112/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.026690  [205312/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.032786  [256512/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.028997  [307712/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.033467  [358912/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.033388  [410112/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.030804  [461312/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.031101  [512512/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.037523  [563712/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.038827  [614912/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.039264  [666112/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.039727  [717312/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.042295  [768512/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.040320  [819712/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.039768  [870912/961219]\n",
      "Epoch 132. Cumulative Train loss: 0.038109  [922112/961219]\n",
      "Epoch  132 TRAIN-MSE:  0.03839096095372305  VAL-MSE: 0.06491454509978599 LR: 1.220703125e-08\n",
      "Epoch 133. Cumulative Train loss: 0.061744  [  512/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.041193  [51712/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.036731  [102912/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.027959  [154112/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.036019  [205312/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.035416  [256512/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.031026  [307712/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.039692  [358912/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.038376  [410112/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.041076  [461312/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.040354  [512512/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.041532  [563712/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.041894  [614912/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.042128  [666112/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.041538  [717312/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.041842  [768512/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.040015  [819712/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.039930  [870912/961219]\n",
      "Epoch 133. Cumulative Train loss: 0.038339  [922112/961219]\n",
      "Epoch  133 TRAIN-MSE:  0.03839786525833578  VAL-MSE: 0.06492714983351687 LR: 1.220703125e-08\n",
      "Epoch 134. Cumulative Train loss: 0.009283  [  512/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.048193  [51712/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.029125  [102912/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.030859  [154112/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.040576  [205312/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.051474  [256512/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.057991  [307712/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.051198  [358912/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.049863  [410112/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.051646  [461312/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.047622  [512512/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.046478  [563712/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.043526  [614912/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.040913  [666112/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.040362  [717312/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.040363  [768512/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.040016  [819712/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.038208  [870912/961219]\n",
      "Epoch 134. Cumulative Train loss: 0.039560  [922112/961219]\n",
      "Epoch  134 TRAIN-MSE:  0.0383927961216329  VAL-MSE: 0.06491554341417678 LR: 1.220703125e-08\n",
      "Epoch 135. Cumulative Train loss: 0.010192  [  512/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.044580  [51712/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.056204  [102912/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.062843  [154112/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.058510  [205312/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.054851  [256512/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.047655  [307712/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.049871  [358912/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.050210  [410112/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.045712  [461312/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.044817  [512512/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.043880  [563712/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.042957  [614912/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.040432  [666112/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.038247  [717312/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.038014  [768512/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.038078  [819712/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.036417  [870912/961219]\n",
      "Epoch 135. Cumulative Train loss: 0.039611  [922112/961219]\n",
      "Epoch  135 TRAIN-MSE:  0.03842912561506136  VAL-MSE: 0.06492430910151055 LR: 1.220703125e-08\n",
      "Epoch 136. Cumulative Train loss: 0.007879  [  512/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.009988  [51712/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.021784  [102912/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.025820  [154112/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.021803  [205312/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.023679  [256512/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.027152  [307712/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.033650  [358912/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.033713  [410112/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.035816  [461312/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.036161  [512512/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.033847  [563712/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.036509  [614912/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.040327  [666112/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.040308  [717312/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.038301  [768512/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.036608  [819712/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.036253  [870912/961219]\n",
      "Epoch 136. Cumulative Train loss: 0.038285  [922112/961219]\n",
      "Epoch  136 TRAIN-MSE:  0.03840464792765788  VAL-MSE: 0.06492336760175989 LR: 1.220703125e-08\n",
      "Epoch 137. Cumulative Train loss: 0.009518  [  512/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.009864  [51712/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.027495  [102912/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.038347  [154112/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.049936  [205312/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.046682  [256512/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.040715  [307712/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.041131  [358912/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.040272  [410112/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.045333  [461312/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.041838  [512512/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.038901  [563712/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.038575  [614912/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.039178  [666112/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.041679  [717312/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.040947  [768512/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.038995  [819712/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.039670  [870912/961219]\n",
      "Epoch 137. Cumulative Train loss: 0.038014  [922112/961219]\n",
      "Epoch  137 TRAIN-MSE:  0.03839216123462017  VAL-MSE: 0.06492656545436129 LR: 1.220703125e-08\n",
      "Epoch 138. Cumulative Train loss: 0.010141  [  512/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.043278  [51712/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.057798  [102912/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.056359  [154112/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.053371  [205312/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.044677  [256512/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.042945  [307712/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.047341  [358912/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.048901  [410112/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.050333  [461312/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.046395  [512512/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.045315  [563712/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.044177  [614912/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.043362  [666112/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.042708  [717312/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.040589  [768512/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.038859  [819712/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.039985  [870912/961219]\n",
      "Epoch 138. Cumulative Train loss: 0.039637  [922112/961219]\n",
      "Epoch  138 TRAIN-MSE:  0.03839551632667478  VAL-MSE: 0.06493391483388049 LR: 1.220703125e-08\n",
      "Epoch 139. Cumulative Train loss: 0.008033  [  512/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.067104  [51712/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.049019  [102912/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.036542  [154112/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.035635  [205312/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.041396  [256512/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.040080  [307712/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.035792  [358912/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.037091  [410112/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.039330  [461312/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.040702  [512512/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.043991  [563712/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.041220  [614912/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.038888  [666112/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.036908  [717312/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.036541  [768512/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.036721  [819712/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.036568  [870912/961219]\n",
      "Epoch 139. Cumulative Train loss: 0.038349  [922112/961219]\n",
      "Epoch  139 TRAIN-MSE:  0.0383687638067685  VAL-MSE: 0.06492207303960272 LR: 1.220703125e-08\n",
      "Epoch 140. Cumulative Train loss: 0.009452  [  512/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.028093  [51712/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.019335  [102912/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.025657  [154112/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.027797  [205312/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.035947  [256512/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.050323  [307712/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.044716  [358912/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.040502  [410112/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.037156  [461312/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.036883  [512512/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.038516  [563712/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.036120  [614912/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.034148  [666112/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.032477  [717312/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.031043  [768512/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.036711  [819712/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.037792  [870912/961219]\n",
      "Epoch 140. Cumulative Train loss: 0.038041  [922112/961219]\n",
      "Epoch  140 TRAIN-MSE:  0.038443086789603544  VAL-MSE: 0.06493335480385638 LR: 1.220703125e-08\n",
      "Epoch 141. Cumulative Train loss: 0.007927  [  512/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.033792  [51712/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.022300  [102912/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.018174  [154112/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.038799  [205312/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.032965  [256512/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.029230  [307712/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.029936  [358912/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.036054  [410112/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.045114  [461312/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.045166  [512512/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.041868  [563712/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.044245  [614912/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.043357  [666112/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.044437  [717312/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.043783  [768512/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.041748  [819712/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.041277  [870912/961219]\n",
      "Epoch 141. Cumulative Train loss: 0.039610  [922112/961219]\n",
      "Epoch  141 TRAIN-MSE:  0.03839829602552512  VAL-MSE: 0.06493292463586685 LR: 1.220703125e-08\n",
      "Epoch 142. Cumulative Train loss: 0.005900  [  512/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.056589  [51712/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.084736  [102912/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.075984  [154112/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.064541  [205312/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.058059  [256512/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.050101  [307712/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.044492  [358912/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.042617  [410112/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.041520  [461312/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.038365  [512512/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.040884  [563712/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.045593  [614912/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.044712  [666112/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.044016  [717312/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.041871  [768512/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.039888  [819712/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.038190  [870912/961219]\n",
      "Epoch 142. Cumulative Train loss: 0.039456  [922112/961219]\n",
      "Epoch  142 TRAIN-MSE:  0.03836718130473908  VAL-MSE: 0.06493154079356092 LR: 1.220703125e-08\n",
      "Epoch 143. Cumulative Train loss: 0.010126  [  512/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.032778  [51712/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.021425  [102912/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.025414  [154112/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.029036  [205312/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.025394  [256512/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.023201  [307712/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.024655  [358912/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.022979  [410112/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.024027  [461312/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.022726  [512512/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.030409  [563712/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.032986  [614912/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.031254  [666112/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.029737  [717312/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.029755  [768512/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.032893  [819712/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.035973  [870912/961219]\n",
      "Epoch 143. Cumulative Train loss: 0.039679  [922112/961219]\n",
      "Epoch  143 TRAIN-MSE:  0.03847678525113482  VAL-MSE: 0.06494907216822847 LR: 1.220703125e-08\n",
      "Epoch 144. Cumulative Train loss: 0.024958  [  512/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.043505  [51712/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.040641  [102912/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.030442  [154112/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.025695  [205312/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.027514  [256512/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.035289  [307712/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.035406  [358912/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.032286  [410112/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.032544  [461312/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.037123  [512512/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.036790  [563712/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.038024  [614912/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.035934  [666112/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.034122  [717312/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.036125  [768512/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.035878  [819712/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.039131  [870912/961219]\n",
      "Epoch 144. Cumulative Train loss: 0.037507  [922112/961219]\n",
      "Epoch  144 TRAIN-MSE:  0.03837646700167102  VAL-MSE: 0.06493139469877203 LR: 1.220703125e-08\n",
      "Epoch 145. Cumulative Train loss: 0.010142  [  512/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.071807  [51712/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.041444  [102912/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.038514  [154112/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.039975  [205312/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.045058  [256512/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.043275  [307712/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.038577  [358912/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.037262  [410112/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.034302  [461312/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.040577  [512512/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.040599  [563712/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.038093  [614912/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.040957  [666112/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.038658  [717312/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.038339  [768512/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.036584  [819712/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.036502  [870912/961219]\n",
      "Epoch 145. Cumulative Train loss: 0.038662  [922112/961219]\n",
      "Epoch  145 TRAIN-MSE:  0.03863424497296492  VAL-MSE: 0.06492927632433303 LR: 1.220703125e-08\n",
      "Epoch 146. Cumulative Train loss: 0.008954  [  512/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.032488  [51712/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.035624  [102912/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.042783  [154112/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.040791  [205312/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.039165  [256512/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.037813  [307712/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.033906  [358912/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.038738  [410112/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.039449  [461312/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.036502  [512512/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.036798  [563712/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.036582  [614912/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.036790  [666112/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.034963  [717312/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.034742  [768512/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.034639  [819712/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.037494  [870912/961219]\n",
      "Epoch 146. Cumulative Train loss: 0.037014  [922112/961219]\n",
      "Epoch  146 TRAIN-MSE:  0.03837525685733389  VAL-MSE: 0.06494915739018867 LR: 1.220703125e-08\n",
      "Epoch 147. Cumulative Train loss: 0.008534  [  512/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.009152  [51712/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.031129  [102912/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.038784  [154112/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.031897  [205312/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.044064  [256512/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.038602  [307712/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.034431  [358912/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.031458  [410112/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.033051  [461312/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.033872  [512512/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.031783  [563712/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.033645  [614912/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.033701  [666112/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.032162  [717312/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.033050  [768512/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.031638  [819712/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.033448  [870912/961219]\n",
      "Epoch 147. Cumulative Train loss: 0.039620  [922112/961219]\n",
      "Epoch  147 TRAIN-MSE:  0.03840694570352539  VAL-MSE: 0.06493346843313663 LR: 1.220703125e-08\n",
      "Epoch 148. Cumulative Train loss: 0.009571  [  512/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.041047  [51712/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.102112  [102912/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.079189  [154112/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.076697  [205312/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.072569  [256512/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.072195  [307712/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.063583  [358912/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.056899  [410112/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.051630  [461312/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.055857  [512512/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.053829  [563712/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.052098  [614912/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.048829  [666112/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.047938  [717312/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.045428  [768512/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.043250  [819712/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.041317  [870912/961219]\n",
      "Epoch 148. Cumulative Train loss: 0.039548  [922112/961219]\n",
      "Epoch  148 TRAIN-MSE:  0.03836517332838819  VAL-MSE: 0.06493831391030169 LR: 1.220703125e-08\n",
      "Epoch 149. Cumulative Train loss: 0.010797  [  512/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.011564  [51712/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.011247  [102912/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.010948  [154112/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.022541  [205312/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.029622  [256512/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.031274  [307712/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.030885  [358912/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.028422  [410112/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.026689  [461312/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.031124  [512512/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.029322  [563712/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.035907  [614912/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.033916  [666112/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.036320  [717312/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.037758  [768512/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.039493  [819712/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.038998  [870912/961219]\n",
      "Epoch 149. Cumulative Train loss: 0.037400  [922112/961219]\n",
      "Epoch  149 TRAIN-MSE:  0.03836673939788041  VAL-MSE: 0.06493175993574427 LR: 1.220703125e-08\n",
      "Epoch 150. Cumulative Train loss: 0.011623  [  512/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.068348  [51712/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.039677  [102912/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.030168  [154112/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.025630  [205312/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.032296  [256512/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.028511  [307712/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.034137  [358912/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.031204  [410112/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.033852  [461312/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.041324  [512512/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.038442  [563712/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.042720  [614912/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.041998  [666112/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.039808  [717312/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.040995  [768512/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.042152  [819712/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.040239  [870912/961219]\n",
      "Epoch 150. Cumulative Train loss: 0.039576  [922112/961219]\n",
      "Epoch  150 TRAIN-MSE:  0.038372492112459355  VAL-MSE: 0.06493163413189827 LR: 1.220703125e-08\n",
      "Epoch 151. Cumulative Train loss: 0.009307  [  512/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.069851  [51712/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.040449  [102912/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.045069  [154112/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.036667  [205312/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.050867  [256512/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.048016  [307712/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.048682  [358912/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.047002  [410112/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.042883  [461312/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.039663  [512512/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.039085  [563712/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.036711  [614912/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.036989  [666112/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.038909  [717312/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.038458  [768512/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.036740  [819712/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.036564  [870912/961219]\n",
      "Epoch 151. Cumulative Train loss: 0.036636  [922112/961219]\n",
      "Epoch  151 TRAIN-MSE:  0.03838213380719748  VAL-MSE: 0.06495594673968376 LR: 1.220703125e-08\n",
      "Epoch 152. Cumulative Train loss: 0.010209  [  512/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.010051  [51712/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.034424  [102912/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.049325  [154112/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.039603  [205312/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.043138  [256512/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.041657  [307712/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.037193  [358912/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.036503  [410112/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.038598  [461312/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.035759  [512512/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.035557  [563712/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.033604  [614912/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.037090  [666112/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.038251  [717312/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.039373  [768512/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.041565  [819712/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.041301  [870912/961219]\n",
      "Epoch 152. Cumulative Train loss: 0.039585  [922112/961219]\n",
      "Epoch  152 TRAIN-MSE:  0.03839663196731006  VAL-MSE: 0.06494251413548247 LR: 1.220703125e-08\n",
      "Epoch 153. Cumulative Train loss: 0.007621  [  512/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.084831  [51712/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.061607  [102912/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.055528  [154112/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.061069  [205312/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.057990  [256512/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.049936  [307712/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.044299  [358912/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.040129  [410112/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.036853  [461312/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.036429  [512512/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.036165  [563712/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.039257  [614912/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.042139  [666112/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.039870  [717312/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.040866  [768512/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.040550  [819712/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.040115  [870912/961219]\n",
      "Epoch 153. Cumulative Train loss: 0.039543  [922112/961219]\n",
      "Epoch  153 TRAIN-MSE:  0.03836086289600109  VAL-MSE: 0.06493798925521525 LR: 1.220703125e-08\n",
      "Epoch 154. Cumulative Train loss: 0.010360  [  512/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.010401  [51712/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.009891  [102912/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.019975  [154112/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.030539  [205312/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.031501  [256512/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.028033  [307712/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.036653  [358912/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.033451  [410112/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.035682  [461312/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.035628  [512512/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.038880  [563712/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.038330  [614912/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.040995  [666112/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.041342  [717312/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.039217  [768512/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.038783  [819712/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.038828  [870912/961219]\n",
      "Epoch 154. Cumulative Train loss: 0.037232  [922112/961219]\n",
      "Epoch  154 TRAIN-MSE:  0.03838621443648316  VAL-MSE: 0.0649345560276762 LR: 1.220703125e-08\n",
      "Epoch 155. Cumulative Train loss: 0.012611  [  512/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.032689  [51712/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.021730  [102912/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.017752  [154112/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.041340  [205312/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.042361  [256512/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.048184  [307712/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.049026  [358912/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.044102  [410112/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.040450  [461312/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.040213  [512512/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.039551  [563712/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.037198  [614912/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.038427  [666112/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.038098  [717312/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.036280  [768512/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.036478  [819712/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.037900  [870912/961219]\n",
      "Epoch 155. Cumulative Train loss: 0.037550  [922112/961219]\n",
      "Epoch  155 TRAIN-MSE:  0.03836689243381754  VAL-MSE: 0.0649398073236993 LR: 1.220703125e-08\n",
      "Epoch 156. Cumulative Train loss: 0.010776  [  512/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.011601  [51712/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.022518  [102912/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.019126  [154112/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.022549  [205312/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.020254  [256512/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.018598  [307712/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.025583  [358912/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.023688  [410112/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.024442  [461312/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.025357  [512512/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.028906  [563712/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.027341  [614912/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.031412  [666112/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.036677  [717312/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.036918  [768512/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.038178  [819712/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.036499  [870912/961219]\n",
      "Epoch 156. Cumulative Train loss: 0.039503  [922112/961219]\n",
      "Epoch  156 TRAIN-MSE:  0.03837519380486153  VAL-MSE: 0.06493844783052485 LR: 1.220703125e-08\n",
      "Epoch 157. Cumulative Train loss: 0.011196  [  512/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.012172  [51712/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.011376  [102912/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.018293  [154112/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.025743  [205312/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.022721  [256512/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.036204  [307712/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.039933  [358912/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.039016  [410112/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.035906  [461312/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.035615  [512512/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.035384  [563712/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.036218  [614912/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.036284  [666112/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.036215  [717312/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.035947  [768512/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.039065  [819712/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.039606  [870912/961219]\n",
      "Epoch 157. Cumulative Train loss: 0.038044  [922112/961219]\n",
      "Epoch  157 TRAIN-MSE:  0.038347237612516544  VAL-MSE: 0.06493860204169091 LR: 1.220703125e-08\n",
      "Epoch 158. Cumulative Train loss: 0.008552  [  512/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.033354  [51712/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.022490  [102912/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.045967  [154112/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.037110  [205312/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.036269  [256512/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.037675  [307712/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.045474  [358912/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.047685  [410112/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.052432  [461312/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.048216  [512512/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.049840  [563712/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.048573  [614912/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.045636  [666112/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.046123  [717312/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.043818  [768512/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.043021  [819712/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.041177  [870912/961219]\n",
      "Epoch 158. Cumulative Train loss: 0.039509  [922112/961219]\n",
      "Epoch  158 TRAIN-MSE:  0.03835391809026233  VAL-MSE: 0.06496586901076297 LR: 1.220703125e-08\n",
      "Epoch 159. Cumulative Train loss: 0.011334  [  512/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.033635  [51712/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.040937  [102912/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.045962  [154112/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.037029  [205312/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.031640  [256512/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.039443  [307712/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.038006  [358912/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.034603  [410112/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.034461  [461312/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.034261  [512512/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.032128  [563712/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.035853  [614912/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.035625  [666112/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.035860  [717312/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.037317  [768512/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.038574  [819712/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.038398  [870912/961219]\n",
      "Epoch 159. Cumulative Train loss: 0.038559  [922112/961219]\n",
      "Epoch  159 TRAIN-MSE:  0.03845383180925251  VAL-MSE: 0.06500929568676238 LR: 1.220703125e-08\n",
      "Epoch 160. Cumulative Train loss: 0.009623  [  512/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.045258  [51712/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.041929  [102912/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.031821  [154112/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.037273  [205312/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.031861  [256512/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.031660  [307712/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.041485  [358912/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.041039  [410112/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.041490  [461312/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.040626  [512512/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.037861  [563712/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.035642  [614912/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.033754  [666112/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.033678  [717312/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.033411  [768512/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.031953  [819712/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.036362  [870912/961219]\n",
      "Epoch 160. Cumulative Train loss: 0.039530  [922112/961219]\n",
      "Epoch  160 TRAIN-MSE:  0.03832891347041478  VAL-MSE: 0.0649404525756836 LR: 1.220703125e-08\n",
      "Epoch 161. Cumulative Train loss: 0.007396  [  512/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.011141  [51712/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.021301  [102912/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.017640  [154112/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.029001  [205312/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.029951  [256512/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.042174  [307712/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.037686  [358912/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.037141  [410112/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.036556  [461312/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.036281  [512512/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.041646  [563712/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.040910  [614912/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.043233  [666112/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.044783  [717312/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.042449  [768512/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.041968  [819712/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.041186  [870912/961219]\n",
      "Epoch 161. Cumulative Train loss: 0.039538  [922112/961219]\n",
      "Epoch  161 TRAIN-MSE:  0.03833530338717123  VAL-MSE: 0.06494484759391622 LR: 1.220703125e-08\n",
      "Epoch 162. Cumulative Train loss: 0.008025  [  512/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.045692  [51712/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.028214  [102912/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.030898  [154112/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.038544  [205312/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.033149  [256512/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.029490  [307712/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.029911  [358912/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.030833  [410112/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.038583  [461312/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.039334  [512512/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.038394  [563712/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.040540  [614912/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.038305  [666112/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.039996  [717312/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.040816  [768512/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.041869  [819712/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.039985  [870912/961219]\n",
      "Epoch 162. Cumulative Train loss: 0.039643  [922112/961219]\n",
      "Epoch  162 TRAIN-MSE:  0.038377423855030755  VAL-MSE: 0.06495678678471992 LR: 1.220703125e-08\n",
      "Epoch 163. Cumulative Train loss: 0.009107  [  512/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.010406  [51712/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.010507  [102912/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.010612  [154112/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.010906  [205312/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.017419  [256512/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.016382  [307712/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.015599  [358912/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.022193  [410112/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.020766  [461312/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.023139  [512512/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.030354  [563712/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.030366  [614912/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.032405  [666112/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.030825  [717312/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.033907  [768512/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.032521  [819712/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.032880  [870912/961219]\n",
      "Epoch 163. Cumulative Train loss: 0.037456  [922112/961219]\n",
      "Epoch  163 TRAIN-MSE:  0.03842004469007698  VAL-MSE: 0.06497801922737284 LR: 1.220703125e-08\n",
      "Epoch 164. Cumulative Train loss: 0.006063  [  512/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.034291  [51712/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.022216  [102912/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.028143  [154112/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.030402  [205312/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.054769  [256512/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.047466  [307712/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.049489  [358912/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.047808  [410112/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.043666  [461312/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.042508  [512512/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.039673  [563712/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.037251  [614912/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.038416  [666112/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.038111  [717312/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.039579  [768512/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.040428  [819712/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.038626  [870912/961219]\n",
      "Epoch 164. Cumulative Train loss: 0.037043  [922112/961219]\n",
      "Epoch  164 TRAIN-MSE:  0.03838308395285266  VAL-MSE: 0.06495678678471992 LR: 1.220703125e-08\n",
      "Epoch 165. Cumulative Train loss: 0.006570  [  512/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.047921  [51712/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.047372  [102912/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.053542  [154112/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.048931  [205312/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.041139  [256512/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.036021  [307712/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.035342  [358912/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.035048  [410112/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.032584  [461312/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.030365  [512512/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.036006  [563712/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.041705  [614912/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.041003  [666112/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.040442  [717312/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.038492  [768512/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.039836  [819712/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.039760  [870912/961219]\n",
      "Epoch 165. Cumulative Train loss: 0.039605  [922112/961219]\n",
      "Epoch  165 TRAIN-MSE:  0.03834859634718845  VAL-MSE: 0.06496572697416265 LR: 1.220703125e-08\n",
      "Epoch 166. Cumulative Train loss: 0.006327  [  512/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.009356  [51712/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.020153  [102912/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.038813  [154112/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.044117  [205312/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.044183  [256512/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.041668  [307712/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.037167  [358912/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.036586  [410112/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.039038  [461312/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.038515  [512512/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.038155  [563712/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.041598  [614912/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.043919  [666112/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.043257  [717312/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.041211  [768512/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.039233  [819712/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.037578  [870912/961219]\n",
      "Epoch 166. Cumulative Train loss: 0.037935  [922112/961219]\n",
      "Epoch  166 TRAIN-MSE:  0.038335240612657494  VAL-MSE: 0.06496404688409034 LR: 1.220703125e-08\n",
      "Epoch 167. Cumulative Train loss: 0.008493  [  512/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.011424  [51712/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.027844  [102912/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.022302  [154112/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.019128  [205312/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.017763  [256512/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.023828  [307712/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.026037  [358912/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.027727  [410112/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.028352  [461312/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.037397  [512512/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.036981  [563712/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.037113  [614912/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.041005  [666112/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.038803  [717312/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.036864  [768512/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.038039  [819712/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.037788  [870912/961219]\n",
      "Epoch 167. Cumulative Train loss: 0.036228  [922112/961219]\n",
      "Epoch  167 TRAIN-MSE:  0.03834178675009737  VAL-MSE: 0.06496548348284782 LR: 1.220703125e-08\n",
      "Epoch 168. Cumulative Train loss: 0.008675  [  512/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.057504  [51712/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.034227  [102912/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.026314  [154112/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.028594  [205312/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.025125  [256512/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.034889  [307712/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.031418  [358912/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.035122  [410112/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.034977  [461312/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.034496  [512512/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.034176  [563712/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.037714  [614912/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.037380  [666112/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.035469  [717312/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.036123  [768512/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.041258  [819712/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.039407  [870912/961219]\n",
      "Epoch 168. Cumulative Train loss: 0.037748  [922112/961219]\n",
      "Epoch  168 TRAIN-MSE:  0.03836168046084583  VAL-MSE: 0.06497250820728058 LR: 1.220703125e-08\n",
      "Epoch 169. Cumulative Train loss: 0.008805  [  512/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.011523  [51712/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.010728  [102912/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.018110  [154112/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.016672  [205312/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.025746  [256512/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.030551  [307712/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.030657  [358912/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.034704  [410112/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.035041  [461312/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.035977  [512512/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.033753  [563712/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.041236  [614912/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.041886  [666112/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.041285  [717312/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.039198  [768512/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.038814  [819712/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.039839  [870912/961219]\n",
      "Epoch 169. Cumulative Train loss: 0.039516  [922112/961219]\n",
      "Epoch  169 TRAIN-MSE:  0.03834057580560636  VAL-MSE: 0.06497198064276513 LR: 1.220703125e-08\n",
      "Epoch 170. Cumulative Train loss: 0.012720  [  512/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.033256  [51712/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.022087  [102912/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.025904  [154112/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.029396  [205312/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.025450  [256512/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.023070  [307712/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.021452  [358912/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.029779  [410112/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.030095  [461312/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.034400  [512512/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.032196  [563712/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.034713  [614912/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.043773  [666112/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.041388  [717312/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.041116  [768512/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.040527  [819712/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.038763  [870912/961219]\n",
      "Epoch 170. Cumulative Train loss: 0.039568  [922112/961219]\n",
      "Epoch  170 TRAIN-MSE:  0.03835414670615063  VAL-MSE: 0.0649640144185817 LR: 1.220703125e-08\n",
      "Epoch 171. Cumulative Train loss: 0.023586  [  512/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.034766  [51712/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.094983  [102912/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.095012  [154112/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.074077  [205312/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.065624  [256512/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.056478  [307712/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.049868  [358912/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.045164  [410112/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.041181  [461312/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.043265  [512512/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.046763  [563712/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.044030  [614912/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.041366  [666112/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.040808  [717312/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.040104  [768512/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.038265  [819712/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.039698  [870912/961219]\n",
      "Epoch 171. Cumulative Train loss: 0.039503  [922112/961219]\n",
      "Epoch  171 TRAIN-MSE:  0.03832492953800141  VAL-MSE: 0.06497187512986204 LR: 1.220703125e-08\n",
      "Epoch 172. Cumulative Train loss: 0.008472  [  512/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.056447  [51712/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.033221  [102912/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.037625  [154112/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.035993  [205312/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.030943  [256512/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.031182  [307712/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.031542  [358912/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.031756  [410112/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.036172  [461312/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.037042  [512512/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.037240  [563712/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.036985  [614912/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.034975  [666112/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.034590  [717312/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.039191  [768512/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.037379  [819712/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.035782  [870912/961219]\n",
      "Epoch 172. Cumulative Train loss: 0.037096  [922112/961219]\n",
      "Epoch  172 TRAIN-MSE:  0.03832675936965111  VAL-MSE: 0.06496230997937791 LR: 1.220703125e-08\n",
      "Epoch 173. Cumulative Train loss: 0.008094  [  512/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.011459  [51712/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.055171  [102912/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.048032  [154112/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.043825  [205312/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.044721  [256512/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.042863  [307712/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.038072  [358912/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.037889  [410112/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.034840  [461312/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.035281  [512512/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.036291  [563712/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.035987  [614912/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.035786  [666112/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.034002  [717312/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.036237  [768512/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.034636  [819712/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.033221  [870912/961219]\n",
      "Epoch 173. Cumulative Train loss: 0.037793  [922112/961219]\n",
      "Epoch  173 TRAIN-MSE:  0.03833145203358465  VAL-MSE: 0.0649702843199385 LR: 1.220703125e-08\n",
      "Epoch 174. Cumulative Train loss: 0.008445  [  512/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.032840  [51712/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.021958  [102912/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.025799  [154112/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.032020  [205312/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.033122  [256512/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.041605  [307712/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.039950  [358912/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.038841  [410112/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.035602  [461312/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.036383  [512512/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.034020  [563712/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.037929  [614912/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.039424  [666112/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.037440  [717312/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.037151  [768512/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.037275  [819712/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.035737  [870912/961219]\n",
      "Epoch 174. Cumulative Train loss: 0.036758  [922112/961219]\n",
      "Epoch  174 TRAIN-MSE:  0.03845500204483041  VAL-MSE: 0.06495448173360621 LR: 1.220703125e-08\n",
      "Epoch 175. Cumulative Train loss: 0.009383  [  512/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.010804  [51712/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.021686  [102912/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.025691  [154112/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.021838  [205312/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.030598  [256512/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.031072  [307712/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.031782  [358912/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.031561  [410112/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.031328  [461312/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.034428  [512512/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.034317  [563712/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.032373  [614912/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.035695  [666112/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.035398  [717312/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.033697  [768512/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.037152  [819712/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.035542  [870912/961219]\n",
      "Epoch 175. Cumulative Train loss: 0.038134  [922112/961219]\n",
      "Epoch  175 TRAIN-MSE:  0.038396528282542974  VAL-MSE: 0.06495102821512425 LR: 1.220703125e-08\n",
      "Epoch 176. Cumulative Train loss: 0.007528  [  512/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.044577  [51712/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.027953  [102912/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.030019  [154112/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.030250  [205312/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.032088  [256512/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.032349  [307712/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.029177  [358912/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.031723  [410112/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.029318  [461312/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.031651  [512512/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.031887  [563712/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.032581  [614912/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.032434  [666112/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.030962  [717312/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.031032  [768512/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.035617  [819712/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.034136  [870912/961219]\n",
      "Epoch 176. Cumulative Train loss: 0.036553  [922112/961219]\n",
      "Epoch  176 TRAIN-MSE:  0.03831815868353271  VAL-MSE: 0.06495896603198761 LR: 1.220703125e-08\n",
      "Epoch 177. Cumulative Train loss: 0.006977  [  512/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.031766  [51712/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.030462  [102912/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.035093  [154112/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.037968  [205312/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.036975  [256512/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.036771  [307712/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.047546  [358912/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.048828  [410112/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.044663  [461312/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.044095  [512512/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.043290  [563712/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.040527  [614912/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.043248  [666112/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.040927  [717312/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.038985  [768512/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.040671  [819712/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.038930  [870912/961219]\n",
      "Epoch 177. Cumulative Train loss: 0.037308  [922112/961219]\n",
      "Epoch  177 TRAIN-MSE:  0.038311407976477335  VAL-MSE: 0.06495972897144074 LR: 1.220703125e-08\n",
      "Epoch 178. Cumulative Train loss: 0.006323  [  512/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.038114  [51712/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.047996  [102912/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.035548  [154112/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.036730  [205312/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.031470  [256512/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.031797  [307712/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.035154  [358912/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.031994  [410112/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.038661  [461312/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.037880  [512512/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.038422  [563712/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.036032  [614912/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.039458  [666112/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.039041  [717312/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.037215  [768512/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.039206  [819712/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.037512  [870912/961219]\n",
      "Epoch 178. Cumulative Train loss: 0.037690  [922112/961219]\n",
      "Epoch  178 TRAIN-MSE:  0.038340109599559816  VAL-MSE: 0.0650406614263007 LR: 1.220703125e-08\n",
      "Epoch 179. Cumulative Train loss: 0.009786  [  512/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.074150  [51712/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.042564  [102912/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.046897  [154112/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.055707  [205312/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.051223  [256512/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.044270  [307712/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.039256  [358912/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.035624  [410112/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.035575  [461312/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.033217  [512512/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.037634  [563712/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.035450  [614912/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.033507  [666112/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.034007  [717312/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.034005  [768512/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.039791  [819712/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.040050  [870912/961219]\n",
      "Epoch 179. Cumulative Train loss: 0.039654  [922112/961219]\n",
      "Epoch  179 TRAIN-MSE:  0.038446042935400646  VAL-MSE: 0.06496052031821394 LR: 1.220703125e-08\n",
      "Epoch 180. Cumulative Train loss: 0.011119  [  512/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.084981  [51712/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.059566  [102912/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.050293  [154112/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.040165  [205312/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.033992  [256512/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.030137  [307712/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.027239  [358912/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.043223  [410112/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.041768  [461312/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.040774  [512512/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.039926  [563712/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.037454  [614912/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.038719  [666112/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.040855  [717312/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.042067  [768512/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.040094  [819712/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.039687  [870912/961219]\n",
      "Epoch 180. Cumulative Train loss: 0.039650  [922112/961219]\n",
      "Epoch  180 TRAIN-MSE:  0.038454939525215406  VAL-MSE: 0.0649539460527136 LR: 1.220703125e-08\n",
      "Epoch 181. Cumulative Train loss: 0.009334  [  512/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.085422  [51712/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.060540  [102912/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.064863  [154112/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.063491  [205312/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.057566  [256512/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.055388  [307712/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.049203  [358912/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.044309  [410112/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.043206  [461312/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.045797  [512512/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.044454  [563712/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.047065  [614912/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.046034  [666112/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.044832  [717312/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.043772  [768512/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.043151  [819712/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.041211  [870912/961219]\n",
      "Epoch 181. Cumulative Train loss: 0.039544  [922112/961219]\n",
      "Epoch  181 TRAIN-MSE:  0.038352811336864474  VAL-MSE: 0.06495261902504779 LR: 1.220703125e-08\n",
      "Epoch 182. Cumulative Train loss: 0.009245  [  512/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.083540  [51712/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.058889  [102912/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.056933  [154112/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.051012  [205312/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.042869  [256512/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.037425  [307712/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.033602  [358912/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.034014  [410112/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.036178  [461312/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.037120  [512512/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.034643  [563712/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.032674  [614912/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.030990  [666112/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.032783  [717312/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.035693  [768512/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.034133  [819712/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.035855  [870912/961219]\n",
      "Epoch 182. Cumulative Train loss: 0.037618  [922112/961219]\n",
      "Epoch  182 TRAIN-MSE:  0.03837761594948614  VAL-MSE: 0.06495991158992687 LR: 1.220703125e-08\n",
      "Epoch 183. Cumulative Train loss: 0.011275  [  512/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.009705  [51712/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.028237  [102912/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.040043  [154112/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.038438  [205312/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.032872  [256512/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.029035  [307712/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.032589  [358912/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.029834  [410112/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.027750  [461312/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.037748  [512512/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.037405  [563712/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.035288  [614912/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.034789  [666112/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.034591  [717312/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.034538  [768512/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.034418  [819712/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.033094  [870912/961219]\n",
      "Epoch 183. Cumulative Train loss: 0.034456  [922112/961219]\n",
      "Epoch  183 TRAIN-MSE:  0.03831638307513965  VAL-MSE: 0.06495776886635639 LR: 1.220703125e-08\n",
      "Epoch 184. Cumulative Train loss: 0.008980  [  512/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.094548  [51712/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.064537  [102912/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.072789  [154112/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.062757  [205312/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.052204  [256512/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.045182  [307712/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.043333  [358912/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.042475  [410112/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.041487  [461312/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.038236  [512512/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.038071  [563712/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.038324  [614912/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.043538  [666112/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.041111  [717312/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.039240  [768512/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.038892  [819712/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.038462  [870912/961219]\n",
      "Epoch 184. Cumulative Train loss: 0.036868  [922112/961219]\n",
      "Epoch  184 TRAIN-MSE:  0.038309504375946775  VAL-MSE: 0.06497008952688664 LR: 1.220703125e-08\n",
      "Epoch 185. Cumulative Train loss: 0.006077  [  512/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.011918  [51712/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.033797  [102912/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.033639  [154112/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.038140  [205312/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.045321  [256512/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.039546  [307712/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.035540  [358912/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.035933  [410112/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.035456  [461312/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.032882  [512512/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.032847  [563712/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.031033  [614912/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.034484  [666112/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.032705  [717312/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.031179  [768512/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.035423  [819712/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.035996  [870912/961219]\n",
      "Epoch 185. Cumulative Train loss: 0.036477  [922112/961219]\n",
      "Epoch  185 TRAIN-MSE:  0.03831124696528482  VAL-MSE: 0.06497667596695271 LR: 1.220703125e-08\n",
      "Epoch 186. Cumulative Train loss: 0.008487  [  512/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.011835  [51712/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.029729  [102912/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.054142  [154112/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.057585  [205312/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.048076  [256512/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.050363  [307712/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.048725  [358912/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.050418  [410112/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.048668  [461312/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.044903  [512512/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.041761  [563712/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.041156  [614912/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.041341  [666112/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.040667  [717312/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.038652  [768512/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.039072  [819712/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.040066  [870912/961219]\n",
      "Epoch 186. Cumulative Train loss: 0.038429  [922112/961219]\n",
      "Epoch  186 TRAIN-MSE:  0.03831805564890615  VAL-MSE: 0.06498301079932679 LR: 1.220703125e-08\n",
      "Epoch 187. Cumulative Train loss: 0.011196  [  512/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.013546  [51712/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.011622  [102912/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.022342  [154112/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.028015  [205312/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.034003  [256512/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.045903  [307712/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.043427  [358912/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.042169  [410112/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.038626  [461312/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.038282  [512512/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.040579  [563712/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.038129  [614912/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.037960  [666112/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.038720  [717312/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.038703  [768512/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.040398  [819712/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.039954  [870912/961219]\n",
      "Epoch 187. Cumulative Train loss: 0.039504  [922112/961219]\n",
      "Epoch  187 TRAIN-MSE:  0.038303711851790975  VAL-MSE: 0.06498111562525972 LR: 1.220703125e-08\n",
      "Epoch 188. Cumulative Train loss: 0.012216  [  512/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.031137  [51712/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.021170  [102912/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.032261  [154112/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.026602  [205312/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.035164  [256512/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.034759  [307712/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.031403  [358912/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.031824  [410112/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.029456  [461312/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.027640  [512512/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.033286  [563712/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.031403  [614912/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.031332  [666112/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.033480  [717312/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.038714  [768512/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.038382  [819712/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.036900  [870912/961219]\n",
      "Epoch 188. Cumulative Train loss: 0.036887  [922112/961219]\n",
      "Epoch  188 TRAIN-MSE:  0.038302587434010736  VAL-MSE: 0.06500020534434217 LR: 1.220703125e-08\n",
      "Epoch 189. Cumulative Train loss: 0.009693  [  512/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.030174  [51712/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.047091  [102912/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.034677  [154112/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.034284  [205312/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.041282  [256512/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.039289  [307712/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.044068  [358912/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.047464  [410112/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.045944  [461312/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.042543  [512512/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.039627  [563712/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.039686  [614912/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.039932  [666112/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.037843  [717312/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.037969  [768512/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.037620  [819712/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.037284  [870912/961219]\n",
      "Epoch 189. Cumulative Train loss: 0.036980  [922112/961219]\n",
      "Epoch  189 TRAIN-MSE:  0.03836057262611203  VAL-MSE: 0.06498982855614195 LR: 1.220703125e-08\n",
      "Epoch 190. Cumulative Train loss: 0.009111  [  512/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.081441  [51712/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.057700  [102912/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.076195  [154112/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.065499  [205312/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.066586  [256512/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.064810  [307712/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.060254  [358912/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.054111  [410112/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.051997  [461312/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.048002  [512512/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.044586  [563712/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.041792  [614912/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.039461  [666112/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.037402  [717312/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.035582  [768512/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.036136  [819712/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.039258  [870912/961219]\n",
      "Epoch 190. Cumulative Train loss: 0.037603  [922112/961219]\n",
      "Epoch  190 TRAIN-MSE:  0.0383122148866546  VAL-MSE: 0.06499283161569148 LR: 1.220703125e-08\n",
      "Epoch 191. Cumulative Train loss: 0.007922  [  512/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.062554  [51712/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.066768  [102912/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.047932  [154112/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.043545  [205312/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.046027  [256512/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.040039  [307712/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.035838  [358912/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.035472  [410112/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.037204  [461312/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.037506  [512512/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.037172  [563712/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.039745  [614912/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.037411  [666112/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.037885  [717312/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.035973  [768512/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.034421  [819712/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.034207  [870912/961219]\n",
      "Epoch 191. Cumulative Train loss: 0.036519  [922112/961219]\n",
      "Epoch  191 TRAIN-MSE:  0.03833042381602093  VAL-MSE: 0.06499318467809799 LR: 1.220703125e-08\n",
      "Epoch 192. Cumulative Train loss: 0.008976  [  512/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.010329  [51712/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.023759  [102912/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.028717  [154112/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.041996  [205312/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.039889  [256512/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.035035  [307712/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.031501  [358912/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.031824  [410112/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.033250  [461312/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.033216  [512512/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.031167  [563712/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.031290  [614912/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.034196  [666112/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.036289  [717312/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.038402  [768512/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.037880  [819712/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.039517  [870912/961219]\n",
      "Epoch 192. Cumulative Train loss: 0.039519  [922112/961219]\n",
      "Epoch  192 TRAIN-MSE:  0.03832135109242778  VAL-MSE: 0.06499212954906707 LR: 1.220703125e-08\n",
      "Epoch 193. Cumulative Train loss: 0.007775  [  512/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.009691  [51712/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.027329  [102912/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.021736  [154112/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.030502  [205312/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.038664  [256512/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.041299  [307712/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.041894  [358912/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.037912  [410112/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.037393  [461312/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.037057  [512512/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.034702  [563712/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.036981  [614912/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.038737  [666112/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.038380  [717312/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.038410  [768512/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.037873  [819712/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.039628  [870912/961219]\n",
      "Epoch 193. Cumulative Train loss: 0.039551  [922112/961219]\n",
      "Epoch  193 TRAIN-MSE:  0.038330838124389754  VAL-MSE: 0.0649942398071289 LR: 1.220703125e-08\n",
      "Epoch 194. Cumulative Train loss: 0.009499  [  512/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.053279  [51712/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.061117  [102912/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.061905  [154112/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.056200  [205312/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.058658  [256512/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.060974  [307712/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.053737  [358912/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.052850  [410112/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.050650  [461312/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.046470  [512512/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.043166  [563712/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.040509  [614912/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.039772  [666112/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.039391  [717312/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.038807  [768512/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.038710  [819712/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.039903  [870912/961219]\n",
      "Epoch 194. Cumulative Train loss: 0.039542  [922112/961219]\n",
      "Epoch  194 TRAIN-MSE:  0.038332246701928356  VAL-MSE: 0.06500183267796293 LR: 1.220703125e-08\n",
      "Epoch 195. Cumulative Train loss: 0.006931  [  512/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.060623  [51712/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.034933  [102912/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.034142  [154112/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.034044  [205312/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.033929  [256512/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.033859  [307712/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.030412  [358912/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.032468  [410112/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.034768  [461312/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.040011  [512512/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.037328  [563712/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.035218  [614912/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.039037  [666112/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.038496  [717312/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.042007  [768512/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.040026  [819712/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.038231  [870912/961219]\n",
      "Epoch 195. Cumulative Train loss: 0.036746  [922112/961219]\n",
      "Epoch  195 TRAIN-MSE:  0.038284812307558536  VAL-MSE: 0.06498115620714554 LR: 1.220703125e-08\n",
      "Epoch 196. Cumulative Train loss: 0.008586  [  512/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.044825  [51712/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.040707  [102912/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.036852  [154112/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.036307  [205312/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.036946  [256512/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.041080  [307712/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.044603  [358912/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.040328  [410112/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.040371  [461312/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.039767  [512512/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.038920  [563712/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.040093  [614912/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.039536  [666112/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.040715  [717312/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.038726  [768512/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.039307  [819712/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.039818  [870912/961219]\n",
      "Epoch 196. Cumulative Train loss: 0.039475  [922112/961219]\n",
      "Epoch  196 TRAIN-MSE:  0.03831272893431532  VAL-MSE: 0.0649858961714075 LR: 1.220703125e-08\n",
      "Epoch 197. Cumulative Train loss: 0.007213  [  512/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.060131  [51712/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.035008  [102912/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.044078  [154112/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.041161  [205312/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.046294  [256512/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.040333  [307712/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.042068  [358912/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.038145  [410112/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.034988  [461312/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.036358  [512512/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.034055  [563712/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.038766  [614912/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.038190  [666112/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.037848  [717312/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.036071  [768512/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.036428  [819712/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.037465  [870912/961219]\n",
      "Epoch 197. Cumulative Train loss: 0.039407  [922112/961219]\n",
      "Epoch  197 TRAIN-MSE:  0.03831379502169645  VAL-MSE: 0.06509689168727144 LR: 1.220703125e-08\n",
      "Epoch 198. Cumulative Train loss: 0.008475  [  512/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.045071  [51712/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.050588  [102912/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.056733  [154112/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.045031  [205312/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.038242  [256512/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.044859  [307712/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.042660  [358912/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.041557  [410112/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.037992  [461312/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.041169  [512512/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.038423  [563712/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.036129  [614912/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.038496  [666112/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.041019  [717312/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.039065  [768512/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.038534  [819712/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.038184  [870912/961219]\n",
      "Epoch 198. Cumulative Train loss: 0.037953  [922112/961219]\n",
      "Epoch  198 TRAIN-MSE:  0.038314866782061925  VAL-MSE: 0.06499018973492561 LR: 1.220703125e-08\n",
      "Epoch 199. Cumulative Train loss: 0.006663  [  512/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.010896  [51712/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.025464  [102912/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.027316  [154112/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.032497  [205312/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.033224  [256512/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.029488  [307712/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.030254  [358912/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.033275  [410112/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.035901  [461312/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.038098  [512512/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.038179  [563712/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.037518  [614912/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.037022  [666112/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.037143  [717312/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.035399  [768512/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.035921  [819712/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.036493  [870912/961219]\n",
      "Epoch 199. Cumulative Train loss: 0.038238  [922112/961219]\n",
      "Epoch  199 TRAIN-MSE:  0.038279939001511394  VAL-MSE: 0.06497643653382647 LR: 1.220703125e-08\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.RAdam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimiser, 'min',factor=0.5,patience=3,min_lr =1e-8)\n",
    "\n",
    "num_epochs = 200\n",
    "hist = np.zeros(num_epochs)\n",
    "hist_val = np.zeros(num_epochs)\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim =look_back-1  \n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    #model.hidden = model.init_hidden()\n",
    "    \n",
    "    model.train()\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        \n",
    "        # Forward pass\n",
    "        y_train_pred = model(x_batch)\n",
    "        loss = loss_fn(y_train_pred, y_batch)\n",
    "        del y_train_pred\n",
    "        \n",
    "        \n",
    "        hist[t] += loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1) #Prevent exploding gradient\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Every 100 batches, print the loss for this batch\n",
    "        # as well as the number of examples processed so far \n",
    "        if id_batch % 100 == 0:\n",
    "            loss, current =  hist[t]/(id_batch+1), (id_batch + 1)* len(x_batch)\n",
    "            print(f\"Epoch {t}. Cumulative Train loss: {loss:>7f}  [{current:>5d}/{len(dataloader.dataset):>5d}]\")        \n",
    "        \n",
    "    \n",
    "    \n",
    "    hist[t]/=(id_batch+1)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    #Compute validation loss\n",
    "    model.eval()\n",
    "    val_loss =0\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(dataloader_test):\n",
    "        y_test_pred = model(x_batch)\n",
    "        hist_val[t] += loss_fn(y_test_pred, y_batch)\n",
    "        del y_test_pred\n",
    "    \n",
    "    hist_val[t]/=(id_batch+1)\n",
    "    print(\"Epoch \", t, \"TRAIN-MSE: \", hist[t],\" VAL-MSE:\",hist_val[t],\"LR:\",optimiser.param_groups[0]['lr'])\n",
    "\n",
    "    # Update Scheduler\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'models/lstmv2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_val, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_test_pred = model(x_test)\n",
    "\n",
    "# invert predictions\n",
    "y_train_pred_unscaled = scaler.inverse_transform(y_train_pred.to('cpu').detach().numpy())\n",
    "y_train_actual_unscaled = scaler.inverse_transform(y_train.to('cpu').detach().numpy())\n",
    "\n",
    "y_test_pred_unscaled = scaler.inverse_transform(y_test_pred.to('cpu').detach().numpy())\n",
    "y_test_actual_unscaled = scaler.inverse_transform(y_test.to('cpu').detach().numpy())\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train_pred_unscaled, y_train_actual_unscaled))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test_pred_unscaled, y_test_actual_unscaled))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "figure, axes = plt.subplots(figsize=(15, 6))\n",
    "axes.xaxis_date()\n",
    "\n",
    "axes.plot(data_px[len(data_px)-len(y_test):].index, y_test_unscaled, color = 'red', label = 'Real IBM Stock Price')\n",
    "axes.plot(data_px[len(data_px)-len(y_test):].index, y_test_pred, color = 'blue', label = 'Predicted IBM Stock Price')\n",
    "#axes.xticks(np.arange(0,394,50))\n",
    "plt.title('IBM Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('IBM Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(figsize=(15, 6))\n",
    "axes.plot(y_test_pred_unscaled[120],color=\"green\",label=\"predicted\")\n",
    "axes.plot(y_test_actual_unscaled[120],color=\"blue\",label=\"actual\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
